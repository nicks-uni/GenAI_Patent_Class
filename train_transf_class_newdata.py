# -------------------------------------------------------------
# Import Required Packages
# -------------------------------------------------------------
# These lines import the necessary modules and libraries that the script will use.

import os  # Provides functions for interacting with the operating system, such as creating directories.
import logging  # Facilitates logging messages to track the execution of the script.
import warnings  # Used to control warning messages generated by the code.
import gc  # Provides access to the garbage collector for memory management.

import pandas as pd  # A powerful library for data manipulation and analysis, especially for tabular data.
import numpy as np  # Supports large, multi-dimensional arrays and matrices, along with a collection of mathematical functions.
from tqdm import tqdm  # Displays progress bars for loops, helping to track the progress of long-running operations.
import matplotlib.pyplot as plt  # A plotting library used for creating static, animated, and interactive visualizations.

# For Transformer Models
from simpletransformers.classification import ClassificationModel  # Provides an easy interface to train and use transformer-based classification models.
import torch  # A deep learning library that provides GPU acceleration for faster computations.

# Suppress warnings and set logging level to ERROR to reduce output clutter
warnings.filterwarnings("ignore")  # Ignores all warning messages to keep the output clean.
logging.basicConfig(level=logging.ERROR)  # Sets the logging level to only show errors, ignoring warnings and informational messages.
transformers_logger = logging.getLogger("transformers")  # Retrieves the logger for the transformers library.
transformers_logger.setLevel(logging.ERROR)  # Sets the transformers logger to only show errors.

# Check if CUDA is available (for GPU acceleration)
use_cuda = torch.cuda.is_available()  # Checks if a CUDA-compatible GPU is available for faster computations. Returns True if available, else False.

# -------------------------------------------------------------
# Define the PatentClassifier Class (XLNet Only)
# -------------------------------------------------------------
class PatentClassifier:
    def __init__(self, data_path, anti_seed_path, delimiter):
        """
        Initialize the PatentClassifier with data paths and configuration.

        Parameters:
        - data_path: Path to the training data CSV file.
        - anti_seed_path: Path to the anti-seed data CSV file.
        - delimiter: Delimiter used in the CSV files.
        """
        # Store the paths and delimiter for later use
        self.data_path = data_path
        self.anti_seed_path = anti_seed_path
        self.delimiter = delimiter

        # Initialize variables to store data and results
        self.data = None  # Will hold the combined dataset
        self.IDs = None  # Array to store patent IDs
        self.texts = None  # List to store patent abstracts
        self.labels = None  # List to store corresponding labels (GenAI or not)
        self.total_entries = None  # Total number of patents in the dataset
        self.total_ratio = None  # Ratio of GenAI to non-GenAI patents

        self.results = []  # List to store evaluation results for each model
        self.classified_values = []  # List to store individual classification results
        self.classified_values_p = []  # List to store prediction probabilities (if any)
        self.metrics = ["Accuracy", "AUC", "Precision", "Recall", "F1"]  # Metrics used for evaluation
        self.results_table = None  # DataFrame to store the final results table

        # Configuration specific to Large Language Models (LLMs)
        self.use_cuda = use_cuda  # Whether to use GPU acceleration
        self.model_name = "XLNet"  # Name of the model being used
        self.model_type = "xlnet"  # Type identifier for the model
        self.model_name_or_path = "xlnet-base-cased"  # Pre-trained model name or path
        self.model = None  # Placeholder for the ClassificationModel instance

    def load_data(self):
        """
        Load and preprocess the training data from a CSV file.
        """
        try:
            # Load the CSV file containing patent data using pandas
            self.data = pd.read_csv(self.data_path, delimiter=self.delimiter)
            print(f"Successfully loaded training data from {self.data_path}")

            # Create a new column 'label_genai' where patents with 'Finales Resultat' >= 1 are labeled as 1 (GenAI),
            # and others as 0 (non-GenAI). This converts the problem into a binary classification task.
            self.data["label_genai"] = (self.data["Finales Resultat"] >= 1).astype(int)
            print("Label encoding completed: 'label_genai' column created.")
        except FileNotFoundError:
            # Handle the case where the specified file does not exist
            print(f"Error: The file {self.data_path} does not exist.")
            raise
        except pd.errors.ParserError as e:
            # Handle parsing errors in the CSV file
            print(f"Error parsing the CSV file: {e}")
            raise
        except Exception as e:
            # Handle any other unexpected errors
            print(f"An unexpected error occurred while loading data: {e}")
            raise

    def load_anti_seed(self, n):
        """
        Load and integrate the anti-seed data to balance the dataset.

        Parameters:
        - n: Number of anti-seed entries to include.
        """
        try:
            # Load the anti-seed CSV file which contains non-GenAI patents
            anti_seed = pd.read_csv(self.anti_seed_path, delimiter=",")
            print(f"Successfully loaded anti-seed data from {self.anti_seed_path}")

            # Filter the anti-seed data to include only patents that are not related to AI
            anti_seed = anti_seed[anti_seed["actual"] == 0]
            print(f"Filtered anti-seed data to include only non-AI patents (actual == 0).")

            # Select the first 'n' entries from the anti-seed data for balancing
            anti_seed = anti_seed.iloc[:n]
            print(f"Selected the first {n} entries from the anti-seed dataset.")

            # Assign a label of 0 to all anti-seed patents, indicating they are non-GenAI
            anti_seed["label_genai"] = 0  # All anti-seed patents are defined as non-AI

            # Rename columns to match those in the training data for consistency
            anti_seed = anti_seed.rename(
                columns={"app number": "patent_id", "abstract": "patent_abstract"}
            )
            print("Renamed columns in anti-seed data for consistency.")

            # Select only the relevant columns needed for merging
            anti_seed = anti_seed[["patent_id", "patent_abstract", "label_genai"]]

            # Combine the original training data with the anti-seed data to balance the classes
            self.data = pd.concat(
                [
                    self.data[["patent_id", "patent_abstract", "label_genai"]],
                    anti_seed,
                ],
                ignore_index=True,
            )
            print("Combined training data with anti-seed data successfully.")
        except FileNotFoundError:
            # Handle the case where the anti-seed file does not exist
            print(f"Error: The file {self.anti_seed_path} does not exist.")
            raise
        except pd.errors.ParserError as e:
            # Handle parsing errors in the anti-seed CSV file
            print(f"Error parsing the anti-seed CSV file: {e}")
            raise
        except Exception as e:
            # Handle any other unexpected errors
            print(f"An unexpected error occurred while loading anti-seed data: {e}")
            raise

    def prepare_data(self):
        """
        Prepare data for text classification.

        Extracts IDs, texts, and labels from the dataset and calculates the label distribution.
        """
        try:
            # Extract patent IDs, abstracts, and labels from the dataset
            self.IDs = self.data["patent_id"].values  # Array of patent IDs
            self.texts = self.data["patent_abstract"].tolist()  # List of patent abstracts
            self.labels = self.data["label_genai"].tolist()  # List of labels (1 for GenAI, 0 for non-GenAI)

            # Calculate the percentage distribution of GenAI and non-GenAI patents
            label_counts = self.data["label_genai"].value_counts(normalize=True) * 100
            self.total_entries = len(self.data)  # Total number of patents
            self.total_ratio = f"{label_counts.get(1, 0):.0f}-{label_counts.get(0, 0):.0f}"  # Format ratio as "GenAI% - Non-GenAI%"

            # Print out the distribution and total count of patents
            print(f"Share of entries with '1': {label_counts.get(1, 0):.2f}%")  # Percentage of GenAI patents
            print(f"Share of entries with '0': {label_counts.get(0, 0):.2f}%")  # Percentage of non-GenAI patents
            print(f"1/0 Ratio: {self.total_ratio}")  # Ratio of GenAI to non-GenAI
            print(f"Total number of entries: {self.total_entries}")  # Total number of patents
        except KeyError as e:
            # Handle missing expected columns in the dataset
            print(f"Error: Missing expected column {e} in the data.")
            raise
        except Exception as e:
            # Handle any other unexpected errors during data preparation
            print(f"An unexpected error occurred during data preparation: {e}")
            raise

    def train_model(self):
        """
        Train the XLNet model on the entire dataset.
        """
        try:
            # Create a DataFrame with two columns: 'text' for abstracts and 'labels' for GenAI classification
            train_df = pd.DataFrame({"text": self.texts, "labels": self.labels})
            print("Prepared training DataFrame.")

            # Define the arguments for the transformer model training
            model_args = {
                "num_train_epochs": 5,  # Number of times the model will see the entire training dataset
                "overwrite_output_dir": True,  # Overwrite the output directory if it exists
                "use_early_stopping": False,  # Disable early stopping to train for all epochs
                "train_batch_size": 50,  # Number of samples processed before the model is updated
                "do_lower_case": False,  # Whether to lowercase the input text; False because XLNet is cased
                "silent": False,  # Whether to suppress verbose output during training
                "no_cache": True,  # Disable caching of models and tokenizers
                "no_save": False,         # Enable saving the trained model
                "save_model_every_epoch": False,  # Do not save the model after every epoch
                "save_eval_checkpoints": False,    # Do not save checkpoints during evaluation
                "evaluate_during_training": False, # Do not perform evaluation during training
            }

            print(f"Initializing the {self.model_name} model for training.")
            # Initialize the ClassificationModel with specified parameters
            self.model = ClassificationModel(
                self.model_type,            # Type of the model (e.g., 'xlnet')
                self.model_name_or_path,    # Pre-trained model name or path
                use_cuda=self.use_cuda,     # Whether to use GPU acceleration
                args=model_args,            # Training arguments defined above
            )

            # Train the model on the prepared training DataFrame
            print(f"Training {self.model_name} on the entire dataset...")
            self.model.train_model(train_df)
            print(f"Training of {self.model_name} completed successfully.")

        except Exception as e:
            # Handle any unexpected errors during model training
            print(f"An unexpected error occurred during model training: {e}")
            raise

    def predict_on_new_data(self, new_data_path, delimiter, num_entries=100):
        """
        Predict on new data using the trained model.

        Parameters:
        - new_data_path: Path to the new data CSV file.
        - delimiter: Delimiter used in the new data CSV file.
        - num_entries: Number of entries to predict on (default is 100).
        """
        try:
            # Load the new data CSV file containing patent abstracts to classify
            new_data = pd.read_csv(new_data_path, delimiter=delimiter)
            print(f"Successfully loaded new data from {new_data_path}")

            # Select the first 'num_entries' rows from the new data for prediction
            new_data = new_data.iloc[:num_entries]
            print(f"Selected the first {num_entries} entries for prediction.")

            # Ensure that the necessary columns exist in the new data
            required_columns = ["patent_id", "patent_abstract"]
            missing_columns = [col for col in required_columns if col not in new_data.columns]
            if missing_columns:
                raise ValueError(f"Missing required columns: {missing_columns}")

            # Extract patent IDs and abstracts from the new data
            new_IDs = new_data["patent_id"].values  # Array of patent IDs
            new_texts = new_data["patent_abstract"].tolist()  # List of patent abstracts
            total_new_entries = len(new_data)  # Total number of new patents to predict
            print(f"Loaded {total_new_entries} new entries for prediction.")

            # Validate that all abstracts are strings to prevent errors during prediction
            if not all(isinstance(text, str) for text in new_texts):
                raise ValueError("All patent abstracts must be strings.")

            # Prepare a DataFrame to store the prediction results
            predictions_df = pd.DataFrame({"id": new_IDs})  # DataFrame with patent IDs

            # Use the trained model to make predictions on the new abstracts
            print(f"Predicting new data with {self.model_name}...")
            predictions, raw_outputs = self.model.predict(new_texts)  # Get predictions and raw outputs

            # Attempt to calculate prediction probabilities using the raw outputs
            try:
                # simpletransformers does not provide predict_proba by default
                # Use softmax on raw outputs to calculate probabilities
                y_pred_proba = [
                    float(torch.nn.functional.softmax(torch.tensor(output), dim=0)[1])
                    for output in raw_outputs
                ]
            except Exception as e:
                # If probability calculation fails, assign NaN to probabilities
                print(f"Error calculating prediction probabilities: {e}")
                y_pred_proba = [np.nan] * len(predictions)  # Assign NaN if probability calculation fails

            # Store the predictions and their probabilities in the DataFrame
            predictions_df[self.model_name] = predictions  # Add prediction labels
            predictions_df[f"{self.model_name}_prob"] = y_pred_proba  # Add prediction probabilities

            # Save the prediction results to a CSV file for future reference
            output_path = f"Output/Predictions_{self.model_name}_on_New_Data_{total_new_entries}.csv"
            predictions_df.to_csv(output_path, index=False)
            print(f"Predictions on new data saved to {output_path}")

        except FileNotFoundError:
            # Handle the case where the new data file does not exist
            print(f"Error: The file {new_data_path} does not exist.")
            raise
        except pd.errors.ParserError as e:
            # Handle parsing errors in the new data CSV file
            print(f"Error parsing the CSV file: {e}")
            raise
        except ValueError as ve:
            # Handle value errors, such as missing columns or incorrect data types
            print(f"Value Error: {ve}")
            raise
        except Exception as ex:
            # Handle any other unexpected errors during prediction
            print(f"An unexpected error occurred during prediction: {ex}")
            raise

    def output_results(self):
        """
        Output the classification results and save them.

        Saves the results table and the classification results to CSV files.
        """
        try:
            # Convert the results list to a pandas DataFrame with specified column names
            self.results_table = pd.DataFrame(
                self.results,
                columns=[
                    "Name",
                    "Share",
                    "True-Positives",
                    "False-Negatives",
                    "False-Positives",
                    "True-Negatives",
                    "Accuracy",
                    "AUC",
                    "Precision",
                    "Recall",
                    "F1",
                ],
            )

            # Add a new column 'Type' with the value 'Individual' to categorize the model type
            self.results_table["Type"] = "Individual"

            # Rearrange columns for better readability in the output
            self.results_table = self.results_table[
                [
                    "Name",
                    "Type",
                    "Share",
                    "True-Positives",
                    "False-Negatives",
                    "False-Positives",
                    "True-Negatives",
                    "Accuracy",
                    "AUC",
                    "Precision",
                    "Recall",
                    "F1",
                ]
            ]

            # Define the output path for the results CSV file, including dataset size and ratio in the filename
            output_path = (
                f"Output/Classification_{self.model_name}_Performance_{self.total_entries}_{self.total_ratio}.csv"
            )
            # Save the results table to a CSV file, sorted by descending accuracy
            self.results_table.sort_values("Accuracy", ascending=False).to_csv(
                output_path, index=False
            )
            print(f"Results saved to {output_path}")

            # Display the sorted results table in the console for quick reference
            print("\nClassification Performance:")
            print(self.results_table.sort_values("Accuracy", ascending=False))

            # Create a final DataFrame to store classification results for each patent
            Final = pd.DataFrame({
                "id": self.IDs,  # Patent IDs
                "Actual": self.labels,  # Actual labels (1 or 0)
            })

            # Add the model's predictions to the Final DataFrame
            Final[self.model_name] = [
                pred for _, _, _, pred in self.classified_values[0]
            ]

            # Add the prediction probabilities to the Final DataFrame
            Final[f"{self.model_name}_prob"] = [
                prob for _, _, _, prob in self.classified_values_p[0]
            ]

            # Define the output path for the final classification results CSV file
            final_output_path = (
                f"Output/Classification_{self.model_name}_Results_{self.total_entries}_{self.total_ratio}.csv"
            )
            # Save the final classification results to a CSV file
            Final.to_csv(final_output_path, index=False)
            print(f"Classification results saved to {final_output_path}")

        except Exception as e:
            # Handle any unexpected errors during the output process
            print(f"An unexpected error occurred while outputting results: {e}")
            raise

    def plot_metrics(self):
        """
        Creates and saves a bar chart for all metrics,
        displaying individual metric values on the bars.

        Visualizes the performance metrics of the model.
        """
        try:
            # Select the relevant columns for plotting (model names and metrics)
            df_plot = self.results_table[["Name"] + self.metrics]

            # Define the number of metrics to plot
            num_metrics = len(self.metrics)
            ind = np.arange(num_metrics)  # The x locations for the groups
            width = 0.6  # The width of the bars

            # Create a figure and a set of subplots with specified size
            fig, ax = plt.subplots(figsize=(10, 6))

            # Create a bar for each metric
            bars = ax.bar(ind, df_plot.loc[0, self.metrics], width, color='skyblue')

            # Add labels on top of each bar to show the metric value
            for idx, bar in enumerate(bars):
                height = bar.get_height()
                ax.annotate(
                    f"{height:.2f}",  # The text label showing the metric value
                    xy=(bar.get_x() + bar.get_width() / 2, height),  # Position at the top center of the bar
                    xytext=(0, 3),  # Offset the text by 3 points vertically
                    textcoords="offset points",
                    ha="center",  # Horizontal alignment center
                    va="bottom",  # Vertical alignment bottom
                    fontsize=10,  # Font size of the text
                )

            # Set the labels for the x and y axes
            ax.set_xlabel("Metrics")  # Label for the x-axis
            ax.set_ylabel("Scores")  # Label for the y-axis
            # Set the title of the plot, including dataset size and ratio
            ax.set_title(
                f"Performance Metrics of {self.model_name} ({self.total_entries} patents, ratio: {self.total_ratio})"
            )
            # Set the positions and labels of the x ticks
            ax.set_xticks(ind)
            ax.set_xticklabels(self.metrics)
            ax.set_ylim(0, 1.05)  # Set the y-axis limits from 0 to slightly above 1 for better visualization

            # Adjust the layout to prevent clipping of tick-labels and titles
            plt.tight_layout()
            # Define the filename for saving the plot, including dataset size and ratio
            figure_filename = f"Figures/{self.model_name}_performance_bar_{self.total_entries}_{self.total_ratio}.png"
            # Save the plot as a PNG file with high resolution
            plt.savefig(
                figure_filename,
                dpi=300,
            )
            print(f"Performance metrics bar chart saved to {figure_filename}")
            # Display the plot to the user
            plt.show()

        except Exception as e:
            # Handle any unexpected errors during the plotting process
            print(f"An unexpected error occurred while plotting metrics: {e}")
            raise

# -------------------------------------------------------------
# Main Execution
# -------------------------------------------------------------
# The following block runs the code when the script is executed directly.
# To run the code, execute the following command in the terminal:
# python train_transf_class_newdata.py

if __name__ == "__main__":
    try:
        # Ensure that the necessary directories for saving figures and outputs exist.
        # If they do not exist, create them.
        os.makedirs("Figures", exist_ok=True)  # Creates 'Figures' directory if it doesn't exist
        os.makedirs("Output", exist_ok=True)   # Creates 'Output' directory if it doesn't exist

        # Define the paths to the data files.
        data_path = os.path.join(
            "Training Data", "20240819_WIPO Patents GenAI US matched_1-1000.csv"
        )  # Path to the main training data CSV file
        delimiter = ";"  # Delimiter used in the main training data CSV file

        anti_seed_path = os.path.join("Training Data", "4K Patents - AI 20p.csv")  # Path to the anti-seed data CSV file

        # Initialize an instance of the PatentClassifier with the specified data paths and delimiter.
        classifier = PatentClassifier(data_path, anti_seed_path, delimiter)

        # Load the main training data.
        classifier.load_data()
        # Load and integrate the anti-seed data to balance the dataset with a specified number of entries.
        classifier.load_anti_seed(n=3146)  # Adjust 'n' as needed based on desired balance
        # Prepare the data by extracting IDs, texts, labels, and calculating distribution metrics.
        classifier.prepare_data()

        # Train the model on the entire dataset.
        # Uncomment the following line to enable training.
        classifier.train_model()

        # Output the classification results and save them to CSV files.
        classifier.output_results()
        # Generate and save the performance metrics plot.
        classifier.plot_metrics()

        # Predict on new data using the trained model.
        new_data_path = os.path.join("Training Data", "chunk_Applications_Grants_Combined.csv")  # Path to the new data CSV file
        new_data_delimiter = ";"  # Delimiter used in the new data CSV file
        classifier.predict_on_new_data(new_data_path, new_data_delimiter, num_entries=100)  # Predict on the first 100 entries

    except Exception as e:
        # Handle any unexpected errors during the main execution process
        print(f"An error occurred during execution: {e}")