#;patent_id;app_year;patent_year;Assignee_Org;Assignee 1;CPC;patent_title;patent_abstract;Finales Resultat
1;5660176;1993;1997;First Opinion Corporation;First Opinion Corporation;G16H50/20,G16H10/20,G16H10/60,G16H15/00,G16H20/70,G16H40/67,G16H70/20,G16H70/60,H04M3/493,A61B5/0002,A61B5/4094,G16H20/10,G16H40/63,H04M2201/40,H04M2203/355,Y10S128/92,Y10S128/925,Y10S706/924;Computerized medical diagnostic and treatment advice system;"A system and method for providing computerized, knowledge-based medical diagnostic and treatment advice. The medical advice is provided to the general public over a telephone network. Two new authoring languages, interactive voice response and speech recognition are used to enable expert and general practitioner knowledge to be encoded for access by the public. ""Meta"" functions for time-density analysis of a number of factors regarding the number of medical complaints per unit of time are an integral part of the system. A semantic discrepancy evaluator routine along with a mental status examination are used to detect the consciousness level of a user of the system. A re-enter feature monitors the user's changing condition over time. A symptom severity analysis helps to respond to the changing conditions. System sensitivity factors may be changed at a global level or other levels to adjust the system advice as necessary.";1
2;6516298;2000;2003;Sumitomo Electric Industries, Ltd.;Sumitomo Electric Industries, Ltd.;G10L13/08;System and method for synthesizing multiplexed speech and text at a receiving terminal;The reception terminal receives a code series from the communication path. The separator separates the code series into a speech code series and text information. The speech code series is decoded into a pitch period, a LSP coefficient, and code numerals by the synthesizer to reproduce the speech sound in the CELP system. Also, the text information is converted into pronunciation and accent information by the language analyzer and added to prosody information, such as phoneme time length and pitch pattern by the prosody generator. The LSP coefficient, and code numerals suitable for the phoneme are read from the segment database and the pitch frequency from the prosody information is inputted to the synthesizer and synthesized into speech sound.;1
3;6542867;2000;2003;Sumitomo Electric Industries, Ltd.;Sumitomo Electric Industries, Ltd.;G10L13/10,G10L13/08;Speech duration processing method and apparatus for Chinese text-to-speech system;"The duration of speech varies according to the characteristics of pronounced speech and pronouncing habit of the speaker. In the speech duration processing method and apparatus of this invention, a large amount of natural speech was analyzed, and the following was known: Speech duration of monosyllables will vary according to factors, such as phonemes, tones, phrase construction, locations in the phrases, locations in the sentence, and front and rear connected phonemes, etc. of the syllables. Through the use of these varying factors, a &#8220;speech duration parameter storage portion&#8221; for speech duration parameters is constructed. By retrieving the speech duration parameters and combining the same with the basic speech duration of a syllable during syllable speech duration calculation, the speech duration of each monosyllable in any sentence can be accurately decided. As recognized from experimental results, a text-to-speech system using the speech duration processing apparatus of this invention can synthesize speech with natural speech duration.";1
4;7643990;2003;2010;Apple Inc.;Apple Inc.;G10L15/187,G10L25/90;Global boundary-centric feature extraction and associated discontinuity metrics;Portions from time-domain speech segments are extracted. Feature vectors that represent the portions in a vector space are created. The feature vectors incorporate phase information of the portions. A distance between the feature vectors in the vector space is determined. In one aspect, the feature vectors are created by constructing a matrix W from the portions and decomposing the matrix W. In one aspect, decomposing the matrix W comprises extracting global boundary-centric features from the portions. In one aspect, the portions include at least one pitch period. In another aspect, the portions include centered pitch periods.;0
5;7657289;2004;2010;;;G10L13/033,G10L2021/0135;Synthesized voice production;A communications system for receiving and transmitting information signals. An electronic processor is adapted to receive information signals from at least one source, operatively connected to the electronic processor. An audible signal generator generates sounds related to the information signals. The source of information signals can be a telephone, cell phone, microphone, PDA, computer, printed document, Internet web site, e-mail or immediate message. The processor has a mechanism for generating an audible signal reminiscent of a celebrity voice, a cartoon voice, or a computer-generated sound.;6
6;7660715;2004;2010;Avaya Inc.;Avaya Inc.;G10L15/22,G10L15/065,G10L15/063;Transparent monitoring and intervention to improve automatic adaptation of speech models;A system and method to improve the automatic adaptation of one or more speech models in automatic speech recognition systems. After a dialog begins, for example, the dialog asks the customer to provide spoken input and it is recorded. If the speech recognizer determines it may not have correctly transcribed the verbal response, i.e., voice input, the invention uses monitoring and if necessary, intervention to guarantee that the next transcription of the verbal response is correct. The dialog asks the customer to repeat his verbal response, which is recorded and a transcription of the input is sent to a human monitor, i.e., agent or operator. If the transcription of the spoken input is correct, the human does not intervene and the transcription remains unmodified. If the transcription of the verbal response is incorrect, the human intervenes and the transcription of the misrecognized word is corrected. In both cases, the dialog asks the customer to confirm the unmodified and corrected transcription. If the customer confirms the unmodified or newly corrected transcription, the dialog continues and the customer does not hang up in frustration because most times only one misrecognition occurred. Finally, the invention uses the first and second customer recording of the misrecognized word or utterance along with the corrected or unmodified transcription to automatically adapt one or more speech models, which improves the performance of the speech recognition system.;1
7;7664313;2002;2010;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G06V30/146,G06F40/106,G06F40/30,G06T17/00;Text-to scene conversion;The invention relates to a method of converting a set of words into a three-dimensional scene description, which may then be rendered into three-dimensional images. The invention may generate arbitrary scenes in response to a substantially unlimited range of input words. Scenes may be generated by combining objects, poses, facial expressions, environments, etc., so that they represent the input set of words. Poses may have generic elements so that referenced objects may be replaced by those mentioned in the input set of words. Likewise, a character may be dressed according to its role in the set of words. Various constraints for object positioning may be declared. The environment, including but not limited to place, time of day, and time of year, may be inferred from the input set of words.;5
8;7674966;2005;2010;;;G10H1/0025,G10H2210/026,G10H2210/151,G10H2240/085;System and method for realtime scoring of games and other applications;The invention provides a software framework that allows real-time computer-generated music to be used in interactive applications, particularly video games, by modularizing music-producing and music-modifying computer procedures into musically logical and programmatically convenient structures, as well as providing a communication mechanism between the application and the music-generating system which will allow the music to reflect the appropriate mood given the current application state.;5
9;7689416;2004;2010;;;G10L15/30,G10L15/07;System for transferring personalize matter from one computer to another;This invention combines methodologies that enhance voice recognition dictation. It describes features for moving speaker voice files eliminating redundant training of speech recognition dictation applications. It defines how to create synthetic voice models reducing speaker dependency. It combines accuracy and performance into a single measure called RAP Rate. Moreover, the invention describes enhancing voice recognition applications and systems by measure/adjusting hardware and software features for optimal voice recognition dictation incorporating methodical processes based on RAP Rate. Using these approaches and tools the invention includes a method for constructing a handheld transcriber that immediately translates audio speech into text with real-time display. The invention describes a method for applying RAP Rate and synthetic voice models to applications like voice mail to text. With the ability to move and translate voice models the invention describes new services that could be provided for a fee.;1
10;7693760;2002;2010;Oracle International Corporation;Oracle International Corporation;G06Q40/00,G06Q20/207,G06Q40/12,G06Q40/123;Method and apparatus for providing a tax service that is configurable for local jurisdictions;One embodiment of the present invention provides a system that implements a tax service that is configurable for local jurisdictions by an expert in the field, such as a tax professional, who may have little or no knowledge of computer programming. The system operates by receiving a request from a subscriber, wherein servicing the request involves calculating taxes for one or more local jurisdictions. Tax rules for a local jurisdiction can be fed into a tax rule base that is used by a tax engine that is configured to use these externally specified rules in performing tax computations. Next, the system uses the tax engine to calculate taxes for each local jurisdiction, and subsequently uses the results of the calculations in servicing the request.;0
11;7713127;2005;2010;;;A63F13/10,A63F13/47,A63F13/44,A63F13/45,A63F13/822,A63F2300/6081,A63F2300/632,A63F2300/638;Simulating dialog in electronic games;The present disclosure includes systems and techniques relating to simulating dialog in electronic games. In general, in one implementation, the technique includes: receiving a player interruption signal during presentation of a speech by a machine-controlled character in an electronic game, modifying a state of the machine-controlled character based on a time of the received player interruption relative to a length of the speech, and generating an action of the machine-controlled character based on the modified state. Modifying the state of the machine-controlled character can include adjusting an attitude variable of the machine-controlled character based on the time of the received player interruption, selecting a location in a dialog tree for the machine-controlled character based on the time of the received player interruption, or both.;2
12;7720683;2004;2010;Sensory, Incorporated;Sensory, Incorporated;G10L15/22,G10L15/285,G10L2015/0631;Method and apparatus of specifying and performing speech recognition operations;A speech recognition technique is described that has the dual benefits of not requiring collection of recordings for training while using computational resources that are cost-compatible with consumer electronic products. Methods are described for improving the recognition accuracy of a recognizer by developer interaction with a design tool that iterates the recognition data during development of a recognition set of utterances and that allows controlling and minimizing the computational resources required to implement the recognizer in hardware.;0
13;7732697;2007;2010;;;G10H1/0041,G10H1/0025,G10H2210/141,G10H2240/131;Creating music and sound that varies from playback to playback;"A method and apparatus for the creation and playback of music and/or sound; such that each time a composition is played back, a different sound sequence may be generated. In one embodiment, during composition creation, artist(s) may define how the composition may vary from playback to playback using visually interactive display(s). The artist's definition may be embedded into a composition dataset. During playback, a composition data set may be processed by a playback device and/or a playback program, so that each time the composition is played-back a unique version may be generated. Variability during playback may include: the variable selection of alternative sound segment(s); variable editing of sound segment(s) during playback processing; variable placement of sound segment(s) during playback processing; the spawning of group(s) of alternative sound segments from initiating sound segment(s); and the combining and/or mixing of alternative sound segments in one or more sound channels. MIDI-like variable compositions and the variable use of sound segments comprised of MIDI-like command sequences are also disclosed.";3
14;7742919;2005;2010;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L13/06,G10L15/187;System and method for repairing a TTS voice database;The present invention provides various elements of a toolkit used for generating a TTS voice for use in a spoken dialog system. The embodiments in each case may be in the form of the system, a computer-readable medium, or a method for generating the TTS voice. One embodiment of the invention relates to a method of correcting a database associated with the development of a text-to-speech (TTS) voice. The method comprises generating a pronunciation dictionary for use with a TTS voice, generating a TTS voice to a stage wherein it is prepared to be tested before being deployed, identifying mislabeled phonetic units associated with the TTS voice, for each identified mislabeled phonetic unit, linking to an entry within the pronunciation dictionary to correct the entry and deleting utterances and all associated data for unacceptable utterances.;1
15;7761296;1999;2010;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G10L15/08,G10L13/02,G10L15/10;System and method for rescoring N-best hypotheses of an automatic speech recognition system;A system and method for rescoring the N-best hypotheses from an automatic speech recognition system by comparing an original speech waveform to synthetic speech waveforms that are generated for each text sequence of the N-best hypotheses. A distance is calculated from the original speech waveform to each of the synthesized waveforms, and the text associated with the synthesized waveform that is determined to be closest to the original waveform is selected as the final hypothesis. The original waveform and each synthesized waveform are aligned to a corresponding text sequence on a phoneme level. The mean of the feature vectors which align to each phoneme is computed for the original waveform as well as for each of the synthesized hypotheses. The distance of a synthesized hypothesis to the original speech signal is then computed as the sum over all phonemes in the hypothesis of the Euclidean distance between the means of the feature vectors of the frames aligning to that phoneme for the original and the synthesized signals. The text of the hypothesis which is closest under the above metric to the original waveform is chosen as the final system output.;1
16;7788262;2007;2010;;;G06F16/36;Method and system for creating context based summary;A method and system for generating electronic data summary based on context and semantics of the document and its subsequent usage is disclosed. The method and system provides for generating a taxonomy of concepts, assigning unique-identifiers and weights to the taxonomy concepts using a given corpus of electronic data, using the taxonomy to identify the semantics of the document to be summarized, selecting text from a given document based on the weights of unique-identifiers in the taxonomy, generation of a list of semantic-structures and pruning of the list based upon an entropy threshold, generation of summary from the list of semantic-structures, generation of a topic summary from a given list of similar summaries, publishing of the summary in a known format, and usage of the summary to prevent denial of information.;2
17;7835910;2003;2010;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L15/063,G10L2015/0635,G10L2015/0636;Exploiting unlabeled utterances for spoken language understanding;A system and method for exploiting unlabeled utterances in the augmentation of a classifier model is disclosed. In one embodiment, a classifier is initially trained using a labeled set of utterances. Another set of utterances is then selected from an available set of unlabeled utterances. In one embodiment, this selection process can be based on a confidence score threshold. The trained classifier is then augmented using the selected set of unlabeled utterances.;0
18;7890374;2000;2011;Rovi Technologies Corporation;Rovi Technologies Corporation;G06Q30/02,G06Q30/06,G06Q30/0633;System and method for presenting music to consumers;The present invention provides a computer-implemented method and system for providing user requested music. The method comprises of receiving user input that defines a plurality of music search parameters. Some of the music search parameters may describe music content attributes. Thereafter, the process searches for music samples based upon the user provided search parameters. The process then presents music samples to the user.The method also determines if the user wants to buy any of the presented music samples, a set of music samples similar to the music purchased and/or a set of music samples different than the music bought.;0
19;7902447;2006;2011;SONY COMPUTER ENTERTAINMENT INC.;SONY COMPUTER ENTERTAINMENT INC.;G10H1/0025;Automatic composition of sound sequences using finite state automata;In one embodiment, a method for the automatic composition of music is disclosed. The method begins by receiving a plurality of input sound sequences containing sound frequencies with corresponding time duration. The method continues with converting the plurality of input sound sequences to a finite state automaton using a system that allows over-generation, followed by receiving exploration rules that constrain how the finite state automaton is to be traversed. The next step is creating a path marker data structure indexing a plurality of path markers, where each path marker contains a path marker history and a path marker registry. After the path marker data structure is created, the method continues by traversing the finite state automaton with a graph exploration procedure that uses the exploration rules and the plurality of path markers to determine paths across the finite state automaton. During the exploration the path marker history and the path marker registry of particular path markers are updated when traversing the finite state automaton. As the finite state automaton is traversed the method includes storing the paths across the finite state automaton to the path marker data structure to define recorded path markers, wherein the recorded path markers that are not found in the plurality of input sound sequences define new music compositions.;3
20;7912718;2006;2011;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L13/06;Method and system for enhancing a speech database;A system, method and computer readable medium that enhances a speech database for speech synthesis is disclosed. The method may include labeling audio files in a primary speech database and a secondary speech database, enhancing the primary speech database by placing the labeled audio files from the secondary speech database into the primary speech database, and storing the enhanced primary speech database for use in speech synthesis.;0
21;7925496;2007;2011;The United States of America as represented by the Secretary of the Navy;The United States of America as represented by the Secretary of the Navy;G06F16/345;Method for summarizing natural language text;A method includes the steps of comparing a first body of text with a user-created summary of the first body of text, creating rules based on the comparison of the first body of text with the user-created summary of the first body of text, selecting one or more summary rules for generating a computer-created summary of a second body of text, and applying the selected summary rules to the second body of text to generate a computer-created summary of the second body of text. The first body of text may be a user-corrected summary of a computer-created summary of the first body of text. The rules may be selected based on previous use, frequency of use, context of the body of text, or most-specific applicability. The rules may be iteratively applied to generate a summary. A method is also provided for generating a heading for a summary of text.;2
22;7945952;2005;2011;GOOGLE LLC;GOOGLE LLC;H04L63/08,G06F21/316,G06F2221/2103,G06F2221/2105;Methods and apparatuses for presenting challenges to tell humans and computers apart;Methods and apparatuses to tell humans and machines apart. In one embodiment, automated determination of whether a human or machine is at the other end of a communication channel is based on whether the correct response to a challenge/question is received from the other end of the communication channel. The challenge is provided in a non-image format, such as a text based questions or a sound clip, to prevent abuses of the communication system by computer software robots. The communication system may be primarily for text and/or voice communications, or be such that the rendering image-based challenges can cause significant disruption in normal flow of usage. In one example, the challenge includes deliberately generated random misspelling. In one example, the challenge is context sensitive, factual based, and/or instructive of an operation in a natural language to obtain a result.;0
23;7962530;2008;2011;;;G06F16/68,G06F16/40,G06F16/634,G06F16/683;Method for locating information in a musical database using a fragment of a melody;A system and method for retrieving musical information from a database based on a melody fragment. The system may be used by a person without formal musical training to retrieve musical data (e.g., the name of a song, bibliographic information about a song, or the song itself) from a database by providing the melody or fragment of the melody of the desired music to a computer interface. The melody may be provided by singing, humming, whistling, or playing a musical instrument, for example, a keyboard. The inputted melodic information is encoded using relative changes in pitch and rhythm throughout the melody. The encoded information is then compared using a matching algorithm to similarly encoded melodic information representative of many musical pieces (e.g., songs). Results may also be sorted using a weighted algorithm.;1
24;8032383;2007;2011;FoneWeb, Inc.;FoneWeb, Inc.;G10L15/30,G10L2015/223,G10L2015/228;Speech controlled services and devices using internet;A speech service, including a speech-to-text engine and a text-to-speech engine, creates and maintains user profiles at a central location accessible over the Internet. A user connects to a software application over a mobile telephone and delivers a voice command. The speech service transcribes the voice command into a text command for the software application. The software application performs a service desired by the user and delivers a text result to the speech service that is converted into a speech result that is delivered to the user. A user speaks to a hardware device to perform a function. The hardware device sends the speech to the speech service over the Internet that transcribes the speech into a text command that is sent over the Internet to a device service provider. The device service provider maps the text command into a device command that is then sent back over the Internet to the hardware device to perform the function. A remote hardware device can be controlled using the software application.;1
25;8041565;2007;2011;FoneWeb, Inc.;FoneWeb, Inc.;G10L15/28;Precision speech to text conversion;"A speech-to-text conversion module uses a central database of user speech profiles to convert speech to text. Incoming audio information is fragmented into numerous audio fragments based upon detecting silence. The audio information is also converted to numerous text files by any number of speech engines. Each text file is then fragmented into numerous text fragments based upon the boundaries established during the audio fragmentation. Each set of text fragments from the different speech engines corresponding to a single audio fragments is then compared. The best approximation of the audio fragment is produced from the set of text fragments; a hybrid may be produced. If no agreement is reached, the audio fragment and set the text fragments are sent to human agents who verify and edit to produce a final edited text fragment that best corresponds to the audio fragment. Fragmentation that produces overlapping audio fragments requires splicing of the final text fragments to produce the output text file.";1
26;8044288;2009;2011;;;G09B15/00,G09B15/023;Proprietary music rotation supporting the equal temperament tuning system;"A musical notation system is provided wherein the twelve pitches of the 12-TET tuning system are represented by horizontal lines placed at the top, center and bottom of four different reference heads. Pitches, intervals chords and scales degrees are named by the positions of these lines at the reference heads and by the relationship between them. Assistant is provided to performers by representing the correlation between these reference heads and positions or keys in the musical instruments as well as a method to aid identify intervals, scale degrees and pitches. Relative note-on is represented in a timing row with demarcation of beats and measures; note-off is represented by the end of the lines as a proportional indication of the duration of sound.";0
27;8065089;2006;2011;The University Of North Carolina At Charlotte;The University Of North Carolina At Charlotte;G16B25/10,G01N33/5008,G01N33/5041,G16B5/30,G16B25/00,G16B40/20,G16B40/30,G16B5/00,G16B40/00;Methods and systems for analysis of dynamic biological pathways;Disclosed is a hierarchical computational method to predict the expression value of genes in time-series microarray data. The method may include the step of first applying a nonlinear independent component analysis (NICA) algorithm that extracts the major components covering all considered genes. An autoregressive exogenous (ARX) model may subsequently be used to quantitatively express the dynamic interactions of all components with each other. Then, using the predicted values for the components, and the nonlinear independent component analysis in the inverse form, the data may be used to predict the expression parameters for individual genes. The method may be used for the analysis of a eukaryotic gene expression throughout the cell cycle.;0
28;8069131;2010;2011;;;G06Q10/10,G06Q10/06;Method, artificially intelligent system and networked complex for facilitating group interactions;An artificially intelligent or rule-based system to assist teams or groups become more effective by improving the communication process between members of the team or group. The system helps members share information, negotiate more effectively and make better group decisions. The system is designed to allow users to generate new interventions and rule sets which are then vetted by all users of the system.;1
29;8103505;2003;2012;Apple Inc.;Apple Inc.;G10L13/033,G10L13/10;Method and apparatus for speech synthesis using paralinguistic variation;A method and apparatus for speech synthesis in a computer-user interface using random paralinguistic variation is described herein. According to one aspect of the present invention, a method for synthesizing speech comprises generating synthesized speech having certain prosodic features. The synthesized speech is further processed by applying a random paralinguistic variation to the acoustic sequence representing the synthesized speech without altering the linguistic prosodic features. According to one aspect of the present invention, the application of the paralinguistic variation is correlated with a previously applied paralinguistic variation to reflect a gradual change in the computer voice, while still maintaining a random quality.;1
30;8135260;2009;2012;;;G11B27/034;Video generation system;Described is a video generation system that allows a user to be filmed and merged into a premade video sequence. The system is configured to receive a video having a plurality of characters, each character having a corresponding audio track. A user can remove one of characters and corresponding audio track from the video, leaving remaining characters and remaining audio track. The video with the remaining characters and remaining audio track is then played on a display. Using a sensor and microphone, a user video image and corresponding user audio track is received. The user video image and user audio track is recorded as it is received while the video with remaining characters and corresponding audio track is played on the display. Thus, once merged, the user video image and user audio track can be played along with the video with the remaining characters and the remaining audio track.;6
31;8180725;2008;2012;GOOGLE LLC;GOOGLE LLC;G06F16/353,G06F16/9027,G06F16/951,G06N5/02,G06F16/3344,G06F40/253,G06F40/30;Method and apparatus for selecting links to include in a probabilistic generative model for text;"Some embodiments of the present invention provide a system that selects links while updating a probabilistic generative model for textual documents. During operation, the system receives a current model, which contains terminal nodes representing words and cluster nodes representing clusters of conceptually related words, wherein nodes in the current model are coupled together by weighted links, wherein if a node fires, a link from the node to another node is activated and causes the other node to fire with a probability proportionate to the weight of the link. Next, the system applies a set of training documents containing words to the current model to produce a new model. While doing so, the system: determines expected counts for activations of links and prospective links; determines link-ratings for the links and the prospective links based on the expected counts, and selects links to be included in the new model based on the determined link-ratings. Finally, the system makes the new model the current model.";3
32;8183451;2009;2012;STC.UNM;STC.UNM;G10H1/0025,G10H1/40,G10H2210/031,G10H2220/351,G10H2220/371;System and methods for communicating data by translating a monitored condition to music;A system and methods for continuously communicating data regarding the status of a monitored condition using music, which trained persons can recognize and interpret. One or more data collector devices monitor conditions and provide data regarding the status of the conditions to an analyzing device. The analyzing device receives the data and creates data music that is played on an audio device with reference music establishing the Hierarchal Music Structure (HMS) to the listener. The data music is a musical representation of the data against the reference music, which are played on an audio device.;1
33;8185400;2005;2012;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L15/22,G10L15/19;System and method for isolating and processing common dialog cues;A method, system and machine-readable medium are provided. Speech input is received at a speech recognition component and recognized output is produced. A common dialog cue from the received speech input or input from a second source is recognized. An action is performed corresponding to the recognized common dialog cue. The performed action includes sending a communication from the speech recognition component to the speech generation component while bypassing a dialog component.;2
34;8190460;2000;2012;CCI Europe A/S;CCI Europe A/S;G06Q10/0631,G06Q30/0276,G06Q30/0635;Advertisement sales and management system;An advertisement sales and management system comprising data storage means, data retrieval means, data processing means, database means, and a number of workstations, performing the tasks of: entering advertisement orders, entering advertisement associated-data, and entering customer associated-data. The database means and the data processing means storing and managing data relating to customers, an advertisement order comprising data related to a group of related advertisements including: a medium in which the advertisement is published, schedules defining insertions, contents of advertisements, and presentation elements of respective advertisement. The advertisement sales and management system generating data defining the advertisement based on entered order data, constituting a single system supporting entry, sales and management of advertisements in media, a WYSIWYG editor supporting entering or editing contents and/or presentation elements of the advertisements, and displaying on a screen the contents and presentation elements in the graphical representation defined by the data.;0
35;8204842;2010;2012;THE RESEARCH FOUNDATION OF STATE UNIVERSITY OF NEW YORK;THE RESEARCH FOUNDATION OF STATE UNIVERSITY OF NEW YORK;G06F16/5846,G06F16/5838,G06F18/24155,G06N5/02,G06N7/01;System and method for image annotation and multi-modal image retrieval using probabilistic semantic models comprising at least one joint probability distribution;Systems and Methods for multi-modal or multimedia image retrieval are provided. Automatic image annotation is achieved based on a probabilistic semantic model in which visual features and textual words are connected via a hidden layer comprising the semantic concepts to be discovered, to explicitly exploit the synergy between the two modalities. The association of visual features and textual words is determined in a Bayesian framework to provide confidence of the association. A hidden concept layer which connects the visual feature(s) and the words is discovered by fitting a generative model to the training image and annotation words. An Expectation-Maximization (EM) based iterative learning procedure determines the conditional probabilities of the visual features and the textual words given a hidden concept class. Based on the discovered hidden concept layer and the corresponding conditional probabilities, the image annotation and the text-to-image retrieval are performed using the Bayesian framework.;6
36;8214213;2006;2012;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L15/187,G10L15/063;Speech recognition based on pronunciation modeling;A system and method for performing speech recognition is disclosed. The method comprises receiving an utterance, applying the utterance to a recognizer with a language model having pronunciation probabilities associated with unique word identifiers for words given their pronunciations and presenting a recognition result for the utterance. Recognition improvement is found by moving a pronunciation model from a dictionary to the langue model.;0
37;8222507;2009;2012;Smule, Inc.;Smule, Inc.;G10H1/0033,G10H1/0008,G10H2220/096,G10H2220/361,G10H2220/395,G10H2240/175,G10H2240/251;System and method for capture and rendering of performance on synthetic musical instrument;Techniques have been developed for capturing and rendering musical performances on handheld or other portable devices using signal processing techniques suitable given the somewhat limited capabilities of such devices and in ways that facilitate efficient encoding and communication of such captured performances via wireless networks. The developed techniques facilitate the capture, encoding and use of gesture streams for rendering of a musical performance. In some embodiments, a gesture stream encoding facilitates audible rendering of the musical performance locally on the portable device on which the musical performance is captured, typically in real time. In some embodiments, a gesture stream efficiently codes the musical performance for transmission from the portable device on which the musical performance is captured to (or toward) a remote device on which the musical performance is (or can be) rendered. Indeed, is some embodiments, a gesture stream so captured and encoded may be rendered both locally and on remote devices using substantially identical or equivalent instances of a digital synthesis of the musical instrument executing on the local and remote devices.;1
38;8239200;2008;2012;GOOGLE LLC;GOOGLE LLC;G06F40/45,G10L15/183;Delta language model;Among other disclosed subject matter, a computer-implemented method for generating a language model includes generating a language model based on a data set, the data set including at least training data and test data. The test data is to be used in testing the language model. The method includes identifying the test data in the data set. The method includes generating a delta language model based on at least the test data, the delta language model configured for evaluating the test data.;1
39;8265930;2005;2012;SPRINT COMMUNICATIONS COMPANY, L.P.;SPRINT COMMUNICATIONS COMPANY, L.P.;G06Q10/107,G10L15/26;System and method for recording voice data and converting voice data to a text file;The present invention relates to recording voice data using a voice communication device connected to a communication network and converting the voice data into a text file for delivery to a text communication device. In accordance with the present invention, the voice communication device may transfer the voice data in real-time or store the voice data on the device to be transmitted at a later time. Transcribing the voice data into a text file may be accomplished by automated computer software, either speaker-independent or speaker-dependent or by a human who transcribes the voice data into a text file. After transcribing the voice data into a text file, the text file may be delivered to a text communication device in a number of ways, such as email, file transfer protocol (FTP), or hypertext transfer protocol (HTTP).;1
40;8296257;2009;2012;GOOGLE LLC;GOOGLE LLC;G06N20/00,G06F16/35,G06F16/358,G06F16/93;Comparing models;Methods, systems and computer program products for evaluating performance of generative models are disclosed. One method includes providing a base model and a candidate model having observed variables and first and second conceptually related variables related to the observed variables, respectively, receiving observations assigned to a subset of the observed variables, and for each observation, evaluating the observation by the base model to produce a base assessment of the observation, evaluating the observation by the candidate model to produce a second assessment of the observation, determining a similarity measure of the assessment of the observation based on the base and second assessments, and selecting a subset of observations having low similarity measures for use in evaluating performance of the candidate model.;0
42;8355903;2010;2013;NORTHWESTERN UNIVERSITY;NORTHWESTERN UNIVERSITY;G06F40/20,G06F16/22,G06F16/2365,G06F40/106,G06F40/166,G06F40/289,G06Q10/10;System and method for using data and angles to automatically generate a narrative story;A system and method for automatically generating a narrative story receives data and information pertaining to a domain event. The received data and information and/or one or more derived features are then used to identify a plurality of angles for the narrative story. The plurality of angles is then filtered, for example through use of parameters that specify a focus for the narrative story, length of the narrative story, etc. Points associated with the filtered plurality of angles are then assembled and the narrative story is rendered using the filtered plurality of angles and the assembled points.;2
43;8374848;2010;2013;NORTHWESTERN UNIVERSITY;NORTHWESTERN UNIVERSITY;G06F40/40,G06F40/56;System and method for using data and derived features to automatically generate a narrative story;A system and method for automatically generating a narrative story receives data and information pertaining to a domain event. The received data and information and/or one or more derived features are then used to identify a plurality of angles for the narrative story. The plurality of angles is then filtered, for example through use of parameters that specify a focus for the narrative story, length of the narrative story, etc. Points associated with the filtered plurality of angles are then assembled and the narrative story is rendered using the filtered plurality of angles and the assembled points.;2
44;8380504;2010;2013;SPRINT COMMUNICATIONS COMPANY, L.P.;SPRINT COMMUNICATIONS COMPANY, L.P.;G10L13/086,G10L15/005;Generation of voice profiles;Embodiments of the present invention provide systems, methods, and computer-readable media for generating a voice characteristic profile based on detected sound components. In embodiments, a call is initiated between a first caller and a second caller. Information communicated during the call is monitored to determine that sound components have been spoken by the first caller. The sound components are determined to be associated with a language dialect. Further, the sound components are stored in association with the first caller. In particular, the sound components are stored in association with the first caller in a voice characteristic profile of the first caller.;0
45;8407113;2010;2013;INTUIT INC.;INTUIT INC.;G06Q40/123;Inference-based tax preparation;A method for operating a tax inference engine involves receiving tax data items, assigning confidence ratings to the tax data items using inference rules, and assigning relevance ratings to tax questions using the inference rules. The method further involves determining tax questions having ratings that exceed a threshold amount, and ordering the tax questions by the relevance ratings, personalizing a tax question using the inference rules, tax data items, and confidence ratings by altering a wording of the tax question, and determining tax advice using the inference rules, the tax data items, and the confidence ratings.;0
46;8422798;2007;2013;Science Applications International Corporation;Science Applications International Corporation;G06T9/007;Magnitude image compression;Embodiments of the invention view image intensity data as a spectrum of underlying wave forms. The spectrum of these waves can be approximated using spectral estimation techniques where the spectrum parameters constitute the compressed image. The image's underlying wave forms can be recovered using an inverse Fourier transform. The original image can also be symmetrically extended prior to the transform to preserve real valued transformed data and model parameters. The modeling method is typically based on a linear predictive methodology to obtain the spectrum parameters. Other transforms include a 2-D Fourier transform that transforms the image into a holographic representation similar to synthetic aperture radar (SAR) phase history. This 2-D waveform holographic format can be decorrelated into 1-D planar waves by applying a 1D Fourier transform. This process enables 1-D linear predictive modeling to obtain the spectral parameters. For compression applications the model parameters are preferably quantized.;2
47;8422799;2007;2013;Science Applications International Corporation;Science Applications International Corporation;G06T9/004,G06T9/001,G06T9/007;Complex image compression;"Complex image data is transformed to a holographic representation of the image. A subset of the holographic representation is modeled. Model parameters constitute a compressed image representation. A two-dimensional Fourier transform can be applied to obtain the holographic image. Modeling includes applying an analysis portion of an adaptive analysis/synthesis linear prediction methodology to a subset of the holographic representation to obtain an autoregressive model. Prior to modeling, one-dimensional Fourier transform can be performed on the holographic representation and the linear prediction is one-dimensional. Model parameters are preferably quantized. Embodiments include determining error between the model and the model's input data. There the compressed image representation the error, which also can be quantized. The subset of the holographic representation can be less than all the representation. The subset can be a plurality of complete rows; preferably substantially symmetric about 0 Hz.";2
48;8423366;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L13/06,G10L15/26,G10L15/30;Automatically training speech synthesizers;A method includes receiving, by a system, a voice recording associated with a user, transcribing, the voice recording into text that includes a group of words, and storing an association between a portion of each respective word and a corresponding portion of the voice recording. The corresponding portion of the voice recording is the portion of the voice recording from which the portion of the respective word was transcribed. The method may also include determining a modification to a speech synthesis voice associated with the user based at least in part on the association.;1
49;8428228;2012;2013;Weerawan Wongmanee;Weerawan Wongmanee;H04M1/7243,G06Q10/10,H04M2250/06,H04M2250/60,H04M2250/74;Unified communication system;Unified communication system for consolidating different message sources (e.g. emails, SMS, voice message, fax, etc.) into a single accessible location with the ability to incorporate phone calls, VoIP calls, voice commands, etc. This is achieved through integration of a Speech-ready IVR system with an E-mail client to act as the Unified Communication System. Furthermore, the IVR flexibility allows the user to access messages when internet access is not available or read messages such as SMS and emails when not physically convenient e.g. when user is driving. A computer based server receives a message in the format of SMS, fax, e-mail, or phone message. The server identifies what format the message is in and displays a notification on a graphical user interface. Through integration with a speech ready IVR system user may select to listen to the message or read the message.;0
50;8438029;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L13/08,G10L13/033;Confidence tying for unsupervised synthetic speech adaptation;Disclosed are apparatus and methods for generating synthesized utterances. A computing device can receive speech data corresponding to spoken utterances of a particular speaker. Textual elements of an input text corresponding to the speech data can be recognized. Confidence levels associated with the recognized textual elements can be determined. Speech-synthesis parameters of decision trees can be adapted based on the speech data, recognized textual elements, and associated confidence levels. Each adapted decision tree can map individual elements of a text to individual of the speech-synthesis parameters. A second input text can be received. The second input text can be mapped to speech-synthesis parameters using the adapted decision trees. A synthesized spoken utterance can be generated corresponding to the second input text using the speech-synthesis parameters. At least some of the speech-synthesis parameters are configured to simulate the particular speaker.;5
51;8447713;2012;2013;;;G06N7/01,G06N3/042,G06Q50/18;Automated legal evaluation using a neural network over a communications network;A method for legal knowledge modeling and automated legal evaluation, such as for online, questionnaire-based legal analysis, is provided. Information, such as facts and characteristics of a legal situation, as it relates to a legal conclusion, are modeled in an artificial neural network. The artificial neural network may comprise a plurality of nodes, wherein each node is associated with one or more weights and a function that calculates a legal conclusion based on one or more input values and the weights. The artificial neural network is automatically updated on a periodic basis to reflect new legislation or court decisions. Using the artificial neural network, a legal conclusion based on the user's answers to a questionnaire may be determined. The legal conclusion is modified upon the input of evidence, which is in the form of answers to a set of questions designed to identify a legal conclusion.;1
52;8467524;2010;2013;SPRINT COMMUNICATIONS COMPANY, L.P.;SPRINT COMMUNICATIONS COMPANY, L.P.;H04M1/2471,H04M1/6008;Mute indicator;Embodiments of the present invention provide systems, methods, and computer-readable media for presenting a mute indicator to a caller when the caller is attempting to provide input to a call that has been muted. In particular, a determination is made that a mute function is engaged during a call. An area surrounding a calling system, such as a phone, is then monitored by a monitoring system to determine whether a caller is attempting to provide input to the call. A determination is then made that a caller is attempting to provide input to the call. The determination may be based on a caller providing a threshold volume of voice input within a threshold area of proximity near the calling system. A mute indicator is then presented to the caller to notify the caller that the call has a mute function engaged.;0
53;8484022;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L15/063,G06N3/045,G06N3/084,G10L19/0018,G10L15/142,G10L15/16,G10L2015/025,G10L2015/0631;Adaptive auto-encoders;A method and system for adaptive auto-encoders is disclosed. An input audio training signal may be transformed into a sequence of feature vectors, each bearing quantitative measures of acoustic properties of the input audio training signal. An auto-encoder may process the feature vectors to generate an encoded form of the quantitative measures, and a recovered form of the quantitative measures based on an inverse operation by the auto-encoder on the encoded form of the quantitative measures. A duplicate copy of the sequence of feature vectors may be normalized to form a normalized signal in which supra-phonetic acoustic properties are reduced in comparison with phonetic acoustic properties of the input audio training signal. The auto-encoder may then be trained to compensate for supra-phonetic features by reducing the magnitude of an error signal corresponding to a difference between the normalized signal and the recovered form of the quantitative measures.;1
54;8487176;2010;2013;;;G10H1/0066,G10H1/0025,G10H1/0041,G10H7/00,G10H2210/115,G10H2210/141,G10H2240/131,H04R5/04;Music and sound that varies from one playback to another playback;"A method and apparatus for the creation and playback of music and/or sound, so that sound sequences are generated that vary from one playback to another playback. In one embodiment, during composition creation, artist(s) may define how the composition may vary from playback to playback using visually interactive display(s). The artist's definition may be embedded into a composition dataset. During playback, a composition data set may be processed by a playback device and/or a playback program, so that each time the composition is played-back a unique version may be generated. Variability during playback may include: the variable selection of alternative sound segment(s); variable editing of sound segment(s) during playback processing; variable placement of sound segment(s) during playback processing; the spawning of group(s) of alternative sound segments from initiating sound segment(s); and the combining and/or mixing of alternative sound segments in one or more sound channels. MIDI-like variable compositions and the variable use of sound segments comprised of a timed sequence of MIDI-like commands are also disclosed.";5
55;8497859;2012;2013;GOOGLE LLC;GOOGLE LLC;G06Q30/0241,G06Q30/0276,G06T19/00,G06F3/14,G09G3/003,G09G2370/027;Display of information on or within a three-dimensional image;Methods and systems for displaying information on or within a three-dimensional (3D) image are described. In an example, a computing device may be configured to determine, based on a two-dimensional (2D) content displayed on a display device, an object depicted in the 2D content. The computing device may be configured to receive information indicative of a request to initiate a three-dimensional (3D) image viewer to view the object in the 3D image viewer. The computing device, accordingly, may be configured to provide rendering information associated with instructions for rendering in the 3D image viewer a 3D image representing a 3D object data model of the object on the display device. The computing device may be configured to provide, in the 3D image viewer, information relating to the object and including at least a portion of the 2D content.;4
56;8510112;2006;2013;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L13/06,G10L2021/0135;Method and system for enhancing a speech database;A system, method and computer readable medium that enhances a speech database for speech synthesis is disclosed. The method may include labeling audio files in a primary speech database, identifying segments in the labeled audio files that have varying pronunciations based on language differences, modifying the identified segments in the primary speech database using selected mappings, enhancing the primary speech database by substituting the modified segments for the corresponding identified database segments in the primary speech database, and storing the enhanced primary speech database for use in speech synthesis.;1
57;8515750;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L17/14,G10L15/07,G10L15/26;Realtime acoustic adaptation using stability measures;Methods, systems, and computer programs encoded on a computer storage medium for real-time acoustic adaptation using stability measures are disclosed. The methods include the actions of receiving a transcription of a first portion of a speech session, wherein the transcription of the first portion of the speech session is generated using a speaker adaptation profile. The actions further include receiving a stability measure for a segment of the transcription and determining that the stability measure for the segment satisfies a threshold. Additionally, the actions include triggering an update of the speaker adaptation profile using the segment, or using a portion of speech data that corresponds to the segment. And the actions include receiving a transcription of a second portion of the speech session, wherein the transcription of the second portion of the speech session is generated using the updated speaker adaptation profile.;3
58;8527276;2012;2013;GOOGLE LLC;GOOGLE LLC;G06N3/084,G10L13/02,G10L25/30;Speech synthesis using deep neural networks;A method and system for is disclosed for speech synthesis using deep neural networks. A neural network may be trained to map input phonetic transcriptions of training-time text strings into sequences of acoustic feature vectors, which yield predefined speech waveforms when processed by a signal generation module. The training-time text strings may correspond to written transcriptions of speech carried in the predefined speech waveforms. Subsequent to training, a run-time text string may be translated to a run-time phonetic transcription, which may include a run-time sequence of phonetic-context descriptors, each of which contains a phonetic speech unit, data indicating phonetic context, and data indicating time duration of the respective phonetic speech unit. The trained neural network may then map the run-time sequence of the phonetic-context descriptors to run-time predicted feature vectors, which may in turn be translated into synthesized speech by the signal generation module.;5
59;8566102;2002;2013;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L15/22,G10L2015/228;System and method of automating a spoken dialogue service;"A system and method of generating and operating a spoken dialog service for a web-site are disclosed. The system parses web-site data and organizes the web-site data in a task knowledge data bank. The system receives text associated with a user query; processes the received text in a spoken language understanding (SLU) module, the SLU module using the web-site data from the task knowledge data bank; generates a ranked list of relevant responses to the user query; generates a hierarchical tree using the web-site data and the ranked list of relevant responses to the user query, generates a response to the user query using the hierarchical tree; and presents the response to the user.";2
60;8571859;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L17/00,G10L15/065,G10L15/07;Multi-stage speaker adaptation;A first gender-specific speaker adaptation technique may be selected based on characteristics of a first set of feature vectors that correspond to a first unit of input speech. The first set of feature vectors may be configured for use in automatic speech recognition (ASR) of the first unit of input speech. A second set of feature vectors, which correspond to a second unit of input speech, may be modified based on the first gender-specific speaker adaptation technique. The modified second set of feature vectors may be configured for use in ASR of the second unit of input speech. A first speaker-dependent speaker adaptation technique may be selected based on characteristics of the second set of feature vectors. A third set of feature vectors, which correspond to a third unit of speech, may be modified based on the first speaker-dependent speaker adaptation technique.;1
61;8571871;2012;2013;GOOGLE LLC;GOOGLE LLC;G10L13/033,G10L21/003;Methods and systems for adaptation of synthetic speech in an environment;Methods and systems for adaptation of synthetic speech in an environment are described. In an example, a device, which may include a text-to-speech (TTS) module, may be configured to determine characteristics of an environment of the device. The device also may be configured to determine, based on the one or more characteristics of the environment, speech parameters that characterize a voice output of the text-to-speech module. Further, the device may be configured to process a text to obtain the voice output corresponding to the text based on the speech parameters to account for the one or more characteristics of the environment.;5
62;8589164;2013;2013;GOOGLE LLC;GOOGLE LLC;G10L15/06,G06F16/24522,G06F16/3329,G06F40/289,G10L15/197,G10L15/14,G10L2015/0635;Methods and systems for speech recognition processing using search query information;Methods and systems for speech recognition processing are described. In an example, a computing device may be configured to receive information indicative of a frequency of submission of a search query to a search engine for a search query composed of a sequence of words. Based on the frequency of submission of the search query exceeding a threshold, the computing device may be configured to determine groupings of one or more words of the search query based on an order in which the one or more words occur in the sequence of words of the search query. Further, the computing device may be configured to provide information indicating the groupings to a speech recognition system.;0
63;8589262;2011;2013;INTUIT INC.;INTUIT INC.;G06F40/237;Technique for computing relevancy between tax information;The disclosed embodiments relate to a computer system that facilitates identifying additional tax information. During operation, the computer system receives tax information. Next, the computer system determines relevance of the additional tax information to the tax information using a metric indicative of a statistical relationship between the tax information and the additional tax information in a tax-information data structure. This metric is based on co-existence of the tax information and the additional tax information at different hierarchical levels in income-tax documents. Moreover, the tax-information data structure includes tax information and associated context information from income-tax documents. Next, the computer system identifies the additional tax information based on the relevance and a threshold, and the computer system provides the additional tax information.;0
64;8596640;2012;2013;;;G09B19/00,A63F3/00006,A63F3/04,G09B1/00,G09B19/22,A63F1/04,A63F3/00,A63F9/18,A63F2001/0475,A63F2003/00794,A63F2003/00858,A63F2009/2433,A63F2009/2486;Storytelling game and method of play;"A method of play, wherein play emphasizes storytelling and story recounting abilities, and wherein the method comprises: (a) providing a gaining device which provides for: a plurality of subject elements, wherein each subject element from the plurality of subject elements comprises a topic for a particular story; and a plurality of situational elements, wherein each situational element from the plurality of situational elements qualifies the subject element; (b) pairing a subject element from the plurality of subject elements with a situational element from the plurality of situational elements; and (c) providing a story based on the paired subject element and selected situational element.";0
65;8600753;2005;2013;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L13/00,G10L13/08,G10L13/10;Method and apparatus for combining text to speech and recorded prompts;An arrangement provides for improved synthesis of speech arising from a message text. The arrangement stores prerecorded prompts and speech related characteristics for those prompts. A message is parsed to determine if any message portions have been recorded previously. If so then speech related characteristics for those portions are retrieved. The arrangement generates speech related characteristics for those parties not previously stored. The retrieved and generated characteristics are combined. The combination of characteristics is then used as the input to a speech synthesizer.;5
66;8606577;2013;2013;GOOGLE LLC;GOOGLE LLC;G10L15/26,G06F3/167;Visual confirmation of voice recognized text input;A computing device receives an audio input from a user. The computing device determines a series of words from the audio input. The computing device outputs, for display, one or more substituted symbols. The one or more substituted symbols correspond to at least a portion of the series of words. In response to determining that receipt of the audio input has completed, the computing device outputs, for display, alphanumeric characters comprising the series of words in place of the one or more substituted symbols.;1
67;8630844;2012;2014;Narrative Science LLC;Narrative Science LLC;G06F40/279,G06F40/151,G06F40/186,G06F40/58;Configurable and portable method, apparatus, and computer program product for generating narratives using content blocks, angels and blueprints sets;The subject invention provides a method for using available data and information to automatically create narrative stories that describes domain events, circumstances and/or entities in a comprehensible and compelling and audience customized, manner. Computer executable instructions provide for generating a narrative story using standard and uniform structures and data for receiving domain related data and a story specification, parsing the story specification to provide constituent components, transforming the constituent components into executable code, instantiating content blocks having at least one feature for the domain according to the story specification and rendering the narrative story using the constituent components specified by the content blocks.;2
68;8630975;2011;2014;The Research Foundation for the State University of New York;The Research Foundation for the State University of New York;G06N20/00,G06F16/24573,G06F16/24578,G06F16/93,G06F16/9558,G06N7/01,G06Q10/10;Knowledge discovery from citation networks;In a corpus of scientific articles such as a digital library, documents are connected by citations and one document plays two different roles in the corpus: document itself and a citation of other documents. A Bernoulli Process Topic (BPT) model is provided which models the corpus at two levels: document level and citation level. In the BPT model, each document has two different representations in the latent topic space associated with its roles. Moreover, the multi-level hierarchical structure of the citation network is captured by a generative process involving a Bernoulli process. The distribution parameters of the BPT model are estimated by a variational approximation approach.;0
69;8644464;2012;2014;Xerox Corporation;Xerox Corporation;H04M19/041,H04M1/65,H04M1/6505,H04M1/72436,H04M1/72481;Method and system for creating a customized audio snippet;The disclosed embodiments related to a method and system for creating an audio snippet. The method includes receiving a call from a calling party on a communication device. The method further includes recording a first audio of the calling party at the communication device. The method further includes creating the audio snippet by using the first audio of the calling party and, one or more pre-stored second audio on the communication device on the basis of one or more techniques.;2
70;8650035;2005;2014;Verizon Laboratories Inc.;Verizon Laboratories Inc.;G10L21/013,G10L2021/0135;Speech conversion;A speech conversion system facilitates voice communications. A database comprises a plurality of conversion heuristics, at least some of the conversion heuristics being associated with identification information for at least one first party. At least one speech converter is configured to convert a first speech signal received from the at least one first party into a converted first speech signal different than the first speech signal.;1
71;8653349;2011;2014;Podscape Holdings Limited;Podscape Holdings Limited;G10H1/0025,G10H2210/305,G10H2220/106,G10H2220/131,G10H2240/131,G10H2240/175;System and method for musical collaboration in virtual space;A system and method for musical collaboration in virtual space is described. This method is based on the exchange of data relating to position, direction and selection of musical sounds and effects, which are then combined by a software application for each user. The musical sampler overcomes latency of data over the network by ensuring that all loops and samples begin on predetermined temporal divisions of a composition. The data is temporarily stored as a data file and can be later retrieved for playback or conversion into a digital audio file.;0
72;8660678;2010;2014;TONARA LTD.;TONARA LTD.;G10H1/0066,G10H2210/076,G10H2210/091,G10H2240/325,G10H2250/015;Automatic score following;A method for audio processing includes receiving in an electronic processor an audio input from a performance of a musical piece having a score. A two-dimensional state space is defined, including coordinates modeling the performance, each coordinate corresponding to a respective location in the score and a tempo of the performance. For each of a plurality of times during the performance, a probability distribution is computed over the two-dimensional state space based on the audio input. Based on the probability distribution, the performance is matched to the score.;0
73;8671149;2013;2014;;;H04L51/04,G06Q10/10,G06Q10/107,H04M1/7243,H04M3/493,H04M3/5307,H04M3/4936,H04M7/0048,H04M7/0054,H04M2201/52,H04M2250/06,H04M2250/60,H04M2250/74;Unified messaging platform with intelligent voice recognition (IVR);Unified communication system for consolidating different message sources (e.g. emails, SMS, voice message, fax, etc.) into a single accessible location with the ability to incorporate phone calls, VoIP calls, voice commands, etc. This is achieved through integration of a Speech-ready IVR system with an E-mail client to act as the Unified Communication System. Furthermore, the IVR flexibility allows the user to access messages when internet access is not available or read messages such as SMS and emails when not physically convenient e.g. when user is driving. A computer based server receives a message in the format of SMS, fax, e-mail, or phone message. The server identifies what format the message is in and displays a notification on a graphical user interface. Through integration with a speech ready IVR system user may select to listen to the message or read the message.;0
74;8688434;2010;2014;Narrative Science LLC;Narrative Science LLC;G06F40/56,G06F40/20;System and method for using data to automatically generate a narrative story;A system and method for automatically generating a narrative story receives data and information pertaining to a domain event. The received data and information and/or one or more derived features are then used to identify a plurality of angles for the narrative story. The plurality of angles is then filtered, for example through use of parameters that specify a focus for the narrative story, length of the narrative story, etc. Points associated with the filtered plurality of angles are then assembled and the narrative story is rendered using the filtered plurality of angles and the assembled points.;3
75;8700396;2012;2014;GOOGLE LLC;GOOGLE LLC;G10L13/027,G10L15/063,G10L2015/0638;Generating speech data collection prompts;This document generally describes computer technologies relating to generating speech data collection prompts, such as textual scripts and/or textual scenarios. Speech data collection prompts for a particular language can be generated based on a variety of factors, including the frequency with which linguistic elements (e.g., phonemes, syllables, words, phrases) in the particular language occur in one or more corpora of textual information associated with the particular language. Textual prompts can also and/or alternatively be generated based on statistics for previously recorded speech data.;2
76;8725559;2009;2014;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06Q90/00,G06Q30/0251;Attribute based advertisement categorization;System and methods for categorizing electronic advertisements categorized based on the attributes associated with the creatives, such as product type, predominant color, size of the create, media type of the creative are provided. Categories to be associated with electronic advertisements may be predefined, and an electronic advertisement may have various attributes associated with each of these categories.;0
77;8751236;2013;2014;GOOGLE LLC;GOOGLE LLC;G10L13/06;Devices and methods for speech unit reduction in text-to-speech synthesis systems;A device may receive a plurality of speech sounds that are indicative of pronunciations of a first linguistic term. The device may determine concatenation features of the plurality of speech sounds. The concatenation features may be indicative of an acoustic transition between a first speech sound and a second speech sound when the first speech sound and the second speech sound are concatenated. The first speech sound may be included in the plurality of speech sounds and the second speech sound may be indicative of a pronunciation of a second linguistic term. The device may cluster the plurality of speech sounds into one or more clusters based on the concatenation features. The device may provide a representative speech sound of the given cluster as the first speech sound when the first speech sound and the second speech sound are concatenated.;1
78;8764561;2012;2014;Kabam, Inc.;Kabam, Inc.;A63F13/5375,A63F13/25,A63F13/79,A63F13/795,A63F2300/556;System and method for providing targeted recommendations to segments of users of a virtual space;Targeted recommendations may be provided to specific user segments. The users may be segmented on or more user parameters that facilitate targeted provision of recommendations to the individual segments of users. A recommendation may prompt a user to take a recommended action in the game.;0
79;8768687;2013;2014;GOOGLE LLC;GOOGLE LLC;G10L15/26,G06F40/40,G10L15/1822,G10L2015/223;Machine translation of indirect speech;Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating direct speech messages based on voice commands that include indirect speech messages. In one aspect, a method includes receiving a voice input corresponding to an utterance. A determination is made whether a transcription of the utterance includes a command to initiate a communication to a user and a segment that is classified as indirect speech. In response to determining that the transcription of the utterance includes the command and the segment that is classified as indirect speech, the segment that is classified as indirect speech is provided as input to a machine translator. In response to providing the segment that is classified as indirect speech to the machine translator, a direct speech segment is received from the machine translator. A communication is initiated that includes the direct speech segment.;3
80;8775161;2011;2014;Narrative Science LLC;Narrative Science LLC;G06Q40/04,G06F16/3344,G06F16/335;Method and apparatus for triggering the automatic generation of narratives;Artificial intelligence methods and systems for triggering the generation of narratives are disclosed. Specific embodiments relate to real-time evaluation and automated generation of narrative stories based on received data. For example, data can be tested against data representative of a plurality of story angles to determine whether a narrative story incorporating one or more such story angles is to be automatically generated.;3
82;8799124;2011;2014;INTUIT INC.;INTUIT INC.;G06Q40/00,G06Q10/105,G06Q30/0631;Method and system for matching financial management system users with relevantly qualified accounting professionals;An accounting professional database is created that includes, but is not limited to, accounting professional profile data for one or more accounting professionals representing detailed information associated with the accounting professionals. A user of one or more financial management systems who may need the assistance of an accounting professional is identified and user need data indicating the detailed and specific accounting professional needs of the user is obtained. One or more matching criteria for the user are then determined/identified using the user need data and the one or more matching criteria for the user is used to search the accounting professional profile data in the accounting professional database to find one or more accounting professionals that match, or most closely match, the one or more matching criteria for the user.;0
83;8804035;2012;2014;DIRECTV, LLC;DIRECTV, LLC;H04N21/8126,H04H60/13,H04N21/440236,H04N21/4884;Method and system for communicating descriptive data in a television broadcast system;A method and system for communicating text descriptive data has a receiving device that receives a data signal having text description data corresponding to a description of a video signal. A text-to-speech converter associated with the receiving device converts the text description data to a first audio signal. A display device in communication with the text-to-speech converter converts the first audio signal associated with the receiving device to an audible signal.;0
84;8805684;2012;2014;GOOGLE LLC;GOOGLE LLC;G10L15/07;Distributed speaker adaptation;Automatic speech recognition (ASR) may be performed on received utterances. The ASR may be performed by an ASR module of a computing device (e.g., a client device). The ASR may include: generating feature vectors based on the utterances, updating the feature vectors based on feature-space speaker adaptation parameters, transcribing the utterances to text strings, and updating the feature-space speaker adaptation parameters based on the feature vectors. The transcriptions may be based, at least in part, on an acoustic model and the updated feature vectors. Updated speaker adaptation parameters may be received from another computing device and incorporated into the ASR module.;4
85;8818768;2010;2014;GOOGLE LLC;GOOGLE LLC;G06T19/20,G06F30/13,G06T17/05,G06T2210/04,G06T2219/2016;Modeling three-dimensional interiors from photographic images, and applications thereof;Embodiments relate to modeling three-dimensional interiors from photographic images. In a first embodiment, a computer-implemented method creates a three-dimensional model of an interior of a building from a photographic image. Input data is received from a user specifying a floor plan of the interior of the building. A first constraint input by a user indicating that a position on the floor plan corresponds to a position on the three-dimensional model is received. A second constraint input by a user indicating that a position on a photographic image of the interior corresponds to a position on the three-dimensional model is received. Finally, the three-dimensional model and camera parameters representing a camera that took the photographic image of the interior are altered based on the first and second constraints.;2
86;8818793;2003;2014;AT&T Intellectual Property I, L.P.;AT&T Intellectual Property I, L.P.;G10L15/063,G06F40/211,G06F40/284,G10L2015/0635;System and method of extracting clauses for spoken language understanding;A clausifier and method of extracting clauses for spoken language understanding are disclosed. The method relates to generating a set of clauses from speech utterance text and comprises inserting at least one boundary tag in speech utterance text related to sentence boundaries, inserting at least one edit tag indicating a portion of the speech utterance text to remove, and inserting at least one conjunction tag within the speech utterance text. The result is a set of clauses that may be identified within the speech utterance text according to the inserted at least one boundary tag, at least one edit tag and at least one conjunction tag. The disclosed clausifier comprises a sentence boundary classifier, an edit detector classifier, and a conjunction detector classifier. The clausifier may comprise a single classifier or a plurality of classifiers to perform the steps of identifying sentence boundaries, editing text, and identifying conjunctions within the text.;1
87;8886520;2011;2014;Narrative Science LLC;Narrative Science LLC;G06F16/345,G06F16/3344;Method and apparatus for triggering the automatic generation of narratives;Artificial intelligence methods and systems for triggering the generation of narratives are disclosed. Specific embodiments relate to real-time evaluation and automated generation of narrative stories based on received data. For example, data can be tested against data representative of a plurality of story angles to determine whether a narrative story incorporating one or more such story angles is to be automatically generated.;3
88;8892417;2011;2014;Narrative Science LLC;Narrative Science LLC;G06N5/02,G06F40/56;Method and apparatus for triggering the automatic generation of narratives;Artificial intelligence methods and systems for triggering the generation of narratives are disclosed. Specific embodiments relate to real-time evaluation and automated generation of narrative stories based on received data. For example, data can be tested against data representative of a plurality of story angles to determine whether a narrative story incorporating one or more such story angles is to be automatically generated.;3
89;8909534;2012;2014;GOOGLE LLC;GOOGLE LLC;G10L15/06,G10L15/22,G10L15/32,G10L2015/0638,G10L2015/221;Speech recognition training;A method may include selecting, by a computing device, sets of two or more text candidates from a plurality of text candidates corresponding to vocal input. The method may further include for each set, providing, by the computing device, representations of each of the respective two or more text candidates in the set to users, wherein the representations are provided as audio. The method may further include receiving a selection from each of the users of one of a text candidate from the set, wherein the selection is based on satisfying a criterion. The method may further include determining that a text candidate included in the plurality of text candidates has a highest probability out of the plurality of text candidates of being a correct textual transcription of the vocal input based at least in part on selections from the users.;1
90;8914284;2013;2014;Vonage Network, LLC;Vonage Network, LLC;H04M3/42391,H04M7/006,H04M2201/40,H04M2203/352,H04M2203/654;Methods and apparatus for conducting internet protocol telephony communication;IP telephony communications are conducted by sending both data produced by a CODEC that represents received spoken audio input, and a textual representation of the spoken audio input. A receiving device utilizes the textual representation of the spoken audio input to help recreate the spoken audio input when a portion of the CODEC data is missing. The textual representation can be generated by a speech-to-text function. Alternatively, the textual representation can be a notation of extracted phonemes.;1
91;8949125;2010;2015;GOOGLE LLC;GOOGLE LLC;G10L15/08,G10L13/02,G10L13/06,G10L2015/085;Annotating maps with user-contributed pronunciations;Systems and methods are provided to select a most typical pronunciation of a location name on a map from a plurality of user pronunciations. A server generates a reference speech model based on user pronunciations, compares the user pronunciations with the speech model and selects a pronunciation based on comparison. Alternatively, the server compares the distance between one the user pronunciations and every other user pronunciations and selects a pronunciation based on comparison. The server then annotates the map with the selected pronunciation and provides the audio output of the location name to a user device upon a user's request.;2
92;9047622;2011;2015;GOOGLE LLC;GOOGLE LLC;G06Q30/0256,G06Q30/02,G06Q30/0241;Delivering content to users based on advertisement interaction type;Systems and methods for improving the delivery of interactive advertisements are discussed herein. Systems and methods include approaches and solutions for boosting the rank of certain ads and/or ad types based on a combination of their interaction type and an ad interaction history for a given user, vertical, or aggregate indicator. Systems and methods also include suppressing ads unlikely to be interacted with from ranking and format boosting based on such ad interaction history.;0
93;9123339;2010;2015;GOOGLE LLC;GOOGLE LLC;G10L15/22,G10L15/10,G10L15/18,G10L2015/085;Speech recognition using repeated utterances;Subject matter described in this specification can be embodied in methods, computer program products and systems relating to speech-to-text conversion. A first spoken input is received from a user of an electronic device (an âoriginal utteranceâ). Based on the original utterance, a first set of character string candidates are determined that each represent the original utterance converted to textual characters and a selection of one or more of the character string candidates are provided in a format for display to the user. A second spoken input is received from the user and a determination is made that the second spoken input is a repeat utterance of the original utterance. Based on this determination and using the original utterance and the repeat utterance, a second set of character string candidates is determined.;2
94;9159329;2012;2015;GOOGLE LLC;GOOGLE LLC;G10L13/06,G10L13/04;Statistical post-filtering for hidden Markov modeling (HMM)-based speech synthesis;"A method and system for improving the quality of speech generated from Hidden Markov Model (HMM)-based Text-To-Speech Synthesizers using statistical post-filtering techniques. An example method involves: (a) determining a scale factor that, when applied to a synthesized reference spectral envelope, minimizes a statistical divergence between a natural reference spectral envelope and the synthesized reference spectral envelope, where the synthesized reference spectral envelope is generated by a state of an HMM; (b) for a given synthesized subject spectral envelope generated by the state of the HMM, determining an enhanced synthesized subject spectral envelope based on the determined scale factor; and (c) generating, by a computing device, a synthetic speech signal including the enhanced synthesized subject spectral envelope.";3
96;9240178;2014;2016;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/04;Text-to-speech processing using pre-stored results;A text-to-speech (TTS) system is configured with multiple voice corpuses used to synthesize speech. An incoming TTS request may be processed by a first, smaller, voice corpus to quickly return results to the user. The text of the request may be stored by the TTS system and then processed in the background using a second, larger, voice corpus. The second corpus takes longer to process but returns higher quality results. Future incoming TTS requests may be compared against the text of the first TTS request. If the text, or portions thereof match, the system may return stored results from the processing by the second corpus, thus returning high quality speech results in a shorter time.;4
97;9263034;2010;2016;GOOGLE LLC;GOOGLE LLC;G10L15/01,G10L15/065,G10L15/07,G10L15/10,G10L17/02;Adapting enhanced acoustic models;Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enhancing speech recognition accuracy. In one aspect, a method includes receiving voice queries, obtaining, for one or more of the voice queries, feedback information that references an action taken by a user that submitted the voice query after reviewing a result of the voice query, generating, for the one or more voice queries, a posterior recognition confidence measure that reflects a probability that the voice query was correctly recognized, wherein the posterior recognition confidence measure is generated based at least on the feedback information for the voice query, selecting a subset of the one or more voice queries based on the posterior recognition confidence measures, and adapting an acoustic model using the subset of the voice queries.;2
98;9275457;2014;2016;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06T7/0016,G01R33/4806,G06T7/11,G06T11/008,G06V10/273,G06V20/647,G06V20/653,G06F2218/00,G06F2218/22,G06T2207/10012,G06T2207/10084,G06T2207/10088,G06T2207/20221,G06T2207/30016,G06V2201/03,G06V2201/031;Real-time subject-driven functional connectivity analysis;A method and associated systems for real-time subject-driven functional connectivity analysis. One or more processors receive an fMRI time series of sequentially recorded, masked, parcellated images that each represent the state of a subject's brain at the image's recording time as voxels partitioned into a constant set of three-dimensional regions of interest. The processors derive an average intensity of each region's voxels in each image and organize these intensity values into a set of time courses, where each time course contains a chronologically ordered list of average intensity values of one region. The processors then identify time-based correlations between average intensities of each pair of regions and represent these correlations in a graphical format. As each subsequent fMRI image of the same subject's brain arrives, the processors repeat this process to update the time courses, correlations, and graphical representation in real time or near-real time.;0
99;9286877;2014;2016;;;B42D15/022,G10H1/0025,G10H1/0066,G10H2210/115,G10H2210/131,G10H2210/151,G10H2240/071,G10H2250/211;Method and apparatus for computer-aided variation of music and other sequences, including variation by chaotic mapping;An altered but recognizable variant of an input musical composition is created by parsing the input into elements, selecting elements for replacement using a hole-selecting algorithm, and replacing the selected elements using a substitution algorithm. A plurality of tracks from the same composition, or from different compositions, can be separately varied and combined, or concatenated and varied together, or used as a seed source of a substitution algorithm, to create a remix or mash-up. Music variants can be combined with photographs, images, or videos, for distribution to recipients, or for a project presentation or story board. Greeting cards, MP3 players, mobile electronic devices, wearable electronic devices, toys, video games, websites, and ringtones can be configured to create and play a new variant of an input composition each time it is accessed. Websites can be used to host and track successive, âchainâ variations of an input musical composition.;2
100;9299331;2013;2016;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/683,G06F16/68,G06F16/639,G10H2240/081,G10H2240/085,G10H2240/131,H04N21/26258,H04N21/8113;Techniques for selecting musical content for playback;Techniques are described for automatically selecting musical content for playback based on an initial âseedâ of music selected by a user in a way that seamlessly extends the user's listening experience. The initially selected seed music might be, for example, an album or a playlist. Music that follows the seed music is algorithmically selected to match the music selected by the user.;0
101;9302393;2015;2016;;;B25J9/1694,B62D57/032,G06N3/008,Y10S901/01,Y10S901/46,Y10S901/47,Y10S901/50;Intelligent auditory humanoid robot and computerized verbalization system programmed to perform auditory and verbal artificial intelligence processes;The disclosed Auditory RRC-Humanoid Robot equipped with a verbal-phoneme sound generator is a computer-based system programmed to reach high levels of human-like verbal-AI. Behavioral programming techniques are used to reach human-like levels of identification-AI, recognition-AI, and comprehension-AI of all the words and sentences presented to the robot as verbal input signals. An innovative behavioral speech processing methodology is used to recognize and repeat the acoustic sequential set of phoneme signals that comprise the verbally generated speech of human speakers. The recognized and repeated sequential set of phoneme signals are then mapped onto a unique phonetic structure such as all the words and clauses listed in a 50,000 word lexicon that may then make up the vocabulary of the RRC-Robot. The system is programmed to hear and understand verbal speech with its auditory sensors, and intelligently responds by verbally talking with its verbal-phoneme sound generator.;5
102;9304660;2012;2016;CloudMade Limited;CloudMade Limited;G06F8/38,G01C21/3682,G01C21/3697,G06F3/0482,G06F3/0484,G06F8/34;System and method for generating a user interface by auctioning space on the user interface to self-determining, content-providing modules;A system and method is provided for generating a dynamic, user interface that is capable of frequently changing and learning to provide the most relevant information to the user. The method includes executing a plurality of user interface (UI) modules on a computing device. Each UI module is a software component that provides content for display in the user interface. Each UI module determines its own content and its own priority level and bids for user interface space using its determined priority level. A UI controller is executed on the computing device. The UI controller is a software component that receives bids for the UI modules, determines the number of available spots for UI modules in the user interface, and selects UI modules associated with the highest priority for display in the available user interface spots.;0
103;9336782;2015;2016;VocaliD, INC.;VocaliD, INC.;G10L13/027,G10L17/22,G10L13/00,G10L13/033,G10L13/04,G10L13/06,G10L17/24;Distributed collection and processing of voice bank data;Voice data may be collected by a plurality of voice donors and stored in a voice bank. A voice donor may authenticate to a voice collection system to start a session to provide voice data. During the voice collection session, the voice donor may be presented with a sequence of prompts to speak and voice data may be transferred to a server. The received voice data may be processed to determine the speech units spoken by the voice donor and a count of speech units received from the voice donor may be updated. Feedback may be provided to the voice donor indicating, for example, a progress of the voice collection, a quality level of the voice data, or information about speech unit counts. The voice bank may be used to create TTS voices for voice recipients, create a model of voice aging, or for other applications.;1
104;9367763;2015;2016;Xerox Corporation;Xerox Corporation;G06V20/62,G06F16/24578,G06F16/50,G06F16/5846,G06F18/213,G06F21/60,G06F21/6245,G06V10/7715;Privacy-preserving text to image matching;A method for text-to-image matching includes generating representations of text images, such as license plate images, by embedding each text image into a first vectorial space with a first embedding function. With a second embedding function, a character string, such as a license plate number to be matched, is embedded into a second vectorial space to generate a character string representation. A compatibility is computed between the character string representation and one or more of the text image representations to identify a matching one. The compatibility is computed with a function that uses a transformation which is learned on a training set of labeled images. The learning uses a loss function that aggregates a text-to-image-loss and an image-to-text loss over the training set. The image-to-text loss penalizes the transformation when it correctly ranks a pair of character string representations, given an image representation corresponding to one of them.;4
105;9367950;2014;2016;IrisVR, Inc.;IrisVR, Inc.;G06T15/10,G06F3/011,G06F9/451,G06F9/4451,G06F9/455;Providing virtual reality experiences based on three-dimensional designs produced using three-dimensional design software;A system and method for providing a user virtual reality experience to visualize a three-dimensional design produced using three-dimensional design software are disclosed. The user may be enabled to visualize the three-dimensional design through a virtual reality application while the design is in progress. Changes to the three-dimensional design may be obtained dynamically and virtual reality information may be determined based on the dynamically obtained changes. The determined virtual reality information may be provided to the user for implementation of a virtual reality visualizing the three-dimensional design on a client device associated with the user.;1
106;9384451;2015;2016;Quantcast Corporation;Quantcast Corporation;G06Q30/0254,G06F18/214,G06N5/02,G06N5/04,G06N20/00,G06Q10/067,G06Q30/0269,G06Q30/0275;Incremental model training for advertisement targeting using streaming data;Incremental model training for advertisement targeting is performed using streaming data. A model for targeting advertisements of an advertising campaign is initialized. A data stream including data corresponding to converters and data corresponding to non-converters is received. The model is then applied to the data corresponding to the converter and data corresponding to the non-converter (or other ratio of converter to non-converters) to obtain a predicted score for each. The predicted score is compared to the observed score (e.g., an observed score of 1 for a converter, and 0 for a non-converter). The difference between the predicted and observed scores is computed, and the model is incrementally updated based on this difference. Models can optionally be built separately on multiple modeling servers that are geographically dispersed in order to support bidding on advertising opportunities in a real-time bidding environment.;0
107;9390087;2015;2016;Xerox Corporation;Xerox Corporation;G06F40/56,G06F16/3329,G06F16/3344,G06F40/205,G06F40/30,G06F40/253;System and method for response generation using linguistic information;A method for generating a natural language response to a customer inquiry includes parsing sentences in a corpus of natural language dialogs between a respective customer and an agent to extract dependencies. Each parsed sentence is represented by a dependency graph, based on the extracted dependencies. Dependency templates are generated, at least some of which are each generated from two or more of the extracted dependency graphs (e.g., using variables which each represent a group of words referring to a same topic) and are stored in a knowledge base. In response to the customer inquiry, one of the stored dependency templates is identified and a natural language response to the inquiry is generated, using the identified dependency template. The natural language response is generated based on words extracted from the natural language dialogs that are linked, in memory, to the identified dependency template.;2
108;9418385;2011;2016;INTUIT INC.;INTUIT INC.;G06Q40/10;Assembling a tax-information data structure;The disclosed embodiments relate to a tax-information assembly technique, which extracts tax information and associated context information from income-tax documents, where these income-tax documents are associated with an income-tax agency, and some of the income-tax documents include the same tax information in different document formats. During this technique, semantic and structural heuristics are used to identify tax phrases in the extracted tax information. Moreover, additional tax phrases in the extracted tax information are identified using a statistical identification technique. Next, relationships between the tax phrases and the additional tax phrases are determined, and the context information is used to consolidate the tax phrases and the additional tax phrases into a tax-information data structure.;0
109;9436953;2012;2016;2KDirect, Inc.;2KDirect, Inc.;G06Q30/0277,G06F16/5838,G06F16/5866,G06F16/9535,G06F16/9538,G06Q30/0263,G06Q30/0276;Automatic generation of electronic advertising messages containing one or more automatically selected stock photography images;A facility for constructing an advertising message in connection with a web page is described. The facility obtains text associated with the web page, and uses the obtained text to generate a query. The facility uses the generated query to select a stock photography image, and generates an advertising message that incorporates the selected stock photography image.;2
110;9460707;2015;2016;;;G10L15/04,G10L13/02,G10L13/07,G10L15/02,G10L25/18,G10L13/00,G10L13/033,G10L13/04,G10L13/047,G10L13/06,G10L13/08,G10L13/10,G10L2015/027;Method and apparatus for electronically recognizing a series of words based on syllable-defining beats;Speech is modeled as a cognitively-driven sensory-motor activity where the form of speech is the result of categorization processes that any given subject recreates by focusing on creating sound patterns that are represented by syllables. These syllables are then combined in characteristic patterns to form words, which are in turn, combined in characteristic patterns to form utterances. A speech recognition process first identifies syllables in an electronic waveform representing ongoing speech. The pattern of syllables is then deconstructed into a standard form that is used to identify words. The words are then concatenated to identify an utterance. Similarly, a speech synthesis process converts written words into patterns of syllables. The pattern of syllables is then processed to produce the characteristic rhythmic sound of naturally spoken words. The words are then assembled into an utterance which is also processed to produce a natural sounding speech.;3
111;9466292;2013;2016;GOOGLE LLC;GOOGLE LLC;G10L15/16,G10L15/07,G10L15/14;Online incremental adaptation of deep neural networks using auxiliary Gaussian mixture models in speech recognition;Methods and systems for online incremental adaptation of neural networks using Gaussian mixture models in speech recognition are described. In an example, a computing device may be configured to receive an audio signal and a subsequent audio signal, both signals having speech content. The computing device may be configured to apply a speaker-specific feature transform to the audio signal to obtain a transformed audio signal. The speaker-specific feature transform may be configured to include speaker-specific speech characteristics of a speaker-profile relating to the speech content. Further, the computing device may be configured to process the transformed audio signal using a neural network trained to estimate a respective speech content of the audio signal. Based on outputs of the neural network, the computing device may be configured to modify the speaker-specific feature transform, and apply the modified speaker-specific feature transform to a subsequent audio signal.;3
112;9484014;2013;2016;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/086,G10L13/06;Hybrid unit selection / parametric TTS system;In a text-to-speech (TTS) system, a database including sample speech units for unit selection may be include both units represented by sample audio segments as well as parametric representations of units created by Hidden Markov Models (HMMs). Inclusion of parametric representations in the database may reduce the storage necessary to maintain the database. The parametric representations may be configured to match a voice of the audio segments. The parametric representations may correspond to phonetic units that are less frequently encountered in TTS processing, such as rare diphones or phonemes corresponding to foreign languages. Multiple foreign language HMM models may be used to enable polyglot synthesis with a reduction in storage capacity requirements. Parametrically stored speech units may be combined with speech segments generated during processing time by a parametric model.;4
113;9495955;2013;2016;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/063;Acoustic model training;Features are disclosed for generating acoustic models from an existing corpus of data. Methods for generating the acoustic models can include receiving at least one characteristic of a desired acoustic model, selecting training utterances corresponding to the characteristic from a corpus comprising audio data and corresponding transcription data, and generating an acoustic model based on the selected training utterances.;2
114;9507858;2012;2016;GOOGLE LLC;GOOGLE LLC;G06F16/38;Selectively merging clusters of conceptually related words in a generative model for text;One embodiment of the present invention provides a system that merges similar clusters of conceptually-related words in a probabilistic generative model for textual documents. During operation, the system receives a current model, which contains terminal nodes representing random variables for words and contains cluster nodes representing clusters of conceptually related words. Nodes in the current model are coupled together by weighted links, wherein if a node fires, a link from the node to another node causes the other node to fire with a probability proportionate to the weight of the link. Next, the system determines whether cluster nodes in the current model explain other cluster nodes in the current model. If two cluster nodes explain each other, the system merges the two cluster nodes to form a combined cluster node.;3
115;9508338;2013;2016;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/02,G10L13/06,G10L2013/083;Inserting breath sounds into text-to-speech output;A text-to-speech (TTS) system may be configured to incorporate breath sounds in the output speech. By incorporating breath sounds into speech output from text a TTS system may be able to mimic more naturally sounding human speech, particularly for long-form narration of text longer than short phrases. The breath sounds may be stored as units for unit selection or may be generated during parametric synthesis. The acoustic features of the breath sounds and duration between breaths may depend upon the punctuation of text, the linguistic distance between breaths, the breaks between intonational phrases, the linguistic context of the breaths, and other factors.;1
116;9552548;2016;2017;Intraspexion, Inc.;Intraspexion, Inc.;G06Q10/0635,G06F16/35,G06F18/214,G06F40/10,G06N3/04,G06N3/044,G06N3/08,G06N20/00,G06Q10/107,G06Q50/18,H04L9/50;Using classified text and deep learning algorithms to identify risk and provide early warning;Deep learning is used to identify specific, potential risks to an enterprise (of which litigation is the prime example) while such risks are still internal electronic communications. The system involves mining and using existing classifications of data (e.g., from a litigation database) to train one or more deep learning algorithms, and then examining the internal electronic communications with the trained algorithm, to generate a scored output that will enable enterprise personnel to be alerted to risks and take action in time to prevent the risks from resulting in harm to the enterprise or others.;0
117;9558428;2014;2017;;;G06T11/60,G06F18/40,G06V10/945,G06V20/30;Inductive image editing based on learned stylistic preferences;An automatic image editing system is a sophisticated and intelligent computer-based image editing system that automatically applies edits to attributes of captured images in accordance with the preferences and editing style of any given user. The system initially learns a user's image editing style with examples of user-edited images. The system continues to learn or adjust to the user's evolving image editing style through feedback of subsequent manual edits by the user to attributes of computer implemented automatically edited images.;3
118;9576009;2015;2017;Narrative Science LLC;Narrative Science LLC;G06F16/2228,G06F16/2428,G06F16/243,G06F16/248,G06Q10/063,G06Q10/10;Automatic generation of narratives from data using communication goals and narrative analytics;The exemplary embodiments described herein are related to techniques for automatically generating narratives about data based on communication goal data structures that are associated with configurable content blocks. The use of such communication goal data structures facilitates modes of operation whereby narratives can be generated in real-time and/or interactive manners.;2
119;9582501;2014;2017;YSEOP SA;YSEOP SA;G06F40/56,G06F40/30;Techniques for automatic generation of natural language text;"Techniques for use in connection with generating text, the techniques comprise: obtaining a plurality of items of content and associated metadata; and generating a document plan comprising a plurality of rhetorical relations among items of content in the plurality of items of content, the plurality of rhetorical relations comprising a first set of one or more rhetorical relations and a second set of one or more rhetorical relations different from the first set, the generating comprising: obtaining a schema specifying the first set of one or more rhetorical relations; and identifying the second set of rhetorical relations based, at least in part, on the associated metadata, wherein the second set of rhetorical relations is not in the schema.";2
120;9589578;2013;2017;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04L67/565,G10L15/22,H04L67/00,H04L67/10,G10L2015/223;Invoking application programming interface calls using voice commands;Technologies are described herein for invoking API calls through voice commands. An annotated API description is received at a voice API interface. The annotated API description comprises descriptions of one or more APIs and speech annotations for the one or more APIs. The voice API interface further receives a voice API command from a client. By utilizing the annotated API description and the speech annotations contained therein, the voice API interface converts the voice API command into an API call request, which is then sent to the corresponding service for execution. Once the service returns an API call result, the voice API interface interprets the API call result and further converts it into an audio API response based on the information contained in the annotated API description and the speech annotations. The audio API response is then sent to the client.;1
121;9608942;2014;2017;CQuotient, Inc.;CQuotient, Inc.;G06Q30/0277,G06Q30/02,G06Q30/0254,H04L51/04,H04L51/216;Digital data processing methods and apparatus for the automated generation of personalized digital content;The invention provides, in some aspects, digital data processing methods of generating digital content pieces (e.g., email messages or portions thereof) that are customized in accord with individual recipient behaviors. Such methods include the step of generating and digitally transmitting to a digital data devices of a recipient a digital content piece that (i) has a call to action to which the recipient can respond and (ii) that has a plurality of features selected so as to maximize a probability, P(b1,b2, . . . ,bM,x1,x2, . . . ,xM), that the recipient will respond to that call to action, where that probability is defined by the relationP(b1,b2, . . . ,bM,x1,x2, . . . ,xM)=exp(Î£j=1, . . . ,Mbjxj)/(1+exp(Î£j=1, . . . ,Mbjxj));1
122;9633048;2015;2017;Adobe Systems Incorporated;Adobe Systems Incorporated;G06F40/30,G06F16/51,G06F16/58,G06F16/583,G06F16/5838,G06F16/5846,G06F16/587,G06F40/131,G06F40/205,G06F40/211;Converting a text sentence to a series of images;A text sentence is automatically converted to an image sentence that conveys semantic roles of the text sentence. This is accomplished by identifying semantic roles associated with each verb of a sentence, any associated verb adjunctions, and identifying the grammatical dependencies between words and phrases in a sentence, in some embodiments. An image database, in which each image is tagged with descriptive information corresponding to the image depicted, is queried for images corresponding to the semantic roles of the identified verbs. Unless a single image is found to depict every semantic role, the text sentence is split into two smaller fragments. This process is the repeated and performed recursively until a number of images have been identified that describe each semantic role of each sentence fragment.;4
123;9646587;2016;2017;Disney Enterprises, Inc.;Disney Enterprises, Inc.;G10H1/0025,G10H1/0066,G10H1/361,G10H1/46,G10H2210/026,G10H2220/145,G10H2220/411,G10H2220/525,G10H2240/175,G10H2240/305,G10H2240/325;Rhythm-based musical game for generative group composition;A musical game system and associated methods configured to allow for unguided, free-form group-based musical expressivity during generation of a collaborative digital music track (or âsongâ). The musical game system is designed to provide a hardware and software pipeline that functions to record, quantize, and loop multiple (e.g., 1 to 15 or more) users' inputs (e.g., via a piezoelectric MIDI (Musical Instrument Digital Interface) controllers or triggered instruments or other user input devices) in a dynamic playback space. The musical game system further functions to provide volume attenuation and localization of playback to enable participants to express themselves with their user inputs with complete agency while simultaneously adding to an overarching, collaborative musical composition generated using their user inputs and other participants' user inputs. The collaborative musical composition is created by the system so as to maintain coherence and tonality regardless of the user inputs the system receives and processes.;6
124;9646601;2013;2017;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/02,G06F40/10,G10L13/04;Reduced latency text-to-speech system;In delivering text-to-speech (TTS) results to a user, the time between the user request and delivery of initial TTS results is reduced using one or more of various techniques. Caching of TTS results may be reconfigured to cache unit indices rather than full speech synthesis results. More powerful computing resources may be dedicated to early TTS processing. A user may be notified of TTS results prior to complete processing of a TTS request. Early TTS processing may be performed by a local device and then passed to a remote device.;1
125;9653093;2014;2017;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/16,G10L15/08,G10L15/142,G10L15/144;Generative modeling of speech using neural networks;Features are disclosed for using an artificial neural network to generate customized speech recognition models during the speech recognition process. By dynamically generating the speech recognition models during the speech recognition process, the models can be customized based on the specific context of individual frames within the audio data currently being processed. In this way, dependencies between frames in the current sequence can form the basis of the models used to score individual frames of the current sequence. Thus, each frame of the current sequence (or some subset thereof) may be scored using one or more models customized for the particular frame in context.;1
126;9665800;2013;2017;GOOGLE LLC;GOOGLE LLC;G06V20/653,G06F16/00,G06F18/214,G06T15/50,G06V20/647,G06F18/28;Rendering virtual views of three-dimensional (3D) objects;Methods and systems for rendering virtual views of three-dimensional (3D) object data models are described. An example method may include receiving information associated with rendering a 3D object data model, and determining virtual views for rendering the 3D object data model. Based on the virtual views and the received information, synthetic training images of the object may be rendered, and information identifying a given virtual view used to render a synthetic training image may be stored with the synthetic training images. A visual object recognition module may determine from a plurality of images and image that substantially matches one or more of the synthetic training images, and the matched image may be associated with information identifying the object and a virtual view used to render a matching synthetic training image. Another method may also leverage information from a synthetic training image to help recognize other objects of the matched image.;4
127;9679554;2014;2017;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/08,G06Q10/06311,G06Q10/101,G10L13/00;Text-to-speech corpus development system;A system may determine text for inclusion in a voice corpus for use in text-to-speech (TTS) processing using an interface that allows multiple entities to connect and review potential text segments concurrently. The interface may allow networked communication with the system. Individual text segments may be approved or rejected by reviewing entities, such as proofreaders. The system may prioritize text segments to send to reviewers and may re-prioritize text segments based on a linguistic coverage of previously accepted text segments.;1
128;9697178;2012;2017;Narrative Science LLC;Narrative Science LLC;G06F40/00,G06F40/35,G06F40/56;Use of tools and abstraction in a configurable and portable system for generating narratives;The tools and abstractions of the subject invention function as part of or to configure a system to use available data and information to automatically create narrative stories that describes domain events, circumstances and/or entities in a comprehensible and compelling and audience customized, manner. Computer executable instructions provide for generating a narrative story using standard and uniform structures and data for receiving domain related data and a story specification, parsing the story specification to provide constituent components, transforming the constituent components into executable code, instantiating content blocks having at least one feature for the domain according to the story specification and rendering the narrative story using the constituent components specified by the content blocks.;4
129;9697197;2015;2017;Narrative Science LLC;Narrative Science LLC;G06F40/30,G06F40/35,G06F40/56;Automatic generation of narratives from data using communication goals and narrative analytics;The exemplary embodiments described herein are related to techniques for automatically generating narratives about data based on communication goal data structures that are associated with configurable content blocks. The use of such communication goal data structures facilitates modes of operation whereby narratives can be generated in real-time and/or interactive manners.;3
130;9697822;2014;2017;Apple Inc.;Apple Inc.;G10L15/063,G10L15/065,G10L15/22,G10L15/26,G10L15/30,G10L15/07,G10L2015/223;System and method for updating an adaptive speech recognition model;A method for updating an adaptive speech recognition model is provided. In some implementations, the method is performed at a communications device including one or more processors and memory storing instructions for execution by the one or more processors. The method includes determining that a first user of a first mobile communication device is engaged in a call over a communications network and providing an adaptive speech recognition model The method also includes analyzing an outbound audio channel of the first mobile communication device to obtain a call audio signal corresponding to audio input from one or more microphones of the first mobile communication device and updating the adaptive speech recognition model with training data derived from the call audio signal.;1
131;9710829;2013;2017;INTUIT INC.;INTUIT INC.;G06Q30/0276,G06Q50/01;Methods, systems, and articles of manufacture for analyzing social media with trained intelligent systems to enhance direct marketing opportunities;Disclosed are methods, systems, and articles of manufactures for analyzing user generated content items in social media networks with trained intelligent systems to create or enhance direct marketing opportunities. The method or the system monitors user generated content items in social media networks and identifies a relevant user generated content item that may be materialized into a direct marketing opportunity. The method or system further performs language processing on the relevant user generated content item and uses the processing results to prepare a response which is subsequently transmitted to the user to materialize the direct marketing opportunity. The method or system uses various intelligent logic processes or modules that may be further enhanced by machine learning techniques with human expert reviews and intervention to improve their respective accuracy, reliability, or confidence level.;1
132;9754206;2017;2017;Intraspexion, Inc.;Intraspexion, Inc.;G06N3/08,G06N3/044,G06Q10/0635,G06Q10/10;Using classified text and deep learning algorithms to identify drafting risks in a document and provide early warning;Deep learning is used to identify a potential risk that a contract will be unenforceable due to a drafting error whereby one or more terms or phrases are ambiguous. The system involves mining and using existing classifications of data (e.g., from a litigation database) to train one or more deep learning algorithms, and then examining the internal electronic drafts of contracts with the trained algorithm, to generate a scored output that will enable enterprise personnel to be alerted to the ambiguity risks and take action in time to prevent the risks from resulting in harm to the enterprise.;0
133;9754219;2017;2017;Intraspexion, Inc.;Intraspexion, Inc.;G06Q50/18,G06F18/214,G06F18/24,G06F40/40,G06N3/044,G06N3/048,G06N20/00,G06Q10/0635,G06N3/08;Using classified text and deep learning algorithms to identify entertainment risk and provide early warning;Deep learning is used to identify specific, potential entertainment risks to an enterprise while such risks before the enterprise commits large sums of money to a project. The system involves mining and using existing classifications of data (e.g., from a database of previously successful book and film franchises) to train one or more deep learning algorithms, and then examining a proposed entertainment document with the trained algorithm, to generate a scored output that will enable enterprise personnel to be alerted to risks and take action in time to prevent the risks from resulting in harm to the enterprise.;0
134;9754220;2017;2017;Intraspexion, Inc.;Intraspexion, Inc.;G06N20/00,G06F40/40,G06N3/044,G06N3/08,G06Q10/0635,G16H10/60,G16H50/30;Using classified text and deep learning algorithms to identify medical risk and provide early warning;Deep learning is used to identify specific, potential risks of missed diagnosis for a patient and reporting the risk to healthcare provider. The system involves mining and using existing electronic health records for specific medical diagnosis to train one or more deep learning algorithms, and then examining the internal electronic health record of the patient with the trained algorithm, to generate a scored output that will enable a healthcare provider to be alerted to potential risks of a missed diagnosis.;0
135;9761222;2015;2017;;;G06F40/20,G06F40/30,G10L15/04,G10L15/07,G10L15/22,H04L51/00,H04L51/02,G10L2015/223;Intelligent conversational messaging;A system for implementing a computer-based assistance in a conversation can comprise determining an identification indication associated with a first participant in a conversation. The system can also comprise accessing, within a database, user information associated with the identification indication. The user information can comprise data relating to the first participant. Based upon the user information and one or more words communicated in the conversation, the system can include calculating one or more probabilities that the conversation is associated with one or more respective conversation types. Based upon a determined conversation type, the system can activate an expert agent in the conversation. The expert agent can comprise a virtual conversation participant that is associated with a goal. The system can also comprise executing a sequential lists of actions that are associated with the goal, wherein the list of actions comprises interactions with the first participant.;1
136;9786084;2017;2017;LoomAi, Inc.;LoomAi, Inc.;G06T13/40,G06F3/005,G06F18/23,G06T7/11,G06T7/13,G06T7/246,G06T7/248,G06T7/73,G06T7/90,G06T13/205,G06T15/04,G06T17/20,G06T19/20,G06V40/162,G06V40/166,G06V40/171,G06T2207/10016,G06T2207/10024,G06T2207/20021,G06T2207/20036,G06T2207/20081,G06T2207/20084,G06T2207/30201,G06T2219/2012,G06T2219/2021;Systems and methods for generating computer ready animation models of a human head from captured data images;System and methods for computer animations of 3D models of heads generated from images of faces is disclosed. A 2D captured image that includes an image of a face can be received and used to generate a static 3D model of a head. A rig can be fit to the static 3D model to generate an animation-ready 3D generative model. Sets of rigs can be parameters that each map to particular sounds. These mappings can be used to generate a playlists of sets of rig parameters based upon received audio content. The playlist may be played in synchronization with an audio rendition of the audio content.;5
137;9792889;2016;2017;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G10H1/0025,G06N3/08,G10H1/0008,G10H2210/061,G10H2210/066,G10H2210/101,G10H2210/111,G10H2210/145,G10H2240/131,G10H2240/145,G10H2250/005,G10H2250/015,G10H2250/311;Music modeling;A computer implemented method is provided for generating a prediction of a next musical note by a computer having at least a processor and a memory. A computer processor system is also provided for generating a prediction of a next musical note. The method includes storing sequential musical notes in the memory. The method further includes generating, by the processor, the prediction of the next musical note based upon a music model and the sequential musical notes stored in the memory. The method also includes updating, by the processor, the music model based upon the prediction of the next musical note and an actual one of the next musical note. The method additionally includes resetting, by the processor, the memory at fixed time intervals.;2
138;9798653;2010;2017;Nuance Communications, Inc.;Nuance Communications, Inc.;G06F12/00,G06F40/00,G06F40/58,G10L13/00,G10L13/086;Methods, apparatus and data structure for cross-language speech adaptation;Adapted speech models produce fluent synthesized speech in a voice that sounds as if the speaker were fluent in a language in which the speaker is actually non-fluent. A full speech model is obtained based on fluent speech in the language spoken by a first person who is fluent in the language. A limited set of utterances is obtained in the language spoken by a second person who is non-fluent in the language but able to speak the limited set of utterances in the language. The full speech model of the first person is then processed with the limited set of utterances of the second person to produce an adapted speech model. The adapted speech model may be stored to a multi-lingual speech model as a child node of a root with an associated language selection question and branches pointed to the adapted speech model and other speech models, respectively.;1
139;9805018;2014;2017;;;G06F40/205,G06F16/951,G06F16/9535,G06F40/253,G06Q30/0241,G06Q50/01,G06F40/163,G06F40/232;Natural language processing for analyzing internet content and finding solutions to needs expressed in text;A natural language processing methodology to automatically transform push advertising into pull advertising. Text found in forum, blog, and social media postings throughout the Internet is grammatically analyzed to identify potential customers who have expressed a clear problem. Only parsing rules with the least likely elements present are evaluated. In response, personalized replies are produced that contain pertinent and useful information about a potential product or service. Those replies appear to come from other Internet users, thus converting expressed needs of user/prospects into discussions with sales technicians.;3
140;9865253;2014;2018;VoiceCipher, Inc.;VoiceCipher, Inc.;G10L25/90,G06V10/40,G10L15/083,G10L17/02,G10L17/06;Synthetic speech discrimination systems and methods;The present invention is a system and method for discriminating between human and synthetic speech. The method and system include memory for storing a speaker verification application, a communication network that receives from a client device a speech signal having one or more discriminating features, and a processor for executing instructions stored in memory. The execution of the instructions by the processor extracts the one or more discriminating features from the speech signal and classifies the speech signal as human or synthetic based on the extracted features.;0
141;9870797;2016;2018;GOOGLE LLC;GOOGLE LLC;G11B27/031,G11B27/02,G11B27/28,G11B27/34,H04N21/23418,H04N21/23439,H04N21/8549;Generating and providing different length versions of a video;Systems and methods for generating and presenting different length versions of a video are presented. In one or more aspects, a system is provided that includes an analysis component configured to analyze a video and generate summaries of content included in respective segments of the video, and a summary component configured to generate shortened versions of the video having durations less than the duration of the video based in part on the summaries of the content included in the respective segments of the video, wherein the video is made available for streaming in association with an option to select the video or one of the shortened versions.;0
142;9922376;2014;2018;INTUIT INC.;INTUIT INC.;G06Q40/123;Systems and methods for determining impact chains from a tax calculation graph of a tax preparation system;Systems, methods and articles of manufacture for determining impact chains from a calculation graph for calculating taxes. The system includes a computing device, a data store in communication with the computing device and a tax preparation software application executable by the computing device. The tax preparation software application has a tax calculation engine, a tax calculation graph, and an impact chain engine. The tax calculation engine is configured to perform a plurality of tax calculation operations based on a tax calculation graph. The impact chain engine is configured to analyze the tax calculation graph and determine an impact chain for each of a plurality of nodes in the graph, wherein an impact chain for a respective node consists of one of (a) each of the other nodes which are affected by the respective node, or (b) each of the other nodes which affect the respective node.;0
143;9922641;2012;2018;GOOGLE LLC;GOOGLE LLC;G10L15/063,G10L13/033,G10L15/07,G10L15/02,G10L2015/025,G10L2021/0135;Cross-lingual speaker adaptation for multi-lingual speech synthesis;The subject matter of the disclosure is embodied in a method that includes receiving input speech data from a speaker in a first language, and estimating, based on a universal speech model, a speaker transform representing speaker characteristics associated with the input speech data. The method also includes accessing a speaker-independent speech model for generating speech data in a second language that is different from the first language. The method further includes modifying the speaker-independent speech model using the speaker transform to obtain a speaker-specific speech model, and generating speech data in the second language using the speaker-specific speech model.;4
144;9928835;2015;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06V40/20,G10H1/0008,G10L15/24,G10L25/63,G11B27/00,G11B27/102,G11B27/28,G10H2210/056,G10H2210/071,G10H2210/081,G10L2015/227;Systems and methods for determining content preferences based on vocal utterances and/or movement by a user;This disclosure relates to systems and methods for determining when a user likes a piece of content based, at least in part, on analyzing user responses to the content. In one embodiment, the user's response may be monitored by audio and motion detection devices to determine when the user's vocals or movements are emulating the content. When the user's emulation exceeds a threshold amount the content may be designated as âliked.â In certain instances, a similar piece of content may be selected to play when the current content is finished.;1
145;9934515;2016;2018;GOOGLE LLC;GOOGLE LLC;G06Q30/0255,G06F16/3322,G06F16/951,G06F16/954,G06F40/274,G06F40/284,G06F40/40,G06N3/02,G06N3/08,G06Q30/0631,H04N21/44226,H04N21/4666,H04N21/4668;Content recommendation system using a neural network language model;The present disclosure relates to applying techniques similar to those used in neural network language modeling systems to a content recommendation system. For example, by associating consumed media content to words of a language model, the system may provide content predictions based on an ordering. Thus, the systems and techniques described herein may produce enhanced prediction results for recommending content (e.g. word) in a given sequence of consumed content. In addition, the system may account for additional user actions by representing particular actions as punctuation in the language model.;0
146;9959868;2017;2018;Wisconsin Alumni Research Foundation;Wisconsin Alumni Research Foundation;G06F40/56,G06F8/30,G06F40/289,G06F40/35,G06F40/42,G06F40/58;Conversational programming interface;Domain specific programming is facilitated through the use of a conversational interface using natural language commands from the user and natural language cues to the user. The natural language conversation provides the actual program and thus can be saved and edited for future use.;0
147;9972127;2015;2018;Dassault Systemes SolidWorks Corporation;Dassault Systemes SolidWorks Corporation;G06T19/20,G06F3/04815,G06F3/04842,G06F3/04845,G06F30/00,G06T17/10,G06T2200/24,G06T2210/04,G06T2219/2008,G06T2219/2021;3D content creation tool with automatic mating;A computer-implemented method and system provide one or more primitive commands. Each primitive command when executed defines a shape of an object and accepts input data to specify one or more dimensions of the shape. When the dimensions are applied to the shape, a three-dimensional (3D) object that is a 3D part and/or a 3D feature is created. Input data is utilized by a selected primitive command to create a sketch as specified by a definition of the shape on a sketch plane that is automatically selected and to specify a transformation that creates the 3D object from the sketch.;4
148;9972140;2016;2018;;;G06T19/20,G06T15/005,G06T15/20,G06T15/80,G06T17/00,G06T2200/24,G06T2219/2024;Consumer product advertising image generation system and method;"A method of generating a three-dimensional consumer product image includes: distilling one or more objects of a three-dimensional model of a consumer product to corresponding three-dimensional geometric representations; applying a visual attribute to the geometric representations of the objects; rendering a plurality of two dimensional images from the geometric representations of the objects with applied visual attributes; defining one or more stacking orders of at least a portion of the plurality of two dimensional images; and delivering to a user the two-dimensional images and the defined stacking orders in a form that is importable into a computer-executable image editing program, the computer-executable image editing program capable of enabling user selection of one or more of the defined stacking orders and capable of producing a three-dimensional image of at least a portion of the consumer product based on the user-selected one or more defined stacking orders and the imported two-dimensional images.";4
149;9977773;2014;2018;Narrative Science LLC;Narrative Science LLC;G06Q10/063,G06F16/95,G06F40/20,G06Q10/0639,G06Q10/06398,G06Q30/0201,G06F16/20,G06F40/253,G06N20/00;Automatic generation of narratives from data using communication goals and narrative analytics;The exemplary embodiments described herein are related to techniques for automatically generating narratives about data based on communication goal data structures that are associated with configurable content blocks. The use of such communication goal data structures facilitates modes of operation whereby narratives can be generated in real-time and/or interactive manners.;3
150;9978359;2013;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/02,G10L13/06,G10L13/10;Iterative text-to-speech with user feedback;A text-to-speech (TTS) processing system may be configured for iterative processing. Speech units for unit selection may be tagged according to extra segmental features, such as emotional features, dramatic features, etc. Preliminary TTS results based on input text may be provided to a user through a user interface. The user may offer corrections to the preliminary results. Those corrections may correspond to the extra segmental features. The user corrections may then be input into the TTS system along with the input text to provide refined TTS results. This process may be repeated iteratively to obtain desired TTS results.;4
151;9990176;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G06F3/165,G06F16/632,G06F16/957,G10L13/04,G10L15/22,G10L2015/223;Latency reduction for content playback;Methods and devices for determining whether a local version of content is stored on an electronic device associated with a user account on a backend system are described herein. In a non-limiting embodiment, the backend system may track and monitor the content stored on the electronic device using the associated user account. If an individual speaks an utterance requesting a particular content item, the backend system may determine, prior to sending the content to the electronic device, whether a local version is stored within the electronic device's memory. If so, the backend system may instruct the electronic device to output the local version, thereby reducing the amount of bandwidth consumed. The backend system may further be capable of predictively generating and then caching certain audio data to the electronic device. For instance, frequent utterances may be tracked, and likely responses to those utterances may be generated prior to the utterance being spoken so that the response is available substantially instantaneously.;1
153;10008193;2017;2018;OBEN, INC.;OBEN, INC.;G10H1/366,G10H1/20,G10H2210/066,G10H2210/081,G10H2210/165,G10H2210/561,G10H2220/011,G10H2250/031,G10H2250/455,G10H2250/481;Method and system for speech-to-singing voice conversion;A singing voice conversion system configured to generate a song in the voice of a target singer based on a song in the voice of a source singer is disclosed. The embodiment utilizes two complementary approaches to voice timbre conversion. Both combine the natural prosody of a source singer with the pitch of the target singerâtypically the user of the systemâto achieve realistic sounding synthetic singing. The system is able to transpose the key of any song to match the automatically determined or desired pitch range of the target singer, thus allowing the system to generalize to any target singer, irrespective of their gender, natural pitch range, and the original pitch range of the song to be sung.;3
154;10008209;2016;2018;Educational Testing Service;Educational Testing Service;G10L17/18,G10L17/04,G10L15/005,G10L15/16,G10L17/08,G10L17/20;Computer-implemented systems and methods for speaker recognition using a neural network;Systems and methods are provided for providing voice authentication of a candidate speaker. Training data sets are accessed, where each training data set comprises data associated with a training speech sample of a speaker and a plurality of speaker metrics, where the plurality of speaker metrics include a native language of the speaker. The training data sets are used to train a neural network, where the data associated with each training speech sample is a training input to the neural network, and each of the plurality of speaker metrics is a training output to the neural network. Data associated with a speech sample is provided to the neural network to generate a vector that contains values for the plurality of speaker metrics, and the values contained in the vector are compared to values contained in a reference vector associated with a known person to determine whether the candidate speaker is the known person.;1
155;10019525;2017;2018;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F16/9535,G06F16/3334,G06F16/345,G06F16/90332,G06F16/9038,G06F16/93;Extractive query-focused multi-document summarization;A method, computer system, and computer program product for generating a multi-document summary is provided. The embodiment may include receiving a query statement, one or more documents, one or more summary constraints, and quality goals. The embodiment may include identifying one or more keywords within the query statement. The embodiment may include performing a sentence selection from the one or more documents based on the one or more identified keywords. The embodiment may include generating a plurality of candidate summaries of the one or more documents based on the performed sentence selection, the goals, and a cross entropy method. The embodiment may include calculating a quality score for each of the plurality of generated candidate summaries using a plurality of quality features. The embodiment may include selecting a candidate summary from the plurality of generated candidate summaries with the highest calculated quality score that also satisfies a quality score threshold.;3
156;10021276;2018;2018;Beijing Kingsoft Internet Security Software Co., Ltd.;Beijing Kingsoft Internet Security Software Co., Ltd.;H04N5/04,G06F40/40,G06V40/169,G06V40/172,G10L15/26,H04N5/265,H04N21/2343,H04N21/242;Method and device for processing video, electronic device and storage medium;Embodiments of the present disclosure provide a method and a device for processing a video, an electronic device and a storage medium. The method includes the followings. A target recognition is performed to A first video segments and B first speech segments to obtain M second video segments and N second speech segments. A speech processing is performed to the N second speech segments to obtain N target speech text files. First representation information is extracted from the M second video segments to obtain Q pieces of first representation information. A second sound matched with the target character is determined according to the Q pieces of first representation information. The second sound is merged with the N target speech text files to obtain N target speech segments.;6
157;10027662;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04L63/0861,G06F21/32,G10L15/22,G10L17/06,G10L17/22,G10L13/02,G10L13/033,G10L13/06,G10L13/08,G10L15/02,G10L15/16,G10L15/1815,G10L25/78,G10L25/84,G10L2015/088,G10L2015/223,G10L2015/227,H04L2463/082;Dynamic user authentication;Systems, methods, and devices for dynamically authenticating a user are disclosed. A speech-controlled device captures a spoken command, and sends audio data corresponding thereto to a server. The server determines the audio data includes a spoken command to receive content, and therefrom determines a source storing the content. The server also determines threshold user authentication confidence score data associated with the content source. Based at least in part on the threshold user authentication confidence score data, the server determines a user authentication technique, and a device configured to capture user authentication data. The server determines user authentication confidence score data using user authentication data received from the device, and determines weighted user authentication confidence score data therefrom. If the weighted user authentication confidence score data satisfies the threshold user authentication confidence score data, the server receives the requested content from the content source.;0
158;10042836;2012;2018;INTUIT INC.;INTUIT INC.;G06Q40/123,G06F16/36,G06F16/9024,G06Q10/10;Semantic knowledge base for tax preparation;"A method for tax preparation, including: obtaining a tax preparation ontology and a set of electronic tax documents; mining, by a computer processor, the set of electronic tax documents to identify relevant tax data based on the tax preparation ontology; constructing, by the computer processor, a semantic graph including the relevant tax data in a set of triples; receiving a semantic query language query; querying the semantic graph based on the semantic language query to obtain a result set; and returning the result set to a tax preparation application, where the tax preparation application displays at least a portion of the result set to a user in response to a help request.";0
159;10049477;2015;2018;GOOGLE LLC;GOOGLE LLC;G06T11/60,G06T11/001;Computer-assisted text and visual styling for images;Implementations can relate to providing computer-assisted text and visual styling for images. In some implementations, a computer-implemented method includes determining a set of characteristics of an image, and applying one or more first visual modifications to the image based on one or more of the set of characteristics of the image. The method can include receiving user input defining user text, providing the user text in the image, and applying one or more second visual modifications to the image based on the user text and based on at least one of the set of characteristics of the image.;4
160;10068557;2017;2018;GOOGLE LLC;GOOGLE LLC;G10H1/0041,G06N3/04,G06N3/045,G06N3/08,G06N3/084,G10H1/0025,G10H7/10,G10H7/12,G10H2210/111,G10H2220/116,G10H2250/311,G10H2250/605,G10H2250/615,G10H2250/621;Generating music with deep neural networks;The present disclosure provides systems and methods that include or otherwise leverage a machine-learned neural synthesizer model. Unlike a traditional synthesizer which generates audio from hand-designed components like oscillators and wavetables, the neural synthesizer model can use deep neural networks to generate sounds at the level of individual samples. Learning directly from data, the neural synthesizer model can provide intuitive control over timbre and dynamics and enable exploration of new sounds that would be difficult or impossible to produce with a hand-tuned synthesizer. As one example, the neural synthesizer model can be a neural synthesis autoencoder that includes an encoder model that learns embeddings descriptive of musical characteristics and an autoregressive decoder model that is conditioned on the embedding to autoregressively generate musical waveforms that have the musical characteristics one audio sample at a time.;4
161;10074364;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/20,G10L25/51,G06F16/316,G06F40/289,G10L15/10,G10L15/26,G10L15/285,G10L17/04,G10L2015/223;Sound profile generation based on speech recognition results exceeding a threshold;Systems and methods for generating sound profiles of artificial commands detected by multiple voice activated electronic devices is described herein. In some embodiments, numerous voice activated electronic devices may send audio data representing a phrase to a backend system at a substantially same time. Text data representing the phrase, and counts for instances of that text data, may be generated. If the number of counts exceeds a predefined threshold, the backend system may cause any remaining response generation functionality that particular command that is in excess of the predefined threshold to be stopped, and those devices returned to a sleep state. In some embodiments, a sound profile unique to the phrase that caused the excess of the predefined threshold may be generated such that future instances of the same phrase may be recognized prior to text data being generated, conserving the backend system's resources.;1
162;10086261;2014;2018;Kabam, Inc.;Kabam, Inc.;A63F13/803,A63F13/56,A63F13/57,A63F2300/65;System and method for simulating passive racing games in an online game;An online game is provided to players where players may participate in individual episodes of the online racing game. The individual episodes may include individual races within the racing game in which players control game entities through racecourses. User accounts comprising an indication of game entities associated with the players of the online game, an indication of a status of the player, and a game play metric associated with the player may be managed. Races in the online game may be synthesized by simulating participation by game entities of a player in individual synthetic episodes of the online game while the player is absent from the online game. A change in the game play metric of the player may be determined based on the performance of the first game entity in the race of the first synthetic episode.;0
163;10090020;2015;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G11B27/06,G11B27/002,G11B27/005,G11B27/031,G11B27/22,G11B27/3081;Content summarization;Various examples are directed to systems and method for creating a summary video of user content comprising a plurality of frames of data. A summarization engine may identify from the user content a first region-of-interest and a second region-of-interest. The summarization engine may generate a first content clip that comprises the first region-of-interest and has a first time length. The summarization engine may receive a new input variable for the summary video. The summarization engine may determine a modified first time length of the first content clip and generate a modified first content clip having the first modified time length. The summarization engine may generate the summary video including the modified first content clip.;3
164;10091358;2017;2018;SPLUNK INC.;SPLUNK INC.;H04M3/5175,G06F3/0482,G06F3/04847,G06F16/248,G06T11/206,H04M3/2254,H04M3/36,G06T2200/24,H04M2201/42,H04M2203/402,H04M2203/558;Graphical user interface for call center analysis;One or more embodiments related to a method of generating a graphical user interface. The method includes obtaining a metric interface hierarchy having multiple nodes, where each node defines a visualization for the node, and the metric interface hierarchy defines an ordering on the nodes. The method further includes traversing the metric interface hierarchy starting with a selected node to obtain a subhierarchy, and creating the graphical user interface from a general interface by populating the general interface with the visualization from each node in the subhierarchy according to the ordering. The method further includes providing the graphical user interface.;1
165;10091545;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04N21/439,G06F3/165,G06F3/167,G10L15/26,H04N21/233,H04N21/234336,H04N21/2541,H04N21/4108,H04N21/4122,H04N21/4126,H04N21/42203,H04N21/437,H04N21/4415,H04N21/4751,H04N21/64322,H04N21/6547,H04N21/8113,H04N21/8586,G10L25/51;Methods and systems for detecting audio output of associated device;Systems and methods for determining whether a first electronic device detects a media item that is to be output by a second electronic device is described herein. In some embodiments, an individual may request, using a first electronic device, that a media item be played on a second electronic device. The backend system may send first audio data representing a first response to the first electronic device, along with instructions to delay outputting the first response, as well as to continue sending audio data of additional audio captured thereby. The backend system may also send second audio data representing a second response to the second electronic device along with the media item. Text data may be generated representing the captured audio, which may then be compared with text data representing the second response to determine whether or not they match.;1
166;10109182;2017;2018;DSP GROUP LTD.;DSP GROUP LTD.;G08C23/02,G06F3/167,G10L15/22,G10L15/28,G10L21/0232,G08C2201/31,G10L2015/223;Voice command conversion;"A method and a system for voice command conversion, the system may include one or more microphones for sensing a voice command for controlling an acoustically controlled device; one or more speech recognition units for identifying the voice command; an ultrasonic command generator for generating an ultrasonic command that represents the voice command; and one or more speakers for transmitting the ultrasonic command to the acoustically controlled device.";0
167;10127918;2017;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G11B27/038,G10L21/02,G10L15/20,G10L25/51;Methods for reconstructing an audio signal;A system configured to reconstruct audio signals. The system may identify missing audio samples due to packet loss or detect distortion caused by audio clipping and may reconstruct the audio data. The system may employ a forward-looking neural network that recursively predicts audio samples based on previous audio samples and/or a backward-looking neural network that recursively predicts audio samples based on subsequent audio samples. The system may generate audio data using only the forward-looking neural network for low latency applications or may generate audio data using both neural networks for mid to high latency applications. To reduce distortion in output audio data, the system may generate the audio data by cross-fading between outputs of the neural networks and/or may cross-fade between the generated audio data and the input audio data.;1
168;10134131;2017;2018;GOOGLE LLC;GOOGLE LLC;G06N3/08,G06N3/045,G06N3/088,G06T7/0014,G06T2207/20084,G06T2207/30024;Phenotype analysis of cellular image data using a deep metric network;The disclosure relates to phenotype analysis of cellular image data using a machine-learned, deep metric network model. An example method includes receiving, by a computing device, a target image of a target biological cell having a target phenotype. Further, the method includes obtaining, by the computing device, semantic embeddings associated with the target image and each of a plurality of candidate images of candidate biological cells each having a respective candidate phenotype. The semantic embeddings are generated using a machine-learned, deep metric network model. In addition, the method includes determining, by the computing device, a similarity score for each candidate image. Determining the similarity score for a respective candidate image includes computing a vector distance between the respective candidate image and the target image. The similarity score for each candidate image represents a degree of similarity between the target phenotype and the respective candidate phenotype.;0
169;10140089;2017;2018;"2236008 Ontario Inc.;BlackBerry Limited";2236008 Ontario Inc.;G06F3/167,G10L19/0018,G10L21/02,G10L21/0208,H04M9/082,G10L13/00,G10L15/26,G10L2015/025,G10L2021/02082;Synthetic speech for in vehicle communication;A system and method that enhances spoken utterances by capturing one or more microphone signals. The system and method estimates a plurality of echo paths from each of the one or more microphone signals and synthesizes a speech reinforcement signal in response to and corresponding to the one or more microphone signals. The system and method concatenates portions of the synthesized reinforcement signal with the captured microphone signals and processes the captured microphone signals in response to the estimated plurality of echo paths.;0
171;10140973;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/10,G06F40/247,G06F40/30,G06N7/01,G10L13/07,G10L15/26,G10L13/06;Text-to-speech processing using previously speech processed data;Systems, methods, and devices for generating text-to-speech output using previously captured speech are described. Spoken audio is obtained and undergoes speech processing to create text. The resulting text is stored with the spoken audio, with both the text and the spoken audio being associated with the individual that spoke the audio. Various spoken audio and corresponding text are stored over time to create a library of speech units. When the individual sends a text message to a recipient, the text message is processed to determine portions of text, and the portions of text are compared to the library of text associated with the individual. When text in the library is identified, the system selects the spoken audio units associated with the identified stored text. The selected spoken audio units are then used to generate output audio data corresponding to the original text message, with the output audio data being sent to a device of the message recipient.;4
172;10140981;2014;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/183,G10L15/16,G10L15/193;Dynamic arc weights in speech recognition models;Features are disclosed for performing speech recognition on utterances using dynamic weights with speech recognition models. An automatic speech recognition system may use a general speech recognition model, such a large finite state transducer-based language model, to generate speech recognition results for various utterances. The general speech recognition model may include sub-models or other portions that are customized for particular tasks, such as speech recognition on utterances regarding particular topics. Individual weights within the general speech recognition model can be dynamically replaced based on the context in which an utterance is made or received, thereby providing a further degree of customization without requiring additional speech recognition models to generated, maintained, or loaded.;1
173;10142485;2018;2018;Capital One Services, LLC;Capital One Services, LLC;H04M3/568,H04L65/4038,H04M3/566,H04M2250/62;Conference call mute management;A device receives call data associated with a conference call between a plurality of users. The call data includes at least two of: data identifying each of the plurality of users, data identifying user activity associated with at least one user in the plurality of users, and conference call metadata associated with the conference call. The call muting platform receives voice data associated with the conference call. The voice data is associated with at least one vocal utterance associated with the plurality of users. The call muting platform provides the call data and the voice data to a second device, and receives mute data from the second device. The mute data indicates whether to mute or un-mute a particular user of the plurality of users.;0
174;10147428;2018;2018;GREEN KEY TECHNOLOGIES, INC.;GREEN KEY TECHNOLOGIES, INC.;G10L15/26,G10L15/04,G10L15/22,G10L17/00,G10L25/78,G10L15/005,G10L15/01,G10L15/063,G10L15/142,G10L15/16,G10L15/32,G10L25/51;Computer systems exhibiting improved computer speed and transcription accuracy of automatic speech transcription (AST) based on a multiple speech-to-text engines and methods of use thereof;"In some embodiments, an exemplary inventive system for improving computer speed and accuracy of automatic speech transcription includes at least components of: a computer processor configured to perform: generating a recognition model specification for a plurality of distinct speech-to-text transcription engines; where each distinct speech-to-text transcription engine corresponds to a respective distinct speech recognition model; receiving at least one audio recording representing a speech of a person; segmenting the audio recording into a plurality of audio segments; determining a respective distinct speech-to-text transcription engine to transcribe a respective audio segment; receiving, from the respective transcription engine, a hypothesis for the respective audio segment; accepting the hypothesis to remove a need to submit the respective audio segment to another distinct speech-to-text transcription engine, resulting in the improved computer speed and the accuracy of automatic speech transcription; and generating a transcript of the audio recording from respective accepted hypotheses for the plurality of audio segments.";1
175;10147439;2017;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L21/034,G01S5/18,G01S11/14,G10L13/00,G10L25/21,G10L2015/223,G10L2021/02166;Volume adjustment for listening environment;A speech-capturing device that can modulate its output audio data volume based on environmental sound conditions at the location of a user speaking to the device. The device detects the sound pressure of a spoken utterance at the device location and determines the distance of the user from the device. The device also detects the sound pressure of noise at the device and uses information about the location of the noise source and user to determine the sound pressure of noise at the location of the talker. The device can then adjust the gain for output audio (such as a spoken response to the utterance) to ensure that the output audio is at a certain desired sound pressure when it reaches the location of the user.;0
176;10154242;2017;2018;GOOGLE LLC;GOOGLE LLC;H04N13/128,H04N13/261;Conversion of 2D image to 3D video;A two-dimensional input image to be used in a creation of a three-dimensional video may be received and depth values for pixels in the image may be determined. A depth map may be generated based on the depth values for the pixels and pixel shift values for the pixels may be calculated based on the depth map and a view disparity value. A modified image corresponding to a particular frame of the three-dimensional video may be generated based on the input image and the pixel shift values. An additional modified image corresponding to a next frame of the three-dimensional video may be generated based on the modified image and the pixel shift values used to generate the modified image where the modified image in combination with the input image and the additional modified image are a sequence of frames in the three-dimensional video.;4
177;10163436;2016;2018;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/1822,G06F40/295,G06F40/35,G10L13/00,G10L2015/223;Training a speech processing system using spoken utterances;Systems, methods, and devices for training a Natural Language Understanding (NLU) component of a system using spoken utterances of individuals are described. A server sends a device, such as a speech-controlled device, a signal that causes the device to output audio soliciting content regarding how a user would speak a particular command for execution by a particular application. The device captures spoken audio and sends it to the server. The server performs speech processing on received audio data to parse the audio data into multiple portions. The server then associates a first portion of the audio data with a command indicator and a second portion of the audio data with a content indicator. The associated data is then used to update how the NLU component determines how utterances triggering the command are spoken.;1
178;10169826;2014;2019;INTUIT INC.;INTUIT INC.;G06Q40/123;System and method for generating explanations for tax calculations;A computer-implemented method for generating explanations for a tax calculation or operation performed by tax preparation software is disclosed. A computing device executes a tax calculation engine in connection with the tax preparation software and operates on a tax calculation graph to perform a tax calculation. The tax calculation graph semantically describes data dependent tax operations comprising functional nodes connected to input nodes by one of a plurality of functions, wherein each tax operation is associated with one or more explanations. An explanation engine is executed to generate a narrative explanation from the one or more explanations associated with one of the tax operations and is presented to the user on the computing device.;2
179;10171662;2017;2019;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04M3/5175,G06Q10/0635,G10L25/63,H04L12/1831,H04L51/02,H04M3/5166,G06Q30/016,G10L15/22;Intervention in conversation between virtual agent and user;A computer-implemented method, apparatus, computer program product for intervention in a conversation between a virtual agent and a user is disclosed. In the computer-implemented method, a risk level of the conversation is evaluated. In response to the risk level being higher than a threshold, it is indicated that the conversation needs an intervention by a human agent. In response to the human agent intervening in the conversation, the conversation is handed over from the virtual agent to the human agent. Then, a simulative conversation is generated according to the conversation between the user and the human agent. In response to a determination by the human agent based on the simulative conversation, the conversation is handed over from the human agent to the virtual agent.;1
180;10171908;2016;2019;EVERNOTE CORPORATION;EVERNOTE CORPORATION;H04R3/005,G06F40/169,G10L15/26,G10L17/00,G10L21/028,G10L25/51,G10L2021/02087,H04R3/04,H04R2430/01,H04R2499/11;Recording meeting audio via multiple individual smartphones;Recording audio information from a meeting includes determining which audio input audio device (smartphones) correspond to which meeting participant, measuring volume levels in response to each of the meeting participants speaking, identifying one of the participants is speaking based on stored voice profiles and/or relative volume levels at each of the smartphones, recording on a first channel audio input at a first smartphone corresponding to the speaker, identifying another one of the participants is speaking based on stored voice profiles and/or relative volume levels at each of the smartphones, recording on a second channel, separate from the first channel, audio input at a second smartphone corresponding to the other speaker, and merging the first and second channels to provide a storyboard that includes audio input from the channels and identification of speakers based on which specific ones of the channels contains the audio input.;0
181;10176809;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/30,G10L15/18,G10L19/18,G10L19/22,G10L25/90,G10L25/93,H04L12/2818;Customized compression and decompression of audio data;Systems and methods for compressing and decompressing audio data are described. A server may receive input audio data corresponding to a spoken utterance from a speech-controlled device. The server performs speech processing on the input audio data to determine a spoken command and spoken solicited content. The server may then communication with a device associated with an application to obtain output audio data. The server may compress the output audio data by removing portions of audio data therefrom at regular intervals (e.g., milliseconds). The server may then send the compressed output audio data and instructions for decompressing the compressed output audio data to the speech-controlled device. Further, a speech-capturing device may also compress audio data corresponding to an utterance. The speech-capturing device may compress based on various factors, including the speech characteristics of the speaking user.;1
182;10185709;2012;2019;IMDB.COM, INC.;IMDB.COM, INC.;G06F40/186,G06F40/58,G06F40/20,G06F40/40;Dynamic creation and storage of narratives in multiple languages based in part on the popularity of the subject entity;Disclosed are various embodiments for creating and providing narratives associated with a type of subject (e.g., person, movie, place, etc.). Following a detected change in the system, a narrative template is selected based on a variety of factors. The template may provide variations of sentences based on the amount of data provided. Following the selection of the optimal narrative, fields in the selected template are filled with stored data associated with the subject. Some of the stored data may need to be translated prior to adding to the template depending on whether the selected narrative template is in a different language. The new or updated narrative may be stored in a memory. The system may encode a network page with the narrative and send the encoded network page to a client device.;1
183;10186251;2016;2019;OBEN, INC.;OBEN, INC.;G10L13/047,G10L13/0335,G10L15/26,G10L25/30;Voice conversion using deep neural network with intermediate voice training;"A system and method of converting source speech to target speech using intermediate speech data is disclosed. The method comprises identifying intermediate speech data that match target voice training data based on acoustic features; performing dynamic time warping to match the second set of acoustic features of intermediate speech data and the first set of acoustic features of target voice training data; training a neural network to convert the intermediate speech data to target voice training data; receiving source speech data; converting the source speech data to an intermediate speech; converting the intermediate speech to a target speech sequence using the neural network; and converting the target speech sequence to target speech using the pitch from the target voice training data.";3
184;10186252;2016;2019;OBEN, INC.;OBEN, INC.;G10L13/10,G10L2013/105;Text to speech synthesis using deep neural network with constant unit length spectrogram;A system and method for converting text to speech is disclosed. The text is decomposed into a sequence of phonemes and a text feature matrix constructed to define the manner in which the phonemes are pronounced and accented. A spectrum generator then queries a neural network to produce normalized spectrograms based on the input of the sequence of phonemes and features. Normalized spectrograms are fixed-length spectrograms with uniform temporal length (i.e., data size), which enables them to be effectively encoded into a neural network representation. A duration generator output a plurality of durations that are associated with phonemes. A speech synthesizer modifies the temporal length (i.e., de-normalizes) of each normalized spectrogram based on the associated duration, and then combines the plurality of modified spectrograms into speech. To de-normalize the spectrograms retrieved from the neural network, the normalized spectrograms are generally expanded in time or compressed in time, thereby producing variable length spectrograms which yield speech that is realistic sounding.;5
186;10229111;2017;2019;GOOGLE LLC;GOOGLE LLC;G06F40/284,G06F40/30,G06F40/42,G06N3/044,G06N3/045;Sentence compression using recurrent neural networks;Methods, systems, apparatus, including computer programs encoded on computer storage medium, for generating a sentence summary. In one aspect, the method includes actions of tokenizing the sentence into a plurality of tokens, processing data representative of each token in a first order using an LSTM neural network to initialize an internal state of a second LSTM neural network, processing data representative of each token in a second order using the second LSTM neural network, comprising, for each token in the sentence: processing the data representative of the token using the second LSTM neural network in accordance with a current internal state of the second LSTM neural network to (i) generate an LSTM output for the token, and (ii) to update the current internal state of the second LSTM neural network, and generating the summarized version of the sentence using the outputs of the second LSTM neural network for the tokens.;2
187;10229672;2017;2019;GOOGLE LLC;GOOGLE LLC;G10L15/16,G10L15/187,G10L15/30,G10L2015/022;Training acoustic models using connectionist temporal classification;Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training acoustic models and using the trained acoustic models. A connectionist temporal classification (CTC) acoustic model is accessed, the CTC acoustic model having been trained using a context-dependent state inventory generated from approximate phonetic alignments determined by another CTC acoustic model trained without fixed alignment targets. Audio data for a portion of an utterance is received. Input data corresponding to the received audio data is provided to the accessed CTC acoustic model. Data indicating a transcription for the utterance is generated based on output that the accessed CTC acoustic model produced in response to the input data. The data indicating the transcription is provided as output of an automated speech recognition service.;1
188;10249314;2017;2019;OBEN, INC.;OBEN, INC.;G10L21/007,G06N3/08,G10L21/003,G10L25/18,G10L25/24,G10L25/30;Voice conversion system and method with variance and spectrum compensation;"A voice conversion system for generating realistic, natural-sounding target speech is disclosed. The voice conversion system preferably comprises a neural network for converting the source speech data to estimated target speech data; a global variance correction module; a modulation spectrum correction module; and a waveform generator. The global variance correction module is configured to scale and shift (or normalize and de-normalize) the estimated target speech based on (i) a mean and standard deviation of the source speech data, and further based on (ii) a mean and standard deviation of the estimated target speech data. The modulation spectrum correction module is configured to apply a plurality of filters to the estimated target speech data after it has been scaled and shifted by the global variance correction module. Each filter is designed to correct the trajectory representing the curve of one MCEP coefficient over time. Collectively, the plurality of filters are designed to correct the trajectories of each of the MCEP coefficients in the target voice data being generated from the source speech data. Once the MCEP coefficients are corrected, they are then provided to a waveform generator configured to generate the target voice signal that can then be played to the user via a speaker.";3
189;10262107;2016;2019;;;G16H50/30,G06N20/00,G16B30/00,G16H15/00,G16H20/10,G16H70/40,G06N3/045,G06N7/01;Pharmacogenetic drug interaction management system;"A system includes a substance to be consumed by a subject and one or more indicia labeling the substance with: genomic biomarkers; drug exposure and clinical response variability; risk for adverse events; genotype-specific dosing; polymorphic drug target and disposition genes; and treatment based on the biomarker.";0
190;10271768;2011;2019;Jogohealth Inc.;Jogohealth Inc.;A61B5/11,A61B5/1124,A61B5/389,A61H1/00,A61B5/1122,A61B5/4082,A61B5/4836,A61B2505/09,A61H1/02,A61H2201/0165,A61H2201/5012,A61H2201/5043,A61H2201/5058,A61H2201/5061,A61H2201/5069,A61H2201/5097,A61H2230/60,A61N1/0492,A61N1/36003;Method for determining rehab protocol and behavior shaping target for rehabilitation of neuromuscular disorders;A system for treating a patient with neurological disorders of movement includes a patient computing device for use in rehabilitative training and a sensor worn about a body part being rehabilitated. A healthcare computing device is used by a healthcare professional to assist remote patient rehabilitation by accepting input signals and determining for the patient a rehab protocol depending on selected parameters, and determining for the patient a behavior shaping target depending on selected parameters and the rehab protocol and behavior shaping target is communicated to the patient while the patient is undergoing rehabilitation. A plurality of remote health data sites and other public repositories of health data of patients undergoing rehabilitation following neurological events can be included. The remote computing device can include a data repository of publicly available patient data and patient data gathered by system of present invention.;0
191;10272341;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;A63F13/67,A63F13/46;Procedural level generation for games;Embodiments provide techniques for generating game levels based on estimated quality score. For example, a set of training data can be retrieved that specifies attributes and quality scores for each of a plurality of game levels. One embodiment trains a machine learning model for the gaming application using the set of training data. A plurality of generated game levels are generated using a procedural level generation algorithm. Embodiments can determine, for each generated game level, a respective quality score, using the trained machine learning model, and upon determining that the quality score for the generated game level exceeds a threshold level of quality, can select the game level for transmission to a client of the gaming application.;5
192;10276149;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/033,G10L13/06,G10L15/26;Dynamic text-to-speech output;Systems, methods, and devices for dynamically outputting TTS content are disclosed. A speech-controlled device captures a spoken command, and sends audio data corresponding thereto to a server(s). The server(s) determines output content responsive to the spoken command. The server(s) may also determine a user that spoke the command and determine an average speech characteristic (e.g., tone, pitch, speed, number of words, etc.) used by the user when speaking commands. The server(s) may also determine a speech characteristic of the presently spoken command, as well as determine a difference between the speech characteristic of the presently spoken command and the average speech characteristic of the user. The server(s) may then cause the speech-controlled device to output audio based on the difference.;1
193;10283163;2018;2019;Wipro Limited;Wipro Limited;G11B27/031,G06F16/735,G06F16/783,G11B27/34;Method and system for generating video content based on user data;The present disclosure discloses method and video generation system for generating video content based on user data. The video generation system receives user data sequentially from user, where each sequence of user data is converted into text data. One or more objects, relations, emotions, and actions from user data is identified by evaluating text data, a scene descriptor is generated for each sequence of user data, by associating one or more objects with one or more relations, emotions, and actions. The method comprises performing consistency check for scene descriptor of each sequence of user data, based on one or more previously stored scene descriptors, performing, one or more modifications to inconsistent scene descriptors, identified based on consistency check, generating, segments for each of scene descriptor and generating video content for by combining video segments associated with each of scene descriptor.;5
194;10296552;2018;2019;FiaLEAF LIMITED;FiaLEAF LIMITED;G06F40/14,G06F16/951,G06F16/9536,G06F16/957,G06F16/9574,G06F16/986,G06F18/24,G06F21/00,G06Q30/02,G06Q30/0241,G06V20/00,G06V30/19173,G06V30/413;System and method for automated identification of internet advertising and creating rules for blocking of internet advertising;The system allows identifying new Internet advertising with minimal human participation. Also, the system provides a module for heuristic generation of rules for the found advertising, which makes it possible to automate the process of maintaining the relevance of lists of Internet advertising blocking rules. The system can operate in two modes: the mode of finding new advertising and generating rules for it and the mode of automatic data collection (datasets) for learning. The distinctive feature of the system is a comprehensive approach to the analysis of ad units that includes visual appearance of images (color contrast, element layout patterns, etc.), link analysis, and html code analysis.;0
195;10297070;2018;2019;Inception Institute of Artificial Intelligence Ltd;Inception Institute of Artificial Intelligence Ltd;G06T15/205,G06F18/214,G06F18/217,G06F18/241,G06N3/044,G06N3/045,G06N3/08,G06T17/00,G06V10/82,G06V20/00,G06V30/19173,G06T2210/04,G06T2210/61;3D scene synthesis techniques using neural network architectures;This disclosure relates to improved techniques for synthesizing three-dimensional (3D) scenes. The techniques can utilize a neural network architecture to analyze images for detecting objects, classifying scenes and objects, and determining degree of freedom information for objects in the images. These tasks can be performed by, at least in part, using inter-object and object-scene dependency information that captures the spatial correlations and dependencies among objects in the images, as well as the correlations and relationships of objects to scenes associated with the images. 3D scenes corresponding to the images can then be synthesized using the inferences provided by the neural network architecture.;3
196;10298895;2018;2019;Wipro Limited;Wipro Limited;H04N9/43,G06V20/47,G10L15/1822,G10L15/26,G06F2218/08;Method and system for performing context-based transformation of a video;Disclosed herein is a method and system for performing context-based transformation of a video. In an embodiment, a scene descriptor and a textual descriptor are generated for each scene corresponding to the video. Further, an audio context descriptor is generated based on semantic analysis of the textual descriptor. Subsequently, the audio context descriptor and the scene descriptor are correlated to generate a scene context descriptor for each scene. Finally, the video is translated using the scene context descriptor, thereby transforming the video based on context. In some embodiments, the method of present disclosure is capable of automatically changing one or more attributes, such as color of one or more scenes in the video, in response to change in the context of audio/speech signals corresponding to the video. Thus, the present method helps in effective rendering of a video to users.;6
197;10303978;2018;2019;Clinc, Inc.;Clinc, Inc.;G06F16/3329,G06F18/214,G06N3/044,G06N3/045,G06N3/084,G06N3/088,G06N5/04,G06N20/00,G06N20/20,G06N3/047,G06N5/01,G06N5/025,G06N7/01,G06N20/10;Systems and methods for intelligently curating machine learning training data and improving machine learning model performance;"Systems and methods of intelligent formation and acquisition of machine learning training data for implementing an artificially intelligent dialogue system includes constructing a corpora of machine learning test corpus that comprise a plurality of historical queries and commands sampled from production logs of a deployed dialogue system; configuring training data sourcing parameters to source a corpora of raw machine learning training data from remote sources of machine learning training data; calculating efficacy metrics of the corpora of raw machine learning training data, wherein calculating the efficacy metrics includes calculating one or more of a coverage metric value and a diversity metric value of the corpora of raw machine learning training data; using the corpora of raw machine learning training data to train the at least one machine learning classifier if the calculated coverage metric value of the corpora of machine learning training data satisfies a minimum coverage metric threshold.";0
198;10311334;2018;2019;Capital One Services, LLC;Capital One Services, LLC;G06V10/82,G06F18/2115,G06F18/2148,G06F18/22,G06F18/2414,G06N3/045,G06N3/047,G06N3/084,G06V10/454,G06V10/764,G06V10/7747,G06V40/161,G06V40/174;Learning to process images depicting faces without leveraging sensitive attributes in deep learning models;Systems, methods, and articles of manufacture to generate, by a neural network of a variational autoencoder, a latent vector for a first input image, generate, by the neural network of the variational autoencoder, a first reconstructed image by sampling the latent vector for the first input image, determine a reconstruction loss incurred in generating the first reconstructed image based at least in part on: (i) a difference of the first input image and the first reconstructed image, and (ii) a master model trained to detect a sensitive attribute in images, determine a total loss based at least in part on the reconstruction loss and a classification loss, and optimize a plurality of weights of the neural network of the variational autoencoder based on a backpropagation operation and the determined total loss, the optimized neural network trained to not consider the sensitive attribute in images.;2
199;10315536;2017;2019;;;B60N2/2806,G06F16/243,G06F16/3344,G06F16/93,G06N3/044,G06N3/08,B60N2/2818,B60N2002/2815;Seat belt holding clip;The Seat Belt Holding clip is for holding the seat belt straps out of the way when placing individuals in and out of car seats and chairs. The Seat Belt Holding Clip's purpose is to help people with seat belt issues and keep it out of the way when not in use. It will work for harnesses or over the shoulder seat belts and can easily access the seat belt and secure it when needed. The Seat Belt Holder is primarily used for all types of car seats and devices with harnesses. The gripping clamp is used to hoist the belt out of the way by holding the belt itself and can be released by pressing the squeeze handles. The strap attached to the Seat Belt Holder is to attach the device to any particular object that it will be used on.;0
200;10319365;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/10,G10L15/26,G10L13/06,G10L2013/105;Text-to-speech processing with emphasized output audio;Systems and methods for generating output audio with emphasized portions are described. Spoken audio is obtained and undergoes speech processing (e.g., ASR and optionally NLU) to create text. It may be determined that the resulting text includes a portion that should be emphasized (e.g., an interjection) using at least one of knowledge of an application run on a device that captured the spoken audio, prosodic analysis, and/or linguistic analysis. The portion of text to be emphasized may be tagged (e.g., using a Speech Synthesis Markup Language (SSML) tag). TTS processing is then performed on the tagged text to create output audio including an emphasized portion corresponding to the tagged portion of the text.;3
201;10325201;2019;2019;STRADVISION, INC.;STRADVISION, INC.;G06N3/084,G06F18/24143,G06F18/25,G06F18/254,G06N3/045,G06N3/047,G06N3/088,G06V10/764,G06V10/809,G06V10/82,G06V20/41,G06V20/52,G06V20/44;Method and device for generating deceivable composite image by using GAN including generating neural network and discriminating neural network to allow surveillance system to recognize surroundings and detect rare event more accurately;"A method for generating a deceivable composite image by using a GAN (Generative Adversarial Network) including a generating and a discriminating neural network to allow a surveillance system to recognize surroundings and detect a rare event, such as hazardous situations, more accurately by using a heterogeneous sensor fusion is provided. The method includes steps of: a computing device, generating location candidates of a rare object on a background image, and selecting a specific location candidate among the location candidates as an optimal location of the rare object by referring to candidate scores; inserting a rare object image into the optimal location, generating an initial composite image; and adjusting color values corresponding to each of pixels in the initial composite image, generating the deceivable composite image. Further, the method may be applicable to a pedestrian assistant system and a route planning by using 3D maps, GPS, smartphones, V2X communications, etc.";3
202;10325599;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/30,G06F40/295,G10L13/00,G10L15/1815,G10L15/22,H04L51/00,H04L51/02,H04M3/42068,H04M3/5307,H04M7/0042,H04W4/12,G10L13/02,G10L15/02,G10L15/142,G10L2015/223,H04M3/53341,H04M2201/40,H04W4/14;Message response routing;Systems and methods for extracting contact information from a message are described. A system can receive a message for a recipient, where the message originates from a message source having a first contact identifier (i.e., phone number, text address, etc.). The system can determine text data associated with the content of that message and process the text data to determine that the message refers to a second contact identifier that is different from the first contact identifier. The system may output the message to a recipient device (such as using text-to-speech, etc.) and may store an association between the message source and the second contact identifier. When the recipient speaks a command to reply to the first message or contact the message source, the system may determine the reply is intended for the message source and may route the reply using the second contact identifier included in the first message.;0
203;10327661;2015;2019;"Board of Supervisors of Louisiana State University and Agricultural and Mechanical College;Louisiana Tech Research Corporation";Board of Supervisors of Louisiana State University and Agricultural and Mechanical College;A61B5/349,A61B5/316,A61B5/369,A61B5/4094,A61B5/7275,G16H50/30,A61B5/02405,A61B5/352,A61B5/374,A61B5/7203,A61B5/7246,A61B5/7257,A61B5/726;Biomarkers for determining susceptibility to SUDEP;"A method for determining an increased risk of death of a patient includes receiving ECG data of the patient generated during a first time period; receiving EEG data of the patient generated during the first time period; composing a feature of the ECG data and a feature of the EEG data over a common time frame and determining a statistical measure of association between the ECG data and the EEG data; and determining whether the degree of association exceeds a predetermined threshold, thereby indicating whether an increased risk is present.";0
204;10332513;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G10L15/063,G10L15/183,H04M3/42204,G10L15/26,G10L17/00,G10L2015/0635,G10L2015/223,G10L2015/225,H04M3/00,H04M2201/40;Voice enablement and disablement of speech processing functionality;Methods and devices for enabling and disabling applications using voice are described herein. In some embodiments, an individual speak an utterance to their electronic device, which may send audio data representing the utterance to a backend system. The backend system may generate text data representing the utterance, and may determine that an intent of the utterance was for an application to be enabled or disabled for their user account on the backend system. If, for instance, the intent was to enable the application, the backend system may receive one or more rules for performing functionalities of the application, as well as one or more sample templates of sample utterances and sample responses that future utterances may use when requesting the application. Furthermore, one or more invocation phrases that may be used within the future utterances to invoke the application may be received, along with slot values for the sample templates.;1
205;10332517;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G06F21/32,G06F21/6245,G06F21/64,G10L15/26,G10L17/00,G10L2015/223;Privacy mode based on speaker identifier;Techniques for configuring a speech processing system with a privacy mode that is associated with the identity of a user that activated the privacy mode are described. A user may speak an indication to have the speech processing system activate a privacy mode. When such an indication is detected by the speech processing system, the speech processing system determines an identity of the user, determines a unique system identifier associated with the user, and generates a privacy mode flag. The speech processing system then associates the privacy mode flag with the user's unique system identifier. The privacy mode flag indicates to components of the speech processing system that any data related to processing of the user's utterances should not be sent to long term storage, thus causing various components of the system to delete data once the respective component is finished processing with respect to an utterance of the user.;0
206;10334328;2018;2019;;;H04N21/8549,G06F16/78,G06F16/783,G06N20/00,G11B27/031,H04N21/252,H04N21/25891,H04N21/44204,H04N21/8456,G06N3/045,G06N5/022,G06N5/025;Automatic video generation using auto-adaptive video story models;A video processing system generates and automatically updates videos. A client inputs a smart script that defines general preferences and information to include in a video. The video processing system generates a model of the story from the smart script. A story model comprises story beats that define order and characteristics of information that is presented in a story. The video processing system accesses client content (e.g., video clips, images, and testimonials from a client website) and assigns content items to story beats. The story model and associated content is rendered into a video. The video processing system can then adapt the story model in response to viewer feedback, external events, or other client-supplied parameters. Adapting the story model may include reordering, adding, or removing story beats, or altering parameter values associated with story beat characteristics. Thus, the video processing system can iteratively improve and update a video automatically.;6
207;10346524;2018;2019;SAP SE;SAP SE;G06F40/151,G06F40/30,G06N3/02,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06Q10/10,G06Q50/01;Position-dependent word salience estimation;Methods, systems, and computer-readable storage media for receiving two or more electronic documents, each electronic document including text data, a second electronic document including a link to a first electronic document, processing word representations of words of the first electronic document using a first encoder to provide first output and a context vector, processing text data of the second electronic document and the context vector using a first decoder to provide second output, determining, by an attention mechanism, a plurality of weights for each word in the text data of the first electronic document based on the first output, and the second output, and providing a word salience value for each word, a word salience value comprising a sum of weights of a respective word.;1
208;10365887;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G10L15/26,G10L2015/088,G10L2015/223,G10L2015/225;Generating commands based on location and wakeword;Systems and methods for generating command indications, via a computing device, based on audio data including a keyword are described. The computing device receives and processes audio data to determine whether the audio data includes a keyword. The keyword may be a device user identifier, such as an individual's name. Once a keyword is detecting, audio data surrounding the keyword is processed to determine a command contained within the surrounding data, and the command is conveyed to the computing device's user either audibly or visually. Alternatively, a location of the device is determined, a command is determined based on the device's location, and the command is conveyed to a user of the device either audibly or visually.;1
209;10373023;2019;2019;STRADVISION, INC.;STRADVISION, INC.;G06N3/088,G06F17/11,G06F18/2148,G06N3/045,G06N3/047,G06T7/246,G06T11/00,G06T15/10,G06T19/006,G06V10/764,G06V10/82,G06V20/13,G06T2207/20081;Learning method and learning device for runtime input transformation of real image on real world into virtual image on virtual world, to be used for object detection on real images, by using cycle GAN capable of being applied to domain adaptation;"A method for learning a runtime input transformation of real images into virtual images by using a cycle GAN capable of being applied to domain adaptation is provided. The method can be also performed in virtual driving environments. The method includes steps of: (a) (i) instructing first transformer to transform a first image to second image, (ii-1) instructing first discriminator to generate a 1_1-st result, and (ii-2) instructing second transformer to transform the second image to third image, whose characteristics are same as or similar to those of the real images; (b) (i) instructing the second transformer to transform a fourth image to fifth image, (ii-1) instructing second discriminator to generate a 2_1-st result, and (ii-2) instructing the first transformer to transform the fifth image to sixth image; (c) calculating losses. By the method, a gap between virtuality and reality can be reduced, and annotation costs can be reduced.";3
210;10373026;2019;2019;STRADVISION, INC.;STRADVISION, INC.;G06N3/084,G06F18/214,G06F18/217,G06N3/045,G06N3/047,G06N3/088,G06N5/046,G06V10/454,G06V10/82,G06V20/13,G06V20/56;Learning method and learning device for generation of virtual feature maps whose characteristics are same as or similar to those of real feature maps by using GAN capable of being applied to domain adaptation to be used in virtual driving environments;"A method of learning for deriving virtual feature maps from virtual images, whose characteristics are same as or similar to those of real feature maps derived from real images, by using GAN including a generating network and a discriminating network capable of being applied to domain adaptation is provided to be used in virtual driving environments. The method includes steps of: (a) a learning device instructing the generating network to apply convolutional operations to an input image, to thereby generate a output feature map, whose characteristics are same as or similar to those of the real feature maps; and (b) instructing a loss unit to generate losses by referring to an evaluation score, corresponding to the output feature map, generated by the discriminating network. By the method using a runtime input transformation, a gap between virtuality and reality can be reduced, and annotation costs can be reduced.";3
211;10379995;2019;2019;Capital One Services, LLC;Capital One Services, LLC;G06F9/541,G06F8/71,G06F9/54,G06F9/547,G06F11/3608,G06F11/3628,G06F11/3636,G06F11/3684,G06F11/3688,G06F16/215,G06F16/2237,G06F16/2264,G06F16/2423,G06F16/24568,G06F16/248,G06F16/254,G06F16/258,G06F16/283,G06F16/285,G06F16/288,G06F16/335,G06F16/35,G06F16/90332,G06F16/90335,G06F16/9038,G06F16/906,G06F16/93,G06F17/15,G06F17/16,G06F17/18,G06F18/2115,G06F18/213,G06F18/214,G06F18/2148,G06F18/217,G06F18/2193,G06F18/22,G06F18/23,G06F18/24,G06F18/2411,G06F18/2415,G06F18/285,G06F18/40,G06F21/552,G06F21/60,G06F21/6245,G06F21/6254,G06F30/20,G06F40/117,G06F40/166,G06F40/20,G06N3/04,G06N3/044,G06N3/045,G06N3/047,G06N3/06,G06N3/08,G06N3/084,G06N3/088,G06N5/00,G06N5/01,G06N5/02,G06N5/022,G06N5/04,G06N7/00,G06N7/01,G06N20/00,G06N20/10,G06N20/20,G06Q10/04,G06T7/194,G06T7/246,G06T7/248,G06T7/254,G06T11/001,G06V10/768,G06V10/993,G06V30/194,G06V30/1985,H04L63/1416,H04L63/1491,H04L67/306,H04L67/34,H04N21/23412,H04N21/8153,G06T2207/10016,G06T2207/10024,G06T2207/20081,G06T2207/20084;Systems and methods to identify breaking application program interface changes;Systems and methods for managing Application Programming Interfaces (APIs) are disclosed. For example, the system may include one or more memory units storing instructions and one or more processors configured to execute the instructions to perform operations. The operations may include sending a first call to a first node-testing model associated with a first API and receiving a first model output comprising a first model result and a first model-result category. The operations may include identifying a second node-testing model associated with a second API and sending a second call to the second node testing model. The operations may include receiving a second model output comprising a second model result and a second model-result category. The operations may include performing at least one of sending a notification, generating an updated first node-testing model, generating an updated second node-testing model, generating an updated first call, or generating an updated second call.;0
212;10380724;2019;2019;STRADVISION, INC.;STRADVISION, INC.;G06N3/084,G06F18/214,G06F18/217,G06F18/22,G06F18/2431,G06N3/04,G06N3/045,G06N3/047,G06N3/088,G06T5/60,G06T5/73,G06T5/80,G06T7/0002,G06V10/765,G06V10/82,G06T2207/20076,G06T2207/20081,G06T2207/20084,G06T2207/20201,G06T2207/30252,H04N23/682,H04N23/683;Learning method and learning device for reducing distortion occurred in warped image generated in process of stabilizing jittered image by using GAN to enhance fault tolerance and fluctuation robustness in extreme situations;"A method for learning reduction of distortion occurred in a warped image by using a GAN is provided for enhancing fault tolerance and fluctuation robustness in extreme situations. And the method includes steps of: (a) if an initial image is acquired, instructing an adjusting layer included in the generating network to adjust at least part of initial feature values, to thereby transform the initial image into an adjusted image; and (b) if at least part of (i) a naturality score, (ii) a maintenance score, and (iii) a similarity score are acquired, instructing a loss layer included in the generating network to generate a generating network loss by referring to said at least part of the naturality score, the maintenance score and the similarity score, and learn parameters of the generating network. Further, the method can be used for estimating behaviors, and detecting or tracking objects with high precision, etc.";1
214;10387970;2014;2019;INTUIT INC.;INTUIT INC.;G06Q40/123;Systems and methods for analyzing and generating explanations for changes in tax return results;Systems, methods and articles of manufacture for performing a comparison of tax results based on different sets of tax data, and generating an explanation as to why the tax results differ or do not differ due to the differences in the tax data. The system includes a computing device, a data store in communication with the computing device and a tax preparation software application executable by the computing device. The tax preparation software application has a tax calculation engine, a tax calculation graph, and a change analysis engine. The tax calculation engine is configured to perform a plurality of tax calculation operations based on the tax calculation graph. The change analysis engine is configured to determine whether tax results based on different tax data differ or do not differ. The system may also generate explanation(s) of the reasons that the tax results differ or do not differ due to the different tax data.;1
215;10395392;2019;2019;STRADVISION, INC.;STRADVISION, INC.;G06N3/08,G06F18/214,G06N3/045,G06N3/047,G06T7/90,G06T11/001,G06V10/20,G06V10/82,G06T2207/10024,G06T2207/20081,G06T2207/20084;Learning method and learning device for strategic transforming RGB training image sets into non-RGB training image sets, to be used for learning object detection on objects of images in non-RGB format, by using cycle GAN, resulting in significantly reducing computational load and reusing data;"A method for learning transformation of an annotated RGB image into an annotated Non-RGB image, in target color space, by using a cycle GAN and for domain adaptation capable of reducing annotation cost and optimizing customer requirements is provided. The method includes steps of: a learning device transforming a first image in an RGB format to a second image in a non-RGB format, determining whether the second image has a primary or a secondary non-RGB format, and transforming the second image to a third image in the RGB format; transforming a fourth image in the non-RGB format to a fifth image in the RGB format, determining whether the fifth image has a primary RGB format or a secondary RGB format, and transforming the fifth image to a sixth image in the non-RGB format. Further, by the method, training data can be generated even with virtual driving environments.";3
216;10402726;2018;2019;SPARKCOGNITION, INC.;SPARKCOGNITION, INC.;G06N3/088,G06N3/10,G06F3/04842,G06N3/045,G06N3/047,G06N5/01,G06N7/01,G06N20/20;Model building for simulation of one or more target features;A method includes receiving an input data set, each entry including multiple features. The method includes receiving a user input identifying a target feature of the multiple features and a target value of the target feature. The method includes determining, one or more correlated features of the multiple features. The method includes providing the input data set to multiple neural networks (including multiple VAEs) to train the multiple neural networks. The method includes generating a simulated data set based on the input data set, each entry including at least the target feature and the one or more correlated features. Values of the one or more correlated features are randomized or pseudorandomized and the target feature is fixed at the target value. The method includes providing the simulated data set to the multiple neural networks to generate output data and displaying a GUI based on the output data.;1
218;10418024;2018;2019;salesforce.com, inc.;salesforce.com, inc.;G10L13/0335,G10L13/047,G10L13/08,G10L21/003;Systems and methods of speech generation for target user given limited data;Systems and methods are provided for training an audio generation model for a first person using a first voice audio data and a first text transcript of the first voice audio data. Using a second voice audio data and a second text transcript of the second voice audio data, a plurality of pitch voice audio data for the second person may be generated with different pitches. The audio generation model may be trained for the second person using the generated plurality of pitch voice audio data with the different pitches for the second person. Output voice audio may be generated for the second person using received text and the model trained with the generated plurality of pitch voice audio data.;6
219;10418032;2016;2019;SoundHound, Inc.;SoundHound, Inc.;G06F16/3329,G10L15/22,G06F16/24522,G06F16/258,G10L15/1815,G10L15/19,G10L15/26;System and methods for a virtual assistant to manage and use context in a natural language dialog;A dialog with a conversational virtual assistant includes a sequence of user queries and systems responses. Queries are received and interpreted by a natural language understanding system. Dialog context information gathered from user queries and system responses is stored in a layered context data structure. Incomplete queries, which do not have sufficient information to result in an actionable interpretation, become actionable with use of context data. The system recognizes the need to access context data, and retrieves from context layers information required to transform the query into an executable one. The system may then act on the query and provide an appropriate response to the user. Context data buffers forget information, perhaps selectively, with the passage of time, and after a sufficient number and type of intervening queries.;3
220;10418033;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G06F16/00,G06F16/635,G10L13/08,G10L15/1815,G10L15/30,G10L17/22,G10L13/00,G10L15/00,G10L2015/223;Configurable output data formats;Configurable core domains of a speech processing system are described. A core domain output data format for a given command is originally configured with default content portions. When a user indicates additional content should be output for the command, the speech processing system creates a new output data format for the core domain. The new output data format is user specific and includes both default content portions as well as user preferred content portions.;1
221;10440431;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04N21/44218,H04N21/23418,H04N21/252,H04N21/25891,H04N21/44226,H04N21/4532,H04N21/4666,H04N21/4667,H04N21/812,H04N21/84,H04N21/8549;Adaptive and automatic video scripting;Systems and methods are provided for facilitating automated video scripting, Video frames may be analyzed to determine scores indicative of the association between a characteristic of the video frame and an attribute of a theme associated with a particular person. Then, video frames with particular scores can be added together to automatically create a video script. Neural networks can be used to determine the scores. The neural network may also be trained using training data, and updated based on the interaction of a person to a video script.;3
222;10445356;2017;2019;Pulselight Holdings, Inc.;Pulselight Holdings, Inc.;G06F16/345,G06F16/334,G06F40/30,G06F40/58,G06N3/044,G06N3/08,G06N3/084,G06N3/045,G06N7/01;Method and system for analyzing entities;A recurrent neural network (RNN) method implemented on a computer system is used to produce summaries of unstructured text generated by multiple networks of individuals interacting over time by encoding the unstructured text into intermediate representations and decoding the intermediate representations into summaries of each network. Parameter data for the RNN is obtained by using multiple different versions of the same source texts to train the computer system. The method and computer system can be used to identify which of the networks match a query by determining which network generates the query with low or lowest cost.;2
223;10446126;2018;2019;;;G10H1/0025,G10H1/0091,G10H1/20,G10H7/02,G10H2210/105,G10H2210/115,G10H2210/151,G10H2240/131,G10H2250/541;System for generation of musical audio composition;A musical audio composition is generated based on a content library. The library is a collection of sequences and instruments. Sequences are partial musical compositions, while instruments are groups of audio samples. Instruments are made of audio data and musical data describing the events recorded in the audio. The process begins by reading the library. A new chain is created. A succession of sequences are selected to create a series of segments in the chain. The events in the selected sequences determine the selection of instruments. Algorithms determine the final arrangements and exact modulations of source audio to target outputs. The source audio are modulated, mixed and output as a stream of audio data. Finally the selections and events of the finished segment are output as metadata. An unlimited number of segments can be fabricated in series, each building and evolving from the preceding segments in the chain.;2
224;10446147;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/205,G10L15/22,G06F3/167,G06F40/30,G10L15/063,G10L15/26,G10L2015/223;Contextual voice user interface;Techniques for providing a contextual voice user interface that enables a user to query a speech processing system with respect to the decisions made to answer the user's command are described. The speech processing system may store speech processing pipeline data used to process a command. At some point after the system outputs content deemed responsive to the command, a user may speak an utterance corresponding to an inquiry with respect to the processing performed to respond to the command. For example, the user may state âwhy did you tell me that?â In response thereto, the speech processing system may determine the stored speech processing pipeline data used to respond to the command, and may generate output audio data that describes the data and computing decisions involved in determining the content deemed responsive to the command.;2
225;10453434;2018;2019;;;G06F16/634,G10H1/0025,G06F3/0482,G06F7/582,G06F16/632,G06N7/00,G06N7/01,G10L13/06,G10L25/03,G10L25/18,G10L25/24,G10L25/54,G06N3/088,G06N3/126,G10H2210/041,G10H2240/081,G10H2240/141,G10H2240/145,G10H2250/235,G10H2250/261;System for synthesizing sounds from prototypes;"A system is presented for generation of output sounds having psychoacoustic qualities comparable to input sound or sounds. Short term and intermediate term features are computed for each input sound, sound components are clustered, filtered, and scored; and a prediction learning system is trained on the probabilities of classes of regions over time. A decoder can make use of this information to generate outputs that sound similar to, but not the same as, the input sound or sounds. The method and apparatus can be operated with no special training.";2
226;10453476;2017;2019;OBEN, INC.;OBEN, INC.;G10L25/30,G06N3/04,G06N3/045,G06N3/084,G10L21/007,G10L21/013,G10L25/24,G10L2021/0135;Split-model architecture for DNN-based small corpus voice conversion;A voice conversion system suitable for encoding small and large corpuses is disclosed. The voice conversion system comprises hardware including a neural network for generating estimated target speech data based on source speech data. The neural network includes an input layer, an output layer, and a novel split-model hidden layer. The input layer comprises a first portion and a second portion. The output layer comprises a third portion and a fourth portion. The hidden layer comprises a first subnet and a second subnet, wherein the first subnet is directly connected to the first portion of the input layer and the third portion of the output layer, and wherein the second subnet is directly connected to the second portion of the input layer and the fourth portion of the output layer. The first subnet and second subnet operate in parallel, and link to different but overlapping nodes of the input layer.;1
227;10467274;2017;2019;Snap Inc.;Snap Inc.;G06N3/006,G06F16/3344,G06F16/338,G06F18/256,G06N3/044,G06N3/045,G06N3/084,G06V10/811,G06V10/82,G06V10/94,G06N5/02,G06N5/022;Deep reinforcement learning-based captioning with embedding reward;An image captioning system and method is provided for generating a caption for an image. The image captioning system utilizes a policy network and a value network to generate the caption. The policy network serves as a local guidance and the value network serves as a global and lookahead guidance.;4
228;10467347;2016;2019;ARRIA DATA2TEXT LIMITED;ARRIA DATA2TEXT LIMITED;G06F40/157,G06F40/56,G06F16/3344,G06F16/93,G06F40/117,G06F40/211,G06F40/232,G06F40/253,G06F40/268,G06F40/284;Method and apparatus for natural language document orchestrator;Methods, apparatuses, and computer program products are described herein that are configured to be embodied as a natural language document orchestrator. In some example embodiments, a method is provided that comprises generating natural language text. The method of this embodiment may also include executing document orchestration requests from a scripted input language file.;2
229;10468014;2019;2019;Capital One Services, LLC;Capital One Services, LLC;G10L13/033,G10L25/51,G06F3/167,G10L15/07,G10L15/22,G10L17/26,H03G3/32;Updating a speech generation setting based on user speech;A device causes a communication session to be established between the device and a user device to allow the device and the user device to communicate speech, and receives user speech from the user device. The device processes the user speech using a natural language processing technique to determine a plurality of characteristics of the user speech, and updates a speech generation setting of a plurality of speech generation settings based on the plurality of characteristics of the user speech. The device generates, after updating the speech generation setting, device speech using a text-to-speech technique based on the speech generation setting, and sends the device speech to the user device.;1
230;10468019;2017;2019;Kadho, Inc.;Kadho, Inc.;G10L15/187,G10L15/02,G10L15/142,G10L15/16,G10L15/22,G10L15/30,G10L15/06,G10L25/51,G10L2015/025;System and method for automatic speech recognition using selection of speech models based on input characteristics;A method and system method for automatic speech recognition using selection of speech models based on input characteristics is disclosed herein. The method includes obtaining speech data from a speaker utilizing a microphone or an audio upload. The system and method select the best speech recognition model to automatically decode the input speech and continuously update models by updating/creating models in a database based on users speech abilities.;0
231;10469665;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04M3/5191,G06F3/0481,G06F40/205,G06F40/35,G06N3/044,G06N3/08,G06Q10/06,H04L51/02,H04L51/046,H04L51/216,H04L67/141,H04L67/306,H04L67/535,H04M3/42382,H04M3/5233,H04M3/527,G06N3/006;Workflow based communications routing;Disclosed are various embodiments for routing communications to service agents based on a workflow. A computing device identifies a user intent object corresponding to a user interaction with the computing device. The computing device then identifies a workflow corresponding to the user intent object. Later, the computing device identifies a service agent capable of completing the workflow. Then, the computing device routes a user to the service agent.;0
232;10475438;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/10,G10L13/033,G10L13/047,G10L2013/105;Contextual text-to-speech processing;A text-to-speech (TTS) system that is capable of considering characteristics of various portions of text data in order to create continuity between segments of synthesized speech. The system can analyze text portions of a work and create feature vectors including data corresponding to characteristics of the individual portions and/or the overall work. A TTS processing component can then consider feature vector(s) from other portions when performing TTS processing on text of a first portion, thus giving the TTS component some intelligence regarding other portions of the work, which can then result in more continuity between synthesized speech segments.;1
233;10481579;2019;2019;Nanotronics Imaging, Inc.;Nanotronics Imaging, Inc.;G05B19/406,G05B19/19,G05B19/4183,G06Q10/0633,G06Q10/0639,G05B19/41805,G05B2219/31027,G05B2219/31046,G05B2219/40556,G06N3/044,G06N3/045,G06N7/01,G06N20/10,G06N20/20,G06Q50/04,Y02P90/02;Dynamic training for assembly lines;Aspects of the disclosed technology provide an Artificial Intelligence Process Control (AIPC) for automatically detecting errors in a manufacturing workflow of an assembly line process, and performing error mitigation through the update of instructions or guidance given to assembly operators at various stations. In some implementations, the disclosed technology utilizes one or more machine-learning models to perform error detection and/or propagate instructions/assembly modifications necessary to rectify detected errors or to improve the product of manufacture.;1
234;10484542;2018;2019;;;H04M3/5183,G06F40/279,G06F40/35,H04L51/02,H04M3/42382,H04M2203/2061;System and method for hybridized chat automation;"In a system for managing blended agent and automated chat in a contact center setting, the system includes: a chat orchestration server invoking natural language processing on a received communication, selecting a standardized text communication responsive to the natural-language-processed communication, and transmitting the standardized text communication to an agent device; and a knowledge management server for determining confidence values and populating ranked lists of responsive standardized text communications.";1
235;10489393;2016;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/243,G10L15/22,G06F16/24522,G06F16/2455,G06F16/3329,G06F16/9038,G06F40/205,G06F40/211,G06F40/30,G10L15/26,G10L13/00;Quasi-semantic question answering;A voice-controlled question answering system that uses both a knowledge base and a detailed index of other sources that may be accessible over the Internet. The knowledge base is used to answer questions of more general interest whose answers are contained in the knowledge base. The index is used to answer more complex questions that are not answerable using the knowledge base. Web and other sources are analyzed to create the index, where text segments are indexed along with data describing the text segments in a quasi-semantic way. Quasi-semantic features are extracted from incoming spoken questions and used, along with machine learning trained models, to identify an indexed text segment that includes the answer to the question. The text segment may then be rearranged or simply passed to a speech synthesizer so the answer may be spoken aloud to a user.;3
236;10489682;2017;2019;Automation Anywhere, Inc.;Automation Anywhere, Inc.;G06N3/084,G06F18/214,G06F18/41,G06N3/04,G06N3/045,G06N3/08,G06V30/162,G06V30/19133,G06V30/19147,G06V30/224,G06V30/414,G06V30/416,G06N3/044,G06V30/10;Optical character recognition employing deep learning with machine generated training data;An optical character recognition system employs a deep learning system that is trained to process a plurality of images within a particular domain to identify images representing text within each image and to convert the images representing text to textually encoded data. The deep learning system is trained with training data generated from a corpus of real-life text segments that are generated by a plurality of OCR modules. Each of the OCR modules produces a real-life image/text tuple, and at least some of the OCR modules produce a confidence value corresponding to each real-life image/text tuple. Each OCR module is characterized by a conversion accuracy substantially below a desired accuracy for an identified domain. Synthetically generated text segments are produced by programmatically converting text strings to a corresponding image where each text string and corresponding image form a synthetic image/text tuple.;5
237;10490195;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L17/04,G06F3/167,G10L13/00,G10L15/1815,G10L15/22,G10L17/22;Using system command utterances to generate a speaker profile;"Systems, methods, and devices related to establishing voice identity profiles for use with voice-controlled devices are provided. The embodiments disclosed enhance user experience by customizing the enrollment process to utilize voice recognition for each user based on historical information which can be used in the selection process of phrases a user speaks during enrollment of a voice recognition function or skill. The selection process can utilize phrases that have already been spoken to the electronic device; it can utilize phrases, contacts, or other personalized information it can obtain from the user account of the person enrolling; it can use any of the information just described to select specific words to enhance the probably of achieving higher phonetic matches based on words the individual user is more likely to speak to the device.";1
238;10496754;2017;2019;Elemental Cognition Inc.;Elemental Cognition Inc.;G06F40/30,G06F16/22,G06F16/3329,G06F16/3331,G06F40/211,G06F40/35,G06F40/40,G06N3/006,G06N5/02,G06N5/027,G06N5/04,G06N7/01,G06N20/00,G10L15/26;Architecture and processes for computer learning and understanding;An architecture and processes enable computer learning and developing an understanding of arbitrary natural language text through collaboration with humans in the context of joint problem solving. The architecture ingests the text and then syntactically and semantically processes the text to infer an initial understanding of the text. The initial understanding is captured in a story model of semantic and frame structures. The story model is then tested through computer generated questions that are posed to humans through interactive dialog sessions. The knowledge gleaned from the humans is used to update the story model as well as the computing system's current world model of understanding. The process is repeated for multiple stories over time, enabling the computing system to grow in knowledge and thereby understand stories of increasingly higher reading comprehension levels.;1
239;10496884;2017;2019;DEEPRADIOLOGY, INC.;DEEPRADIOLOGY, INC.;G06T7/0012,G06N3/045,G06N3/084,G06V10/82,G06V20/20,G06V30/1916,G06V30/19173,G06V30/413,G06T2207/20081,G06T2207/20084,G06T2207/30176,G06V2201/03;Transformation of textbook information;Methods and systems for transforming image data and training or testing neural networks. Images from textbooks can contain valuable related text. Transforming the related text into discrete determinate labels can be performed using natural language processing. Once transformed, the images and the labels can be advantageously used together to train neural networks.;1
240;10503804;2019;2019;Capital One Services, LLC;Capital One Services, LLC;G06F16/9577,G06F8/22,G06F8/38,G06F8/77,G06F11/3438,G06F11/3672,G06F11/3684,G06F11/3692,G06F16/9535,G06F16/9536,G06F16/958,G06N3/126,G06F11/3419;System for automating the creation and evaluation of website variations to improve user engagement;Described is a system (and method) for automating the process of generating variations of a website that may be continually evaluated to improve a user interaction flow. The process may be based on a genetic algorithm that creates improving generations of websites. For example, the system may automate the creation of different layouts for a website and evaluate the effectiveness of each layout based on a particular user interaction goal. The most effective (e.g., top performing) layouts may be determined and then âbredâ as part of an iterative improvement process.;0
241;10504268;2018;2019;Educational Testing Service;Educational Testing Service;G06V10/82,G06F3/011,G06T11/60,G06T13/40,G06V10/764,G06V10/772,G06V40/174,G06T2200/24,H04N7/157;Systems and methods for generating facial expressions in a user interface;Example systems and methods are disclosed for generating facial expressions in an avatar-based user interface for a computing device. An example system may include a digital camera, a display device and a user interface application. The digital camera may be configured to capture image data of a user over time. The user interface application may be stored on a non-transitory computer-readable medium and executable by a processor, and may include: an expression feature extractor configured to process the image data of the user to generate a plurality of facial expression descriptor vectors, a sketch generation module configured to use a first conditional deep convolutional generative adversarial network (DC-GAN) model to generate an expressive facial sketch image of an avatar based on the plurality of facial expression descriptor vectors, and an image generation module configured to use a second conditional DC-GAN model to generate a facial expression image from the expressive facial sketch image. The display device may be configured to display the facial expression image as a user interface avatar.;3
242;10504504;2018;2019;VocaliD, INC.;VocaliD, INC.;G10L15/063,G10L25/51,G10L15/22,G10L25/18,G10L25/24,G10L25/27,G10L25/69;Image-based approaches to classifying audio data;Image-based machine learning approaches are used to classify audio data, such as speech data as authentic or otherwise. For example, audio data can be obtained and a visual representation of the audio data can be generated. The visual representation can include, for example, an image such as a spectrogram or other visual or electronic representation of the audio data. Before processing the image, the audio data and/or image may undergo various preprocessing techniques. Thereafter, the image representation of the audio data can be analyzed using a trained model to classify the audio data as authentic or otherwise.;1
243;10510358;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/02,G10L13/047,G10L13/08,G10L25/30;Resolution enhancement of speech signals for speech synthesis;An approach to speech synthesis uses two phases in which a relatively low quality waveform is computed, and that waveform is passed through an enhancement phase which generates the waveform that is ultimately used to produce the acoustic signal provided to the user. For example, the first phase and the second phase are each implemented using a separate artificial neural network. The two phases may be computationally preferable to using a direct approach to yield a synthesized waveform of comparable quality.;0
244;10511908;2019;2019;ADOBE INC.;ADOBE INC.;H04R3/04,G06N3/045,G06N3/047,G06N3/08,G06N20/00,G10L21/0224,G10L21/0232,G10L25/30;Audio denoising and normalization using image transforming neural network;Techniques are disclosed for reducing noise from an audio signal. A methodology implementing the techniques according to an embodiment includes generating a 2-dimensional (2D) spectrogram of a received audio signal and applying the 2D spectrogram to an image transformation neural network that is trained to transform the 2D spectrogram to generate an output spectrogram representing a denoised version of the received audio signal. The method further includes converting the output spectrogram to the time domain to generate the denoised audio signal. The neural network is trained on spectrogram images of clean and corrupted versions of training audio signals such that the trained neural network converts a spectrogram image of a corrupted audio signal into a spectrogram image more closely resembling a spectrogram of the associated clean audio signal. The denoising may also include removal of other degradation effects, including reverberation, unwanted voices, and unwanted music, from an audio signal.;1
245;10515637;2017;2019;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/30,G10L15/30,G06F40/279,G10L13/00,G10L15/1815,G10L15/1822,G10L15/183,G10L15/22,G10L25/63,G06F40/169,G06F40/205,G10L15/02,G10L2015/025,G10L2015/223;Dynamic speech processing;Techniques for dynamically maintaining speech processing data on a local device for frequently input commands are described. A system determines a usage history associated with a user profile. The usage history represents at least a first command. The system determines the first command is associated with an input frequency that satisfies an input frequently threshold. The system also determines the first command is missing from first speech processing data stored by a device associated with the user profile. The system then generates second speech processing data specific to the first command and sends the second speech processing data to the device.;1
246;10539881;2018;2020;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G03F7/70433,G06F30/27,G06F30/39,G06F30/398;Generation of hotspot-containing physical design layout patterns;A method for generating physical design layout patterns includes selecting as training data a set of physical design layout patterns of patterned structures. The method also includes training, utilizing physical design layout patterns containing hotspots, a first neural network model configured to generate synthetic physical design layout patterns, and training, utilizing physical design layout patterns that do and do not contain hotspots, a second neural network model configured to classify whether physical design layout patterns contain hotspots. The method further includes generating synthetic physical design layout patterns containing hotspots by utilizing the trained first neural network model to generate synthetic physical design layout patterns and utilizing the trained second neural network model to select the synthetic physical design layout patterns containing hotspots. The method further includes evaluating manufacturability of a given patterned structure comprising at least one of the synthetic physical design layout patterns containing at least one hotspot.;0
247;10546409;2018;2020;ADOBE INC.;ADOBE INC.;G06F3/167,G06T13/40,G06T13/00,G10L15/22,G11B27/031,G11B27/34,G06F40/221,G06T2200/24,G06T2213/08,G10L15/26,G10L2015/223;Animation production system;Techniques described herein relate to a streamlined animation production workflow that integrates script drafting, performance, and editing. A script including animation events is parsed to encode the animation events into nodes of a story model. The animation events are automatically triggered by a performance as a playhead advances through the story model and identifies active node(s). A command interface accepts various commands that allow a performer to act as a director by controlling recording and playback. Recording binds a generated animation event to each active node. Playback triggers generated animation events for active nodes. An animated movie is assembled from the generated animation events in the story model. The animated movie can be presented as a live preview to provide feedback to the performer, and a teleprompter interface can guide a performer by presenting and advancing the script to follow the performance.;1
248;10552667;2019;2020;Neon Evolution Inc.;Neon Evolution Inc.;G06T13/40,G06T11/60,G06T13/80,G06T15/205,G06V10/764,G06V10/776,G06V10/82,G06V40/161,G06V40/171,G06V40/172,G06V40/174,G06T2207/30201;Methods and systems for image processing;Systems and methods are disclosed configured to pre-train an autoencoder using images that include faces, wherein the autoencoder comprises an input layer, an encoder configured to output a latent image from a corresponding input image, and a decoder configured to attempt to reconstruct the input image from the latent image. An image sequence of a CGI sculpted and textured face exhibiting a plurality of facial expressions and transitions between facial expressions is accessed. Images of the plurality of facial expressions and transitions between facial expressions are captured from a plurality of different angles. The pre-trained autoencoder is trained using source images that include a CGI face with different facial expressions captured at different angles, and using destination images that include a real face. The trained autoencoder is used to generate an output where the real face in the destination images is swapped with the CGI face, while preserving expressions of the real face.;3
249;10559299;2019;2020;Apprente LLC;Apprente LLC;G10L15/063,G06N3/006,G06N3/044,G06N3/045,G06N3/047,G06N3/088,G06N5/046,G06N7/01,G06N20/00,G10L15/02,G10L15/16,G10L15/1822,G10L15/22,G10L2015/025,G10L2015/027,G10L2015/0635;Reconciliation between simulator and speech recognition output using sequence-to-sequence mapping;A synthetic training data item comprising a first sequence of symbols that represent a synthetic sentence output by a simulator is received. The synthetic training data item is processed using a machine learning model, which outputs a second sequence of symbols that represent the synthetic sentence. The synthetic training data item is modified by replacing the first sequence of symbols with the second sequence of symbols. A statistically significant mismatch exists between the first sequence of symbols and a third sequence of symbols that would be output by an acoustic model that processes a set of acoustic features that represent an utterance of the synthetic sentence, and no statistically significant mismatch exists between the second sequence of symbols and the third sequence of symbols. The modified synthetic training data item may be used to train a second machine learning model that processes data output by the acoustic model.;3
250;10565234;2019;2020;"ATLASSIAN PTY LTD.;ATLASSIAN, INC.";ATLASSIAN PTY LTD.;G06F16/285,G06F16/3347,G06F16/93,G06F40/12,G06F40/30,H04L41/5074,G06F40/284;Ticket classification systems and methods;"Described herein is a computer implemented method for identifying one or more classifications for a particular ticket maintained by an issue tracking system. The method comprises: receiving data in respect of the particular ticket; retrieving cluster data in respect of a set of clusters; calculating cluster membership data in respect of the particular ticket; and identifying, based on the cluster membership data, one or more specific clusters from the set of clusters. For each specific cluster a classification associated with the specific cluster is identified and returned.";0
251;10565989;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G10L15/22,G10L13/08,G10L15/063,G10L15/1815,G10L15/1822,G10L15/26,G10L2015/088,G10L2015/223;Ingesting device specific content;A system for easily importing content related to a device into a speech-controlled system in a manner that makes the content easily accessible using voice commands. A speech-controlled system that detect a device type from which audio data is received and can determine if the utterance of the audio data includes a query related to the specific device. The system can then obtain and ingest content related to the device and analyze that content to identify the portion of the content responsive to the query. The remaining content can be stored to potentially respond to future queries.;0
252;10567314;2018;2020;D8AI Inc.;D8AI Inc.;H04L51/02,G08C23/04,H04L51/10,H04M3/42323,H04M3/4936,H04M3/5191,H04M7/0012,H04M11/007,H04N7/155,H04N7/157;Programmable intelligent agents for human-chatbot communication;This invention provides programmable intelligent agents that facilitate and manage voice or video conversations between human users and chatbots over the Internet or the Public Switched Telephone Network. Functions of said intelligent agents include providing the communication connectivity, coordinating the human-chatbot conversation, reacting and responding to the human user's conversational behavior, and in certain applications, sending controlling signals to peripheral devices according to intents of the conversation, or receiving data from peripheral sensors as references to alter the course of the conversation. Furthermore, a said intelligent agent can serve as a user interface that enables human users in the vicinity of the intelligent agent to engage in an interactive three-way conversation with a chatbot and remote human users.;6
253;10572953;2016;2020;INTUIT INC.;INTUIT INC.;G06Q40/123;Computer-implemented systems and methods for preparing a tax return in which tax data is requested and entered ad hoc;Systems, methods and articles of manufacture for preparing a tax return in which tax questions may be generated and presented to a user in an ad hoc manner. A tax system includes a computing device executing a tax program. The tax program can display a plurality of user interface presentations, such as interview or question screens, with tax questions asking the user to provide tax data related to a taxpayer for preparing a tax return for the taxpayer. The tax program allows the user to skip a tax question on a user interface presentation, and proceed with subsequent user interface presentations. When the tax program determines that a response to the skipped tax questions is required, the tax program dynamically generates a skipped tax question user interface presentation with the skipped tax question and presents it to the user.;0
254;10580405;2016;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G10L15/22,G10L15/02,G10L15/30,G10L17/22,G10L17/00,G10L2015/223;Voice control of remote device;A system configured to enable remote control to allow a first user to provide assistance to a second user. The system may receive a command from the second user granting remote control to the first user, enabling the first user to initiate a voice command on behalf of the second user. In some examples, the system may enable the remote control by treating a voice command originating from the first user as though it originated from the second user instead. For example, the system may receive the voice command from a first device associated with the first user but may route the voice command as though it was received by a second device associated with the second user.;0
255;10586369;2018;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06T13/205,G06F40/279,G06F40/30,G06T13/40,G10L13/00,G10L13/08,G10L15/26,G06F16/3344,G06F16/38,G06T2213/04,G06T2213/08;Using dialog and contextual data of a virtual reality environment to create metadata to drive avatar animation;One or more services may generate audio data and animations of an avatar based on input text. A speech input ingestion (SII) service may identify tags of objects in a virtual environment and associate tags of those objects with words in the input text, which may be stored as metadata in speech markup data. This association may enable an animation service to generate gestures toward objects while animating an avatar, or may be used to create animations or effects of the object. The SII service may analyze input text to identify dialog including multiple speakers associated with the text. The SII service may create metadata to associate certain words with respective speakers (avatars) of those words, which may be processed by the animation service to animate multiple avatars speaking the dialog.;6
256;10593328;2016;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L15/18,G10L15/30,G10L21/0208,H04L65/1059,H04L65/1069,H04L67/125,H04L67/306,H04M3/42204,G10L2015/088,G10L2015/223,G10L2021/02082,H04M2203/1016;Voice control of remote device;A system configured to enable remote control to allow a first user to provide assistance to a second user. The system may receive a command from the second user granting remote control to the first user, enabling the first user to initiate a voice command on behalf of the second user. In some examples, the system may enable the remote control by enabling wakeword detection for incoming audio data, enabling a second device to detect a wakeword and corresponding voice command from incoming audio data originating from a first device. For example, the second device may disable and/or modify echo cancellation processing, enabling the second device to detect the voice command from audio output based on the incoming audio data and/or from the incoming audio data itself.;0
257;10594840;2018;2020;WEST CORPORATION;WEST CORPORATION;H04L67/34,H04L63/083,H04L69/329,H04W4/14,H04L51/02,H04L67/02,H04L67/142;Bot framework for channel agnostic applications;An example operation may include one or more of receiving a bot communication request from a user device, the bot communication request comprising content from on one or more of a spoken utterance and a typed input received via the user device, retrieving configuration information comprising credentials and unique communication protocol information for each of a plurality of bot services, automatically determining a bot service for responding to the bot communication request from among the plurality of bot services, and establishing a communication channel with the determined bot service based on unique credentials of the determined bot service, and transmitting the bot communication request to the determined bot service via the established communication channel based on unique communication protocol information of the determined bot service.;6
258;10600408;2018;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/20,G10L13/033,G10L13/10,G10L15/1807,G10L25/30,G10L25/51;Content output management based on speech quality;Techniques for ensuring content output to a user conforms to a quality of the user's speech, even when a speechlet or skill ignores the speech's quality, are described. When a system receives speech, the system determines an indicator of the speech's quality (e.g., whispered, shouted, fast, slow, etc.) and persists the indicator in memory. When the system receives output content from a speechlet or skill, the system checks whether the output content is in conformity with the speech quality indicator. If the content conforms to the speech quality indicator, the system may cause the content to be output to the user without further manipulation. But, if the content does not conform to the speech quality indicator, the system may manipulate the content to render it in conformity with the speech quality indicator and output the manipulated content to the user.;1
259;10600414;2018;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G10L15/08,G10L15/30,G10L2015/088,G10L2015/223,H04M7/006,H04M2203/1016;Voice control of remote device;A system configured to enable remote control to allow a first user to provide assistance to a second user. The system may receive a command from the second user granting remote control to the first user, enabling the first user to initiate a voice command on behalf of the second user. In some examples, the system may enable the remote control by treating a voice command originating from the first user as though it originated from the second user instead. For example, the system may receive the voice command from a first device associated with the first user but may route the voice command as though it was received by a second device associated with the second user. To enable this functionality, during a remote control session the first device may disable wakeword detection so that the voice command is correctly routed to the second device.;0
260;10601740;2019;2020;PROGRESSIVE CASUALTY INSURANCE COMPANY;PROGRESSIVE CASUALTY INSURANCE COMPANY;H04L51/02,H04L51/04,H04L67/10;Chatbot artificial intelligence;A system and method simulate a chat-based conversation with a human user. The system and method receive a text message through the chat system that enables a receiver to transmit an automatically generated response in real time during a chat session. The system and method recognize characters in a text message and generates an automatic prediction or classification using a conversational assistant pod that resides in a container. The system and method generate an automatic response and modify computing resources by replacing the conversational assistant pod and the container with a second conversational assistant pod and a second container without interrupting the chat session.;1
261;10602062;2018;2020;3I Inc.;3I Inc.;H04N23/698,G06Q30/0276,G11B27/031,G11B27/036,H04N5/272,H04N23/695;System and method for generating 360Â° video including advertisement;A system and method capable of generating a 360Â° video including advertising content by photographing only a video for a specific direction not an omni-direction covered by a 360Â° video. Devices and methods are capable of generating a 360Â° video using relatively small resources. In particular, embodiments provide a system and method capable of generating a 360Â° video conveniently and relatively accurately using a terminal carried by a user. Furthermore, embodiment provide a system and method capable of generating a 360Â° video by photographing only a video for a specific direction not an omni-directional view covered by a 360Â° video. Furthermore, embodiments provide advertising content in a 360Â° video.;3
262;10607298;2015;2020;INTUIT INC.;INTUIT INC.;G06Q40/123;System and method for indicating sections of electronic tax forms for which narrative explanations can be presented;Computer-implemented method, tax return preparation systems and computer program products for presenting an explanation for a tax calculation or operation performed by a tax return preparation system when the system is in âformsâ mode. An electronic version of a tax authority form that has been modified by inclusion of interface elements is displayed to a user. In response to the user selecting an interface element while in forms mode, presentation of an explanation regarding a tax calculation or operation for the associated field is invoked. The user interface controller provides data in response to selection of the interface element to an explanation engine, which determines an explanation based at least in part upon the tax calculation graph. The explanation is provided to the user interface controller for presentation to the user.;0
263;10607598;2019;2020;Capital One Services, LLC;Capital One Services, LLC;G10L15/063,G06F40/56,G06N3/044,G06N3/045,G06N3/08,G06N20/00,G10L15/07,G10L15/183,G10L15/26,G06F40/205,G06F40/289,G06F40/30,G10L13/00;Determining input data for speech processing;Aspects described herein may relate to the determination of data that is indicative of a greater range of speech properties than input text data. The determined data may be used as input to one or more speech processing tasks, such as model training, model validation, model testing, or classification. For example, after a model is trained based on the determined data, the model's performance may exhibit more resilience to a wider range of speech properties. The determined data may include one or more modified versions of the input text data. The one or more modified versions may be associated with the one or more speakers or accents and/or may be associated with one or more levels of semantic similarity in relation to the input text data. The one or more modified versions may be determined based on one or more machine learning algorithms.;1
264;10614031;2019;2020;Capital One Services, LLC;Capital One Services, LLC;G06F16/13,G06F16/148,G06N3/04,G06N3/044,G06N3/045,G06N3/047,G06N3/084,G06N3/088,G06N5/01,G06N20/20,G06N3/048,G06N5/048;Systems and methods for indexing and mapping data sets using feature matrices;The present disclosure relates to systems and methods for indexing and mapping data sets by feature matrices, comprising at least a processor and a non-transitory memory storing instructions that cause the processor to perform operations including receiving data sets of the same type, applying autoencoders to generate feature matrices, and generating a neural network model trained to generate synthetic data corresponding to the type of data files. Further, the processor performs operations to applying more autoencoders to part of the hidden layer of the neural network model to generate more corresponding feature matrices and indexing the data set using the feature matrices such that the data sets are searchable using an index wherein a search query is received and a third feature matrix is generated so that a data set can be retrieved and compared to the feature matrices using the index.;0
265;10614207;2019;2020;Capital One Services, LLC;Capital One Services, LLC;G06F21/36,G06F18/2148,G06F18/2185,G06N3/045,G06N3/047,G06N3/088,G06T7/194,G06T11/00,G06V10/7747,G06V10/7788,G06V10/82,G06F2221/2103,G06F2221/2133,G06N7/01;Generating captcha images using variations of the same object;Aspects described herein may allow for the application of generating captcha images using variations of the same object. A GAN model may generate objects and backgrounds of the captcha images and the model may be trained based on ground-truth images to obtain refined model parameters. Further aspects described herein may provide for generating variants of the objects based on the trained model and the refined model parameters. The synthetic captcha images may be created based on the backgrounds and variants of the objects. Finally, the synthetic captcha images and ground-truth images may be presented as security challenges and user access requests may be granted or denied based on responses to the security challenges.;3
266;10616257;2019;2020;Verizon Patent and Licensing Inc.;Verizon Patent and Licensing Inc.;H04L63/1425,H04L41/142,H04L41/149,H04L41/16,H04L41/5009,H04L43/16,H04W12/12,H04W24/02,H04W24/10,H04L41/5067;Method and system for anomaly detection and network deployment based on quantitative assessment;A method, a device, and a non-transitory storage medium provide a validation and anomaly detection service. The service includes quantitatively assessing latent space data representative of network performance data, which is generated by a generative model, based on quantitative values pertaining to quantitative criteria. The quantitative criteria may include Hausdorff distances, divergence, joint entropy, and total correlation. The service further includes generating geogrid data for services areas of deployed network devices and service area for prospective and new deployments based on selected latent space data and corresponding network performance data.;0
267;10621390;2015;2020;Massachusetts Institute of Technology;Massachusetts Institute of Technology;G06F40/30,G06F40/35,G06F16/345,G06F40/169,G06F40/211,G06F40/268,G06F40/56;Method and apparatus for summarization of natural language;Summarization of an input story can be accomplished through identification of causal relationships, both explicit and implicit. The input story is transformed into an interpretation sequence, using syntactic cues, and common sense knowledge of an average reader. The interpretation sequence is a time ordered semantic representation of the input story, comprised of story elements. The common sense knowledge includes inference rules, which, from story elements already present, can add additional story elements to the interpretation sequence. Application of inference rules, based on type, can be prioritized. Summarization of the interpretation sequence can be accomplished by the selection of explicit story elements, according to a connection-based strategy, or a concept-based strategy. Regarding a concept-based strategy, metrics can be applied, to select the concepts for contra-causal searching of the interpretation sequence. Options can be provided, for the exclusion of means, or the inclusion of implicit, story elements in the output summary.;2
268;10628185;2019;2020;ADOBE INC.;ADOBE INC.;G06F9/453,G06F3/0481,G06F3/04845,G06F3/04847;Content-adaptive guided tutorial generation;In implementations of content-adaptive guided tutorial generation, a computing device implements a guided tutorial generation module of an image-editing application that generates a content-adaptive guided tutorial based on input content. The content-adaptive guided tutorial provides instructions on how to interact with image-editing controls of the image-editing application, where the editing controls are selectable to initiate modifications to input parameters of the input content. The guided tutorial generation module receives the input content, and creates copy content that replicates content parameters of the input content as modifiable content parameters. The guided tutorial generation module can compute adjustment values for the modifiable content parameters that are selectable to initiate alterations to the copy content, and generates the content-adaptive guided tutorial to indicate how to interact with the editing controls to alter the content parameters based on the adjustment values to produce altered output content.;2
269;10628930;2016;2020;Koninklijke Philips N.V.;Koninklijke Philips N.V.;G06T7/337,A61B6/5235,G06F18/251,G06T5/50,G06T11/003,G06V10/803,G06T7/11,G06T2207/10088,G06T2207/10096,G06T2207/20076,G06T2207/20128,G06T2207/20221,G06T2207/30096,G06V2201/031;Systems and methods for generating fused medical images from multi-parametric, magnetic resonance image data;This invention provides a system and method for fusing and synthesizing a plurality of medical images defined by a plurality of imaging parameters allowing visual enhancements of each image data set to be combined. The system provides an image fusion process/processor that fuses a plurality of magnetic resonance imaging datasets. A first image dataset of the datasets is defined by apparent diffusion coefficient (ADC) values. A second image dataset of the MRI datasets is defined by at least one parameter other than the ADC values. The image fusion processor generates a fused response image that visually displays a combination of image features generated by the ADC values combined with image features generated by the at least one parameter other than the ADC values. The fused response image can illustratively include at least one of color-enhanced regions of interest and intensity-enhanced regions of interest.;3
270;10628931;2019;2020;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04N7/15,G06T5/50,G06T5/60,G06T5/77,G06V10/764,G06V10/82,G06V40/168,H04N23/611,H04N23/90,H04N23/951,G06T2207/10016,G06T2207/10028,G06T2207/20081,G06T2207/20084,G06T2207/30201;Enhancing digital facial image using artificial intelligence enabled digital facial image generation;A method and system for enhancing a facial image of a user in real time by digital generation of a portion of a facial image using artificial intelligence (AI) during a video conference with a plurality of participants. The method and system including receiving a digital image of a first portion of a user's face in real time from a camera viewing the first portion of the user's face. The camera is unable to view the second portion of the user's face. The method and system includes improving resolution and/or digitally completing the second portion of the user's facial image, using an AI system. The improving resolution including, receiving the digital image at the AI system which includes a Generative Adversarial Network (GAN). The method and system includes generating, in real time, a complete enhanced digital facial image of the user's face using the GAN.;3
271;10629176;2019;2020;Obeebo Labs Ltd.;Obeebo Labs Ltd.;G10H1/0025,G10H1/0058,G10H1/0041,G10H1/40,G10L19/20,G10L19/24,G10H2220/101,G10H2240/056;Systems, devices, and methods for digital representations of music;Systems, devices, and methods for encoding digital representations of musical compositions are described. Various components of a musical composition that are defined in modern music theory, such as notes and bars, are encoded as respective hierarchically-dependent data objects in a data file. The hierarchically-dependent data objects encode the musical composition in a tree-like data structure with modular nodes and adjustable relationships between nodes. Note start times and beat start times are encoded independently of one another and characterized by a timing relationship that captures the expressiveness imbued when notes and beats are not precisely synchronized. Musical variations that preserve the timing relationship between the notes and beats of the original composition are also generated and encoded.;0
272;10630726;2018;2020;Bank of America Corporation;Bank of America Corporation;G06F21/577,H04L63/20,G06F16/951,H04L63/1416,H04L63/1441,G06F2221/2101,G06F2221/2111;Cybersecurity threat detection and mitigation system;Apparatus and methods are provided for graphically defining a real-world cybersecurity protocol of an entity. The graphical platform includes searchable, manipulatable, graphs mapping cybersecurity threats. Manipulating nodes and relationships within the graphs translates into real-time modification of a cybersecurity protocol in effect for the entity. An ability to map known cybersecurity threats and analyze them (even according to known frameworks) may streamline and integrate efforts of cybersecurity defense teams. Graphical representation of a security protocol facilitates proactive threat hunting as well as expediting incident response activities by providing evidence-based pathways to inform impact analysis and source event analysis.;0
273;10635088;2019;2020;AUTODESK, INC.;AUTODESK, INC.;G05B19/4099,B29C64/393,B33Y10/00,B33Y50/00,B33Y50/02,G05B19/182,G06F30/23,G05B2219/35167,G06F2113/10,Y02P80/40,Y02P90/02;Hollow topology generation with lattices for computer aided design and manufacturing;"Methods, systems, and apparatus, including medium-encoded computer program products, for computer aided design of physical structures using generative design processes, where three dimensional (3D) models of the physical structures are produced to include lattices and hollows, include: obtaining design criteria for an object; iteratively modifying 3D topology and shape(s) for the object using a generative design process that represents the 3D topology as one or more boundaries between solid(s) and void(s), in combination with physical simulation(s) with a hollow structure and a lattice representation; adjusting a thickness of the hollow structure; adjusting lattice thickness or density; and providing a 3D model of the generative design for the object for use in manufacturing a physical structure corresponding to the object using one or more computer-controlled manufacturing systems. The providing can include generating instructions for manufacturing machine(s), which can employ various manufacturing systems and techniques, including additive, subtractive and casting manufacturing methods.";1
274;10635941;2019;2020;STRADVISION, INC.;STRADVISION, INC.;G06N3/084,G06F18/2148,G06N3/045,G06N3/047,G06N20/00,G06V10/7747,G06V10/776,G06V10/82;Method and device for on-device continual learning of neural network which analyzes input data by optimized sampling of training images, and method and device for testing the neural network for smartphones, drones, vessels, or military purpose;"A method for on-device continual learning of a neural network which analyzes input data is provided for smartphones, drones, vessels, or a military purpose. The method includes steps of: a learning device, (a) uniform-sampling new data to have a first volume, instructing a boosting network to convert a k-dimension random vector into a k-dimension modified vector, instructing an original data generator network to repeat outputting synthetic previous data of a second volume corresponding to the k-dimension modified vector and previous data having been used for learning, and generating a batch for a current-learning; and (b) instructing the neural network to generate output information corresponding to the batch. The method can be used for preventing catastrophic forgetting and an invasion of privacy, and for optimizing resources such as storage and sampling processes for training images. Further the method can be performed through a learning for Generative adversarial networks (GANs).";1
275;10643131;2019;2020;DeepMind Technologies Limited;DeepMind Technologies Limited;G06N3/084,G06F18/2155,G06N3/045,G06N7/01,G06V10/7753,G06F17/18;Training variational autoencoders to generate disentangled latent factors;Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training a variational auto-encoder (VAE) to generate disentangled latent factors on unlabeled training images. In one aspect, a method includes receiving the plurality of unlabeled training images, and, for each unlabeled training image, processing the unlabeled training image using the VAE to determine the latent representation of the unlabeled training image and to generate a reconstruction of the unlabeled training image in accordance with current values of the parameters of the VAE, and adjusting current values of the parameters of the VAE by optimizing a loss function that depends on a quality of the reconstruction and also on a degree of independence between the latent factors in the latent representation of the unlabeled training image.;3
276;10643600;2018;2020;OBEN, INC.;OBEN, INC.;G10L13/02,G10L13/033,G10L13/086,G10L13/10,G10L21/04,G10L2013/105;Modifying syllable durations for personalizing Chinese Mandarin TTS using small corpus;A method and system for personalizing synthetic speech from a text-to-speech (TTS) system is disclosed. The method uses linguistic feature vectors to correct/modify the synthetic speech, particularly Chinese Mandarin speech. The linguistic feature vectors are used to generate or retrieve onset and rime scaling factors encoding differences between the synthetic speech and a user's natural speech. Together, the onset and rime scaling factors are used to modify every word/syllable of the synthetic speech from a TTS system, for example. In particular, segments of synthetic speech are either compressed or stretched in time for each part of each syllable of the synthetic speech. After modification, the synthetic speech more closely resembles the speech patterns of a speaker for which the scaling factors were generated. The modified synthetic speech may then be transmitted to a user and played to the user via a mobile phone, for example. The linguistic feature vectors are constructed based on a plurality of feature attributes including at least a group ID attribute, voicing attribute, complexity attribute, nasality attribute, and tone for the current syllable. The invention is particularly useful when the user speech corpus is either small or otherwise incomplete.;4
277;10645225;2019;2020;;;H04M3/5183,G06F3/04817,G06Q10/10,G06Q30/02,G06Q30/0203,G06Q30/0281,G06T3/18,G06T11/203,G06T11/206,G06T11/40,G06T13/80,H04L51/02,H04L51/10,H04M7/0045,G06T2210/44,G06T2213/12,H04M2203/256,H04M2203/401;Systems and methods relating to chat interfaces;"A method for orchestrating a chat interaction with a customer and multiple sources that includes providing an animation library that includes conversational states, graphic stills corresponding to respective ones of the conversational states, animation templates, and source-based modifications corresponding to respective ones of the multiple sources. The method may include generating a source-informative animation by: determining a source of a chat message; determining a conversational state for the chat message; selecting a graphic still for the message; selecting an animation template for the chat message based on the selected graphic still; selecting a source-based modification that corresponds to the source; and modifying the animation template pursuant to the source-based modification so to generate the source-informative animation. The method may include sending a signal to the customer configured to generate thereon a chat interface having the chat message and source-informative animation.";4
278;10650306;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N3/045,G06F3/167,G06N3/047,G06N3/08,G06N3/084,G10L15/22,G06N7/01,G10L2015/227;User representation using a generative adversarial network;An interactive system makes use of a concise user representation, for example, in the process of making predictions of a user's next action. In some examples, the concise user representation is computed from a larger amount of user data, which is processed using a transformation trained using a Generative Adversarial Network (GAN) approach.;1
279;10652565;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N3/088,G06F18/213,G06N3/045,G06N20/00,G06T9/002,G06V10/764,G06V10/82,H04N19/115,H04N19/463,G06N7/01;Image compression and decompression using embeddings;A processing device receives a representation of an image, wherein the image has a first size and the representation has a second size that is smaller than the first size, the representation having been generated from the image by a first portion of a first trained machine learning model. The processing device processes the representation of the image using a second portion of the trained machine learning model to generate a reconstruction of the image and then outputs the reconstruction of the image.;3
280;10657201;2012;2020;Narrative Science LLC;Narrative Science LLC;G06F40/20,G06F40/35,G06F40/00,G06F40/205;Configurable and portable system for generating narratives;The subject invention functions to use available data and information to automatically create narrative stories that describes domain events, circumstances and/or entities in a comprehensible and compelling and audience customized, manner. Computer executable instructions provide for generating a narrative story using standard and uniform structures and data for receiving domain related data and a story specification, parsing the story specification to provide constituent components, transforming the constituent components into executable code, instantiating content blocks having at least one feature for the domain according to the story specification and rendering the narrative story using the constituent components specified by the content blocks.;3
281;10657584;2020;2020;STRADVISION, INC.;STRADVISION, INC.;G06Q30/0643,G06F18/214,G06N3/045,G06N3/084,G06T7/11,G06V10/764,G06V10/82,G06V40/103,G06T2207/20081,G06T2207/30124,G06V20/593;Method and device for generating safe clothing patterns for rider of bike;"A method for generating safe clothing patterns for a human-like figure is provided. The method includes steps of: a safe clothing-pattern generating device, (a) after acquiring an image of the human-like figure, generating a specific clothing pattern having an initial value, inputting the specific clothing pattern and the image of the human-like figure into a clothing composition network, combining the specific clothing pattern with a clothing of the human-like figure to generate a composite image; (b) inputting the composite image into an image translation network, translating surrounding environment on the composite image to generate a translated image, and inputting the translated image into an object detector to output detection information on the human-like figure; and (c) instructing a 1-st loss layer to calculate losses by referring to the detection information and a GT corresponding to the image of the human-like figure, and updating the initial value by using the losses.";3
282;10657934;2019;2020;Electronic Arts Inc.;Electronic Arts Inc.;G06N3/084,G10H1/0025,G06N3/04,G06N3/044,G06N3/045,G06N5/01,G06N5/025,G10H2210/026,G10H2210/036,G10H2210/105,G10H2210/111,G10H2220/121,G10H2240/085,G10H2240/141,G10H2250/311;Enhancements for musical composition applications;Systems and methods are provided for enhancements for musical composition applications. An example method includes receiving information identifying initiation of a music composition application, the music composition application being executed via a user device of a user, with the received information indicating a genre associated with a musical score being created via the music composition application. One or more constraints associated with the genre are determined, with the constraints indicating one or more features learned based on analyzing music associated with the genre. Musical elements specified by the user are received via the music composition application. Musical score updates are determined based on the musical elements and genre. The determined musical score updates are provided to the user device.;0
283;10658005;2019;2020;Neon Evolution Inc.;Neon Evolution Inc.;G06T9/002,G06F18/214,G06F18/40,G06V10/82,G06V20/46,G06V40/166,G06V40/175,G06V40/176,G10L21/003,G10L21/055,G10L25/24,G10L25/57,G11B27/031,G11B27/036,G11B27/10,G11B27/28,G10L2021/0135;Methods and systems for image and voice processing;Systems and methods are disclosed configured to train an autoencoder using images that include faces, wherein the autoencoder comprises an input layer, an encoder configured to output a latent image from a corresponding input image, and a decoder configured to attempt to reconstruct the input image from the latent image. An image sequence of a face exhibiting a plurality of facial expressions and transitions between facial expressions is generated and accessed. Images of the plurality of facial expressions and transitions between facial expressions are captured from a plurality of different angles and using different lighting. An autoencoder is trained using source images that include the face with different facial expressions captured at different angles with different lighting, and using destination images that include a destination face. The trained autoencoder is used to generate an output where the likeness of the face in the destination images is swapped with the likeness of the source face, while preserving expressions of the destination face.;3
284;10658074;2011;2020;Zeus Data Solutions, Inc.;Zeus Data Solutions, Inc.;G16H10/60,G16H15/00,G06F16/313,G06Q30/02;Medical transcription with dynamic language models;A computer-implemented method for transcribing spoken input into text is disclosed. The method includes identifying a role of a healthcare provider using an automated transcription system, using the identified role to provide a language model for a speech recognition system operating on computer system, receiving spoken input from the healthcare provider, and producing textual output corresponding to the spoken input using the provided language model.;1
286;10664718;2018;2020;Apple Inc.;Apple Inc.;G06N3/08,G06F18/214,G06F18/22,G06N3/045,G06N5/046,G06T3/40,G06T3/4046,G06T5/50,G06T7/251,G06T11/001,G06V10/454,G06V10/764,G06V10/82,G06T2207/20028,G06T2207/20081,G06T2207/20221;Real-time adjustment of hybrid DNN style transfer networks;Artistic styles extracted from one or more source images may be applied to one or more target images, e.g., in the form of stylized images and/or stylized video sequences. The extracted artistic style may be stored as a plurality of layers in a neural network, which neural network may be further optimized, e.g., via the fusion of various elements of the network's architectures. An optimized network architecture may be determined for each processing environment in which the network will be applied. The artistic style may be applied to the obtained images and/or video sequence of images using various optimization methods, such as the use of scalars to control the resolution of the unstylized and stylized images, temporal consistency constraints, as well as the use of dynamically adjustable or selectable versions of Deep Neural Networks (DNN) that are responsive to system performance parameters, such as available processing resources and thermal capacity.;6
287;10671838;2019;2020;Neon Evolution Inc.;Neon Evolution Inc.;G06T11/60,G06F18/2413,G06T11/001,G06T13/40,G06T13/80,G06T15/205,G06V10/26,G06V10/40,G06V10/764,G06V10/774,G06V10/82,G06V40/161,G06V40/171,G06V40/172,G06T2207/30201,G10L2021/0135,G10L2021/105;Methods and systems for image and voice processing;Systems and methods are disclosed configured to train an autoencoder using images that include faces, wherein the autoencoder comprises an input layer, an encoder configured to output a latent image from a corresponding input image, and a decoder configured to attempt to reconstruct the input image from the latent image. An image sequence of a face exhibiting a plurality of facial expressions and transitions between facial expressions is generated and accessed. Images of the plurality of facial expressions and transitions between facial expressions are captured from a plurality of different angles and using different lighting. An autoencoder is trained using source images that include the face with different facial expressions captured at different angles with different lighting, and using destination images that include a destination face. The trained autoencoder is used to generate an output where the likeness of the face in the destination images is swapped with the likeness of the source face, while preserving expressions of the destination face.;3
288;10678402;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/0482,G06F3/0237,G06F3/0488,G06F3/04886,G06F3/167,G06F16/3344,G06F16/338;Interactive bot for natural language analytics;Embodiments of the present disclosure are directed to a system for providing an interactive bot. For example, the system may receive a first user interaction with an interactive bot at a user interface. The user interface may comprise a suggestion field and a text input field. Interface objects may be provided in the suggestion field as terms to provide to the interactive bot. The user may select the interface objects to add to the text input field, or the user may remove these interface objects without affecting other interface objects in the text input field. In some examples, the context terms in the interface objects may be determined from a context suggestion tree with a hierarchical structure.;2
289;10678406;2019;2020;Botsociety, Inc.;Botsociety, Inc.;G06F3/0484,G06F9/453,H04L51/02;Conversational user interface design;A system and method for designing a conversational interface is disclosed. The method includes displaying a first portion of a conversational interface editor, receiving user interaction in the first portion defining a first message object attributed to a first conversational entity, displaying a second portion of the conversational interface editor, receiving user interaction in the second portion defining a second message object attributed to a second conversational entity, and displaying a third portion of the conversational interface editor, the third portion arranging the first message object and the second message object to depict a structure of a conversational interaction between the first conversational entity and the second conversational entity in a conversational interface.;0
290;10679015;2015;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/58,G06F40/279,G06F40/30,G06F40/56,G06N7/00,G06N20/00,G06N7/01;Utilizing artificial intelligence-based machine translation to augment document summarization;Technologies are disclosed for utilizing artificial intelligence-based machine translation to augment document summarization. Text can be extracted from a document in a first language. Machine translation can be utilized to translate the text from the first language to a second language. The translated text can be used to identify documents in the second language that include support for the translated text. A user interface can be provided that indicates the number of documents in the second language that provide support for the extracted text. Documents in the first language can also be translated to the second language. Documents that provide support for a text string can be identified in the documents translated to the second language and in other documents in the second language. A user interface can be provided that indicates the number of documents in the first language and the second language that provide support for the text.;0
291;10682575;2019;2020;Mythical, Inc.;Mythical, Inc.;A63F13/35,A63F13/655,A63F13/213,A63F13/533,A63F13/537,A63F13/58,A63F13/825,G06T11/00,A63F2300/8058,G06F3/0482;Systems and methods for generating in-game assets for a gaming platform based on inheriting characteristics from other in-game assets;"Systems and methods for generating characters for a gaming platform based on inheriting characteristics from other characters are disclosed. Exemplary implementations may: store, in electronic storage, a set of character definitions that define visual representations of individual characters that can operate and interact within the gaming platform; present a user interface to a first player on a first client computing platform associated with the first player; receive user input from the first player; generate a new character definition based on the first character definition and the second character definition; and present the new character on the user interface.";4
292;10685057;2016;2020;Shutterstock, Inc.;Shutterstock, Inc.;G06F16/5866,G06F16/248,G06F16/9535,G06N3/045,G06N3/08,G06N3/084,G06N3/088,G06N7/01;Style modification of images in search results;Methods for style modification of images in search results are provided. In one aspect, a method includes receiving user input identifying a search query from a client device, in which the search query indicates one or more predetermined search terms. The subject system determines a first collection of images that correspond to the one or more predetermined search terms and a second collection of images that exclude images that correspond to the one or more predetermined search terms. The subject system modifies images of the second collection to apply a keyword style that corresponds to the one or more predetermined search terms, and provides a listing of images to the client device. The listing of images includes both the first collection of images and the images of the second collection that are modified with the applied keyword style. Systems and machine-readable media are also provided.;0
293;10692002;2019;2020;STRADVISION, INC.;STRADVISION, INC.;G06V10/454,G06F18/214,G06N3/08,G06N7/00,G06T7/11,G06V10/764,G06V10/82,G06V20/58,G06V40/10,G06T2207/20081,G06T2207/30196;Learning method and learning device of pedestrian detector for robust surveillance based on image analysis by using GAN and testing method and testing device using the same;"A method for learning a pedestrian detector to be used for robust surveillance or military purposes based on image analysis is provided for a solution to a lack of labeled images and for a reduction of annotation costs. The method can be also performed by using generative adversarial networks (GANs). The method includes steps of: a learning device generating an image patch by cropping each of regions on a training image, and instructing an adversarial style transformer to generate a transformed image patch by converting each of pedestrians into transformed pedestrians capable of impeding a detection; and generating a transformed training image by replacing each of the regions with the transformed image patch, instructing the pedestrian detector to detecting the transformed pedestrians, and learning parameters of the pedestrian detector to minimize losses. This learning, as a self-evolving system, is robust to adversarial patterns by generating training data including hard examples.";3
294;10692485;2016;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/01,G10L15/1822,G06F3/017,G06F3/0346,G06F3/167,G10L13/00,G10L15/18,G10L15/187,G10L15/22,G10L15/24,G10L2015/088,G10L2015/227;Non-speech input to speech processing system;A system and method for associating motion data with utterance audio data for use with a speech processing system. A device, such as a wearable device, may be capable of capturing utterance audio data and sending it to a remote server for speech processing, for example for execution of a command represented in the utterance. The device may also capture motion data using motion sensors of the device. The motion data may correspond to gestures, such as head gestures, that may be interpreted by the speech processing system to determine and execute commands. The device may associate the motion data with the audio data so the remote server knows what motion data corresponds to what portion of audio data for purposes of interpreting and executing commands. Metadata sent with the audio data and/or motion data may include association data such as timestamps, session identifiers, message identifiers, etc.;1
295;10692489;2016;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/20,G06F3/167,G10L15/142,G10L15/18,G10L15/22,G10L15/25,G10L15/26,G10L15/28,G10L15/1822,G10L2015/227;Non-speech input to speech processing system;A system and method for incorporating motion into a speech processing system. A wearable device that is capable of both capturing spoken utterances and capturing motion data may be used to interact with a speech processing system. In certain circumstances, such as when voice communication are unreliable (due to noise) or when controlling the system by motion is desired, motion of a device may be used to provide input to a speech processing system. For example, sensor data or gesture data resulting from movement of a device may be processed and input into a natural language system as representative of a spoken command portion or other input. The motion information may be interpreted to provide prompts to the system (e.g., âyes,â âno,â etc.), to perform certain commands (skip, forward, back, cancel) or to otherwise control the system.;1
296;10692602;2017;2020;DEEPRADIOLOGY, INC.;DEEPRADIOLOGY, INC.;G06T7/0012,G06T7/11,G06V10/454,G06V10/764,G06V10/82,G16H15/00,G16H50/20,G06T2207/10081,G06T2207/10088,G06T2207/10092,G06T2207/10116,G06T2207/20081,G06T2207/20084,G06T2207/30008,G06T2207/30016,G06T2207/30096,G06V2201/03;Structuring free text medical reports with forced taxonomies;Methods and systems for medical diagnosis by machine learning are disclosed. Imaging data obtained from different medical techniques can be used as a training set for a machine learning method, to allow diagnosis of medical conditions in a faster a more efficient manner. A three-dimensional convolutional neural network can be employed to interpret volumetric data available from multiple scans of a patient. The imaging data can be analyzed according to a forced taxonomy and any discrepancy in the labels of the taxonomy during data analysis by machine learning and human experts can be resolved based on the forced taxonomy.;0
297;10699079;2018;2020;Narrative Science LLC;Narrative Science LLC;G06F40/30,G06F40/56,G06F40/237,G06F40/295,G06F40/35,G06N3/006,G06N5/02,G06N5/022,G06N5/041;Applied artificial intelligence technology for narrative generation based on analysis communication goals;Artificial intelligence (AI) technology can be used in combination with composable communication goal statements to facilitate a user's ability to quickly structure story outlines using âanalyzeâ communication goals in a manner usable by an NLG narrative generation system without any need for the user to directly author computer code. This AI technology permits NLG systems to determine the appropriate content for inclusion in a narrative story about a data set in a manner that will satisfy a desired analysis communication goal such that the narratives will express various ideas that are deemed relevant to a given analysis communication goal.;3
298;10699695;2018;2020;Amazon Washington, Inc.;Amazon Washington, Inc.;G10L13/10,G10L13/047,G10L13/06;Text-to-speech (TTS) processing;During text-to-speech processing, audio data corresponding to a word part, word, or group of words is generated using a trained model and used by a unit selection engine to create output audio. The audio data is generated at least when an input word is unrecognized or when a cost of a unit selection is too high.;1
299;10701394;2017;2020;Twitter, Inc.;Twitter, Inc.;H04N19/577,G06N3/044,G06N3/045,G06N3/08,H04N19/33,H04N19/43,H04N19/523,H04N19/59;Real-time video super-resolution with spatio-temporal networks and motion compensation;A method includes selecting a plurality of low-resolution frames associated with a video, performing a first motion estimation between a first frame and a second frame, performing a second motion estimation between a third frame and the second frame, generating a high-resolution frame representing the second frame based on the first motion estimation, the second motion estimation and the second frame using a sub-pixel convolutional neural network.;0
300;10706837;2018;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/02,G10L13/033,G10L13/00,G10L13/10;Text-to-speech (TTS) processing;A speech model includes a sub-model corresponding to a vocal attribute. The speech model generates an output waveform using a sample model, which receives text data, and a conditioning model, which receives text metadata and produces a prosody output for use by the sample model. If, during training or runtime, a different vocal attribute is desired or needed, the sub-model is re-trained or switched to a different sub-model corresponding to the different vocal attribute.;1
301;10706867;2018;2020;OBEN, INC.;OBEN, INC.;G10L21/003,G10L21/013,G10L25/21,G10L25/24,G10L25/75,G10L21/01,G10L2021/0135;Global frequency-warping transformation estimation for voice timbre approximation;"A method and system for converting a source voice to a target voice is disclosed. The method comprises: recording source voice data and target voice data; extracting spectral envelope features from the source voice data and target voice data; time-aligning pairs of frames based on the extracted spectral envelope features; converting each pair of frames into a frequency domain; generating a plurality of frequency-warping factor candidates, wherein each of the plurality of frequency-warping factor candidates is associated with one of the pairs of frames; generating a single global frequency-warping factor based on the candidates; acquiring source speech; converting the source speech to target speech based on the global frequency-warping factor; generating a waveform comprising the target speech; and playing the waveform comprising the target speech to a user.";2
303;10713289;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/3344,G06F16/3329,G06F40/30,G06F40/295,G10L15/18,G10L15/22,G10L15/30,G10L2015/223;Question answering system;Systems, methods, and devices for performing interactive question answering using data source credibility and conversation entropy are disclosed. A speech-controlled device captures audio including a spoken question, and sends audio data corresponding thereto to a server(s). The server(s) performs speech processing on the audio data, and determines various stored data that can be used to determine an answer to the question. The server(s) determines which stored data to use based on the credibility of the source from which the stored data was received. The server(s) may also determine a number of user interactions needed to obtain data in order to fully answer the question and may select a question for a dialog soliciting further data based on the number of user interactions.;2
304;10713662;2019;2020;Avaya Inc.;Avaya Inc.;G06Q30/016,G06Q10/06316,H04M3/5175,H04M3/5183,H04M2203/404,H04M2203/551,H04M2203/554,H04M2250/64;Artificial intelligence based identification and data gathering of incomplete interactions and automatically creating tasks to take it to completion;"One aspect of the present invention relates to a computer-implemented process, that includes receiving an incoming contact at a contact center for an interaction between the contact center and a user; monitoring inbound communication and outbound communication between the user and an agent of the contact center; detecting that one of the agent or the user ends the contact; and based on the monitored inbound communication and outbound communication, automatically determining, by the computer, whether the interaction is complete.";0
305;10713821;2019;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06T11/001,G06F40/30,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06T11/00,G06T11/20;Context aware text-to-image synthesis;Techniques are generally described for context aware text-to-image synthesis. First text data comprising a description of an object may be received. A recurrent neural network may determine a first semantic representation data representing the first text data. A generator trained using a first generative adversarial network (GAN) may determine first image data representing the object using the first semantic representation. An encoder of a second GAN may generate a first feature representation of the first image data. The first feature representation may be combined with a projection of the first semantic representation data. A decoder of the second GAN may generate second image data representing the first text data.;4
306;10719596;2019;2020;Capital One Services, LLC;Capital One Services, LLC;G06F21/32,G06N3/047,G06N3/08,G06N3/088,G06V30/373,G06V40/394,G06N3/045,G06V2201/10;System, method, and computer-accessible medium for authentication via handwriting style;"Methods, systems, and computer-accessible mediums are described to authenticate a user using a user's handwriting style rather than using the user's signature through adaptive handwriting challenges which are verified using the machine learning technique of a generative adversarial network. An exemplary system, method, and computer-accessible medium can include, for example, receiving at a first entity, a request for authentication of a user associated with a user identifier; sending from the first entity to the user, a first adaptive phrase; receiving at the first entity, a digital representation of a human transcription of the first adaptive phrase; performing an artificial-intelligence based comparison between the received digital representation of the human transcription of the first adaptive phrase and a user-specific computer-generated discriminator or user-specific computer-generated model of the first adaptive phrase; generating a first score based on the comparison; sending from the first entity (i) an authentication message if the first score is equal to or above a pre-determined threshold or (ii) a failure message if the first score is below a pre-determined threshold.";0
307;10719779;2016;2020;INTELLIGENT AUTOMATION, LLC;INTELLIGENT AUTOMATION, LLC;G06N20/00,G06Q50/01,G06F16/9024,G06N7/01,G06N5/04;System and means for generating synthetic social media data;System and means generates synthetic forms of social media data such as data from microblogging services (e.g., Twitter) and social networking services (e.g., Facebook). This system and means jointly generate interaction graph structures and text features similar to input social media data. First, an interaction graph is generated by mapping social network interactions in input (real) social media data to graph structures. This interaction graph is fitted to a social network model (or a composite model) by minimizing the distance between the input and the synthetic interaction graphs (of potentially different sizes). The distance is measured statistically or based on the performance of social media analytics. Various patterns (such as anomalies), interaction types and temporal dynamics are generated synthetically. Second, text features are extracted from input social media data with topic modeling and statistical analysis of word tuple distributions. Based on these features, synthetic social media text is generated. Third, synthetic graph structures and text features are combined to generate the synthetic social media data. The system is particularly useful in generating data to be used for developing and testing new social media analytics or for generating or analyzing social bot network behavior and campaigns in social media, and for sharing test data with others without rate and privacy concerns.;2
308;10720157;2018;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06Q30/0621,G06Q30/0635,G10L13/00,G10L15/1815,G10L15/30,G10L15/1807,G10L15/1822,G10L2015/223;Voice to voice natural language understanding processing;Techniques for providing a standardized voice user interface (VUI) that enables voice to voice natural language understanding (NLU) processing are described. The standardized VUI may be added to speechlets to enable a customer to interact with a business via server(s) using NLU processing. For example, a first user may initiate a first voice interaction using NLU processing with the server(s) and the system may initiate a second voice interaction using NLU processing between the server(s) and a second user. This enables a customer to initiate a transaction (e.g., request information, place an order, make a reservation, etc.) with the business using the speechlet. Thus, the business may use the speechlet to offer services without requiring additional infrastructure or complicated programming to implement.;0
309;10721189;2018;2020;American Innovative Applications Corporation;American Innovative Applications Corporation;H04L51/02,G06Q30/0277,H04L51/10,H04L51/52,H04L67/34,G06Q30/0617,G06Q50/01,H04L51/046,H04L67/01;Advertising systems and methods employing JavaScript object notation;A JSON advertising system includes an ad management system, a bot builder module, a social network platform, a bot platform, and a client, all mutually connected via a suitable network. An advertiser stores an ad creative into an ad server located in the ad management system, and generates JSON code via a JSON code tool within the bot builder module for each part of advertisement content stored in the ad server. Then, a client logs into a social network platform news feed, retrieving and displaying the advertisement. A user engages the ad and is redirected to a bot platform chat interface, retrieving bot advertisement content from a bot server and linking the content to the advertisement stored in the ad server as determined by the JSON code.;0
310;10733240;2017;2020;INTUIT INC.;INTUIT INC.;G06F16/93,G06F3/04847,G06F40/295,G06Q10/067,G06Q10/10,G06Q50/188;Predicting contract details using an unstructured data source;A method may include extracting first entities from a first portion of an unstructured data source associated with a user, obtaining, based on the first entities, a contract model including elements and a contract type, generating, by applying the contract model to the first entities, a proposed contract including a contract score and, for each element, element values. Each element value may include an element value score. The method may further include identifying a structured data source associated with the user, obtaining, from the structured data source, structured data corresponding to the first entities, correlating the structured data with an element value of the proposed contract, and modifying, by applying the contract model to the structured data, the element value score of the element value.;0
311;10733374;2019;2020;;;G06F40/149,G06F40/197,G06F16/58,G06F40/166,G06F40/279;Live documentation (LiDo);A method for implementing Live Documentation (LiDo) systems to incorporate and authenticate metadata of electronic documents. An electronic document received at a first LiDo system is partitioned into textual units. Each textual unit has a payload with a printable message and metadata that is not visible to the consumer, wherein the metadata includes one or more attributes. A second LiDo system uses the one or more attributes of a textual unit to authenticate the writer of the textual unit and filter textual units to display only printable messages with a validity above a designated threshold.;1
312;10733733;2019;2020;LUNIT INC.;LUNIT INC.;G06N3/088,G06F18/2413,G06N3/045,G06N3/047,G06N20/00,G06T7/0012,G06T7/0014,G06V10/764,G06V10/82,G06T2207/20081,G06T2207/20084,G06T2207/30096;Method for detecting anomaly using generative adversarial networks, apparatus and system thereof;There is provided an anomaly detection method, apparatus, and system that can improve the accuracy and reliability of a detection result using GAN (Generative Adversarial Networks). An anomaly detection apparatus according to some embodiments includes a memory that stores a GAN-based image translation model and an anomaly detection model, and a processor that translates a learning image with a low-difficulty level into a learning image with a high-difficulty level and learns the anomaly detection model using the translated learning image. The anomaly detection apparatus can improve the detection performance by learning the anomaly detection model with the learning image with the high-difficulty level in which it is difficult detect the anomaly.;0
313;10769501;2018;2020;GOOGLE LLC;GOOGLE LLC;G01N33/5005,G06F18/211,G06F18/2148,G06F18/22,G06F18/24,G06F18/25,G06N3/08,G06V10/82,G06V20/698,G06V20/70,G06V30/19173,G06V30/274,G01N33/20,G01N33/48,G06V2201/10;Analysis of perturbed subjects using semantic embeddings;The present disclosure relates to analysis of perturbed subjects using semantic embeddings. One example embodiment includes a method. The method includes applying a respective perturbation to each of a plurality of subjects in a controlled environment. The method also includes producing a respective visual representation for each of the perturbed subjects using at least one imaging modality. Further, the method includes obtaining, by a computing device for each of the respective visual representations, a corresponding semantic embedding associated with the respective visual representation. The semantic embedding associated with the respective visual representation is generated using a machine-learned, deep metric network model. In addition, the method includes classifying, by the computing device based on the corresponding semantic embedding, each of the visual representations into one or more groups.;0
314;10769848;2019;2020;ADOBE INC.;ADOBE INC.;G06T17/20,B29C64/386,B33Y50/00,G06N3/08,G06T7/50,G06T7/55,G06T15/005,G06N3/045,G06N3/084,G06T2200/04,G06T2200/08,G06T2207/20084;3D object reconstruction using photometric mesh representation;Techniques are disclosed for 3D object reconstruction using photometric mesh representations. A decoder is pretrained to transform points sampled from 2D patches of representative objects into 3D polygonal meshes. An image frame of the object is fed into an encoder to get an initial latent code vector. For each frame and camera pair from the sequence, a polygonal mesh is rendered at the given viewpoints. The mesh is optimized by creating a virtual viewpoint, rasterized to obtain a depth map. The 3D mesh projections are aligned by projecting the coordinates corresponding to the polygonal face vertices of the rasterized mesh to both selected viewpoints. The photometric error is determined from RGB pixel intensities sampled from both frames. Gradients from the photometric error are backpropagated into the vertices of the assigned polygonal indices by relating the barycentric coordinates of each image to update the latent code vector.;2
315;10783398;2018;2020;Shutterstock, Inc.;Shutterstock, Inc.;G06F16/9535,G06F16/51,G06F16/54,G06F16/56,G06F16/583,G06F16/5838,G06F18/214,G06F18/22,G06F18/2321,G06F18/2413,G06F18/2415,G06T11/60,G06V10/75,G06V10/763,G06V10/764,G06V10/774,G06V10/82,G06T2200/24;Image editor including localized editing based on generative adversarial networks;A method for receiving an image query from a user via a client device is provided. The method includes determining a user personalized data based on a prior user history, generating a synthetic image with a generative tool, based on the image query and the user personalized data, and evaluating a similarity between the synthetic image and a real image in an image database with a discriminative tool. The method also includes providing the synthetic image to the user for selection and storing a user response to the synthetic image in the prior user history. A system and a non-transitory, computer readable medium storing instructions to cause the system to perform the above method are also disclosed.;2
316;10789606;2011;2020;GOOGLE LLC;GOOGLE LLC;G06Q30/0207,G06Q30/0276;Generation of an advertisement;Aspects of the disclosure assist users with creative development of advertisements by converting successful advertisements into templates for use in the creation of new advertisements. The system and method suggest an advertisement to a user based on the type of content for which the user is advertising. For example, an advertisement may include a set of display criteria, and a set of creative content. The display criteria define a set of circumstances during which the advertisement may be displayed (e.g., a set of keywords that trigger the display of the advertisement). Advertisements that are particularly successful for a given set of display criteria are depersonalized and converted to templates. These templates may be provided during creation of new advertisements as suggested content.;0
317;10789723;2018;2020;FACEBOOK, INC.;FACEBOOK, INC.;G06N3/084,G06T7/536,G06N3/045,G06N3/047,G06N3/088,G06N20/00,G06T5/77,G06T7/55,G06T17/20,G06N3/082,G06T2207/10028,G06T2207/20081,G06T2207/20084;Image object extraction and in-painting hidden surfaces for modified viewpoint rendering;In one embodiment, a method includes generating depth map for a reference image and generating a three-dimensional (3D) model for a plurality of objects in the reference image based on the depth map. The method additionally includes determining, out of the objects in the 3D model, a background object having a boundary adjacent to a foreground object. The method also includes determining that at least a portion of a surface of the background object is hidden by the foreground object and extending, in the 3D model, the surface of the background object to include the portion hidden by the foreground object. The method further includes in-paint pixels of the extended surface of the background object with pixels that approximate the portion of the surface of the background object hidden by the foreground object.;3
318;10789956;2019;2020;Capital One Services, LLC;Capital One Services, LLC;G10L15/26,G06N20/00,G10L13/00,G10L13/02,G10L15/063,G06N3/02;Text-to-speech modeling;A device may receive a set of audio data files corresponding to a set of calls, wherein the set of audio data files includes digital representations of one or more segments of respective calls of the set of calls, and wherein the set of calls includes audio data relating to a particular industry. The device may receive a set of transcripts corresponding to the set of audio data files. The device may determine a plurality of text-audio pairs within the set of calls, wherein a text-audio pair, of the plurality of text-audio pairs, comprises: a digital representation of a segment a call of the set of calls, and a corresponding excerpt of text from the set of transcripts. The device may train, using a machine learning process, an industry-specific text-to-speech model, tailored for the particular industry, based on the plurality of text-audio pairs.;1
320;10796381;2014;2020;INTUIT INC.;INTUIT INC.;G06Q40/123;Systems and methods for determining impact correlations from a tax calculation graph of a tax preparation system;Systems, methods and articles of manufacture for determining impact correlations from a calculation graph for calculating taxes. The system includes a computing device, a data store in communication with the computing device and a tax preparation software application executable by the computing device. The tax preparation software application has a tax calculation engine, a tax calculation graph, and an impact chain engine. The tax calculation engine is configured to perform a plurality of tax calculation operations based on the tax calculation graph. The impact chain engine is configured to analyze the tax calculation graph and determine an impact correlation between a change in a value of a first node caused by a change in a value of a second node utilizing an impact chain which includes both the first node and the second node.;0
321;10802889;2018;2020;NTT DATA Services, LLC;NTT DATA Services, LLC;G06F9/5072,G06F9/5077,G06F9/5005,G06F9/547,G06F11/3055,G06F11/3423,G06F11/3476,G06F2201/805,G06F2209/508;Systems and methods of virtual resource monitoring for robotic processes;In one embodiment, a method includes monitoring, in real-time, a plurality of resources including a first robotic process resident on a first RPA platform and a second robotic process resident on a second RPA platform. The first RPA platform and the second RPA platform provide robotic process data in heterogeneous data formats via heterogeneous interfaces. The method also includes, responsive to a trigger, invoking at least one function on a unified interface. The method also includes receiving at least one function call reply from the unified interface responsive to the invoking, the at least one function call reply including homogeneous data related to the first robotic process and the second robotic process. In addition, the method includes determining real-time statuses of the first robotic process and the second robotic process using the homogeneous data. The method also includes updating a real-time dashboard with the real-time statuses.;0
322;10803646;2019;2020;Neon Evolution Inc.;Neon Evolution Inc.;G06N3/088,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G06T5/60,G06T5/77,G06T11/60,G06V10/764,G06V10/82,G06V40/161,G06V40/168,G06V40/172,G06V40/174,G06T2207/20084,G06T2207/30201;Methods and systems for image and voice processing;Systems and methods are disclosed configured to train an autoencoder using images that include faces, wherein the autoencoder comprises an input layer, an encoder configured to output a latent image from a corresponding input image, and a decoder configured to attempt to reconstruct the input image from the latent image. An image sequence of a face exhibiting a plurality of facial expressions and transitions between facial expressions is generated and accessed. Images of the plurality of facial expressions and transitions between facial expressions are captured from a plurality of different angles and using different lighting. An autoencoder is trained using source images that include the face with different facial expressions captured at different angles with different lighting, and using destination images that include a destination face. The trained autoencoder is used to generate an output where the likeness of the face in the destination images is swapped with the likeness of the source face, while preserving expressions of the destination face.;3
323;10810725;2018;2020;FACEBOOK, INC.;FACEBOOK, INC.;G06T1/0028,G06F18/2148,G06F21/64,G06N3/045,G06N3/084,G06T7/0002,G06T9/002,G06V10/764,G06V10/7747,G06V10/82,G06N5/01,G06N20/10,G06T2201/0053,G06T2201/0201,G06T2207/20084,G06T2207/30196,G06V2201/10;Automated detection of tampered images;A content analyzer determines whether various types of modification have been made to images. The content analyzer computes JPEG ghosts from the images that are concatenated with the image channels to generate a feature vector. The feature vector is provided as input to a neural network that determines whether the types of modification have been made to the image. The neural network may include a constrained convolution layer and several unconstrained convolution layers. An image fake model may also be applied to determine whether the image was generated using a computer model or algorithm.;0
324;10817314;2019;2020;NTT DATA Services, LLC;NTT DATA Services, LLC;G06F8/38,G06F9/451,G06F40/14;Augmented shareable video files for robotic process automation;"In one general aspect, in an embodiment, a method of performance-enhanced machine-learning model creation is performed by a computer system. The method includes receiving a command to record user interface (UI) activity in a computing environment. The method further includes, responsive to the command: receiving video frames of a live screen output of the computing environment; detecting UI events in the computing environment in relation to the video frames of the live screen output; and determining target applications for the UI events, wherein the target applications are executing in the computing environment. The method also includes generating UI metadata comprising information identifying the UI events and the target applications in relation to the video frames. In addition, the method includes sequentially encoding, in a video file, the video frames together with information sufficient to derive the UI metadata.";0
325;10818043;2019;2020;ADOBE INC.;ADOBE INC.;G06T11/001,G06T7/40,G06T2207/20081,G06T2207/20084;Texture interpolation using neural networks;An example method for neural network based interpolation of image textures includes training a global encoder network to generate global latent vectors based on training texture images, and training a local encoder network to generate local latent tensors based on the training texture images. The example method further includes interpolating between the global latent vectors associated with each set of training images, and interpolating between the local latent tensors associated with each set of training images. The example method further includes training a decoder network to generate reconstructions of the training texture images and to generate an interpolated texture based on the interpolated global latent vectors and the interpolated local latent tensors. The training of the encoder and decoder networks is based on a minimization of a loss function of the reconstructions and a minimization of a loss function of the interpolated texture.;3
326;10818293;2020;2020;Drift.com, Inc.;Drift.com, Inc.;G10L15/22,G06F9/542,G06F16/338,G06F16/9017,G06F16/90332,G06F16/9038,G06F40/35,G10L15/063,G10L15/32,G10L2015/0631;Selecting a response in a multi-turn interaction between a user and a conversational bot;A method to select a response in a multi-turn conversation between a user and a conversational bot. The conversation is composed of a set of events, wherein an event is a linear sequence of observations that are user speech or physical actions. Queries are processed against a set of conversations that are organized as a set of inter-related data tables, with events and observations stored in distinct tables. As the multi-turn conversation proceeds, a data model comprising an observation history, together with a hierarchy of events determined to represent the conversation up to at least one turn, is persisted. When a new input (speech or physical action) is received, it is classified using a statistical model to generate a result. The result is then mapped to an observation in the data model. Using the mapped observation, a look-up is performed into the data tables to retrieve a possible response.;1
327;10818308;2018;2020;Snap Inc.;Snap Inc.;G10L21/013,G10H1/06,G10H1/361,G10L13/00,G10L13/0335,G10L15/26,G10H2210/051,G10H2210/066,G10H2210/076,G10H2220/011,G10H2220/096,G10H2220/451,G10H2250/455,G10L2015/027,G10L2021/0135;Speech characteristic recognition and conversion;Systems, devices, media, and methods are presented for converting sounds in an audio stream. The systems and methods receive an audio conversion request initiating conversion of one or more sound characteristics of an audio stream from a first state to a second state. The systems and methods access an audio conversion model associated with an audio signature for the second state. The audio stream is converted based on the audio conversion model and an audio construct is compiled from the converted audio stream and a base audio segment. The compiled audio construct is presented at a client device.;0
328;10827214;2020;2020;Augmented And Segmented Media Interface Corp.;Augmented And Segmented Media Interface Corp.;H04N21/4316,H04N21/812,H04N21/23418,H04N21/23424,H04N21/252,H04N21/2542,H04N21/26603,H04N21/4662,H04N21/4667,H04N21/47202,H04N21/4725,H04N21/47815,H04N21/6582,H04N21/8583,G06T19/006;System and method for in-video product placement and in-video purchasing capability using augmented reality;Techniques are provided by which the digital delivery of a viewer-requested video along with the best chosen advertisement for the viewer is improved. These techniques may be particularly suited for the short video industry. An innovative video analytics mechanism and user-behavioral analytics mechanism are provided, with which the best match of an exact product on the video for the particular viewer is advertised on the video, while the viewer is viewing the video. Further, techniques are provided that enable the viewer to purchase the product while still in the video, not having to leave the video or the site to complete the purchase.;0
329;10831976;2019;2020;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06N3/088,G06F18/23213,G06F18/2433,G06F30/398,G06N3/045,G06N20/00;Predicting local layout effects in circuit design patterns;A method for predicting local layout effect in a circuit design pattern includes obtaining a plurality of circuit design patterns, generating layout images from the circuit design patterns, extracting feature vectors from the layout images by processing the layout images in a computer vision machine learning algorithm, comparing the feature vector extracted from a selected layout image to clusters of feature vectors extracted from the layout images, wherein the clusters of feature vectors include an in-range cluster and an outlier cluster, and labelling a circuit design pattern corresponding to the selected layout image, for which threshold voltage has not been experimentally measured, as being an in-range circuit design pattern or an outlier circuit design pattern, in response to the selected layout image respectively correlating with the in-range cluster or with the outlier cluster.;0
330;10839269;2020;2020;King Abdulaziz University;King Abdulaziz University;G06V10/82,G06F18/2178,G06N3/045,G06N3/08,G06V20/00,G06V30/19167,G06V30/19173,G06N3/047,G06N3/088,G06V30/10;System for fast and accurate visual domain adaptation;In the field of computer vision, without sufficient labeled images, it is challenging to train an accurate model. But through visual adaptation from source to target domains, a relevant labeled dataset can help solve such problem. Many methods apply adversarial learning to diminish cross-domain distribution difference. They are able to greatly enhance the performance on target classification tasks. GAN (Generative Adversarial Networks) loss is widely used in adversarial adaptation learning methods to reduce a across-domain distribution difference. However, it becomes difficult to decline such distribution difference if generator or discriminator in GAN fails to work as expected and degrades its performance. To solve such cross-domain classification problems, an adaptation algorithm and system called as Generative Adversarial Distribution Matching (GADM) is implemented. In GADM, the objective function is improved by taking cross-domain discrepancy distance into consideration, and further minimize the difference through the competition between the generator and discriminator, thereby greatly decreasing the cross-domain distribution difference. Even when the performance of its generator or discriminator degrades, GADM is capable of decreasing the cross-domain distribution difference. The GADM algorithm and system employs a single GAN framework so as to achieve faster domain adaption with less computation resource. Specially, GADM transfers target data distribution to source one to keep accurate label dependence information, which ensures high accuracy and stability of source classifier and thus achieves better classification performance on target data.;0
332;10839809;2017;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L17/04,H04M7/0072,G10L15/063,G10L17/18,G10L19/26,G10L21/02,H04L65/70,H04L65/762,H04L65/764,H04L65/80,H04M3/2236,H04M3/568,G10L25/30;Online training with delayed feedback;Bandwidth-efficient (i.e., compressed) representations of audio data can be utilized for near real-time presentation of the audio on one or more receiving devices. Persons identified as having speech represented in the audio data can have trained speech models provided to the devices. These trained models can be used to classify the compressed audio in order to improve the quality to correspond more closely to the uncompressed version, without experiencing lag that might otherwise be associated with transmission of the uncompressed audio. The uncompressed audio is also received, with potential lag, and is used to further train the speech models in near real time. The ability to utilize the uncompressed audio as it is received prevents a need to store or further transmit the audio data for offline processing, and enables the further trained model to be used during the communication session.;0
333;10841251;2020;2020;MOVEWORKS, INC.;MOVEWORKS, INC.;G06F40/35,H04L51/02,G06F40/30,H04L51/214;Multi-domain chatbot;A multi-domain chatbot is used to service a message of a user. An automated agent of the multi-domain chatbot may act as an intermediary between the user and a plurality of domain-specific modules of the multi-domain chatbot. The automated agent may receive the message from the user, determine an intent of the message, and based on the intent, determine a group of the domain-specific modules that should be investigated. The automated agent may then investigate the group of domain-specific modules by sending the user message to and receiving responses from the domain-specific modules within the group. Based on the received responses, the automated agent may determine whether to provide, to the user, one of the domain-specific responses or a null response, in the event that none of the domain-specific responses is aligned with the intent of the message.;2
334;10841666;2020;2020;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04N21/8455,G06F18/2413,G06N3/045,G06N3/08,G06N20/00,G06V10/764,G06V10/82,G06V20/41,G06V20/46,G06V20/49,G11B27/036,H04N21/233,H04N21/23418,H04N21/812,H04N21/8456,H04N21/8547;Generation of points of insertion of directed content into a video asset;Technologies are provided for generation of points of insertion of directed content into a video asset. In some embodiments, multiple time offsets within an interval spanned by the video asset can be determined using audio data corresponding to the video asset. A time offset defines a boundary between first and second segments of the video asset. Using image data corresponding to the video asset, respective pairs of video clips for the multiple time offsets can be generated. Visual features, aural features, and language features pertaining to the respective pairs of video clips can then be generated. Scores for the multiple time offsets can be generated using the visual features, the aural features, and the language features. A score represents an assessment of suitability to insert directed content into the video asset at a time offset. A file that contains specific time offsets can be generated.;0
335;10849585;2019;2020;Siemens Healthcare GmbH;Siemens Healthcare GmbH;A61B5/7267,A61B5/0515,A61B5/055,A61B6/463,A61B6/5205,A61B6/5217,G06N3/044,G06N3/08,G06N3/088,G06N20/00,G06T7/514,G06T11/003,G06V10/764,G06V10/82,A61B5/0033,A61B5/0037,A61B6/545,G06F18/217,G06F18/24143,G06N3/045,G06N3/047,G06N3/084,G06N7/01,G06T7/0014,G06T7/70,G06T2207/10028,G06T2207/10116,G06T2207/20081,G06T2207/20084,G06T2207/30064,G06V2201/03,G06V2201/033;Anomaly detection using parametrized X-ray images;For anomaly detection based on topogram predication from surface data, a sensor captures the outside surface of a patient. A generative adversarial network (GAN) generates a topogram representing an interior anatomy based on the outside surface of the patient. An X-ray image of the patient is acquired and compared to the generated topogram. By quantifying the difference between the real X-ray image and the predicted one, anatomical anomalies may be detected.;0
336;10860661;2017;2020;INTUIT INC.;INTUIT INC.;G06F16/951,G06F16/3329,G06Q40/123;Content-dependent processing of questions and answers;The disclosed embodiments relate to a computer system that facilitates the providing of an answer to a question. During operation, the computer system receives the question from a user, where the question is related to income taxes. Then, the computer system performs content-dependent processing of the question based on a tax-information data structure to produce the answer to the question. Note that the tax-information data structure includes: tax phrases, context information associated with the tax phrases, tax concepts that encompass multiple tax phrases, and/or statistical association metrics between the tax phrases and the tax concepts. Next, the computer system provides the answer to the user.;2
337;10872116;2019;2020;TIMECODE ARCHIVE CORP.;TIMECODE ARCHIVE CORP.;G06F16/65,G06F40/35,G06F3/0484,G06F3/165,G06F16/61,G06F16/635,G06F16/639,G06F40/30;Systems, devices, and methods for contextualizing media;Disclosed herein are systems, devices, and methods for contextualizing media. In some variations, a method of organizing audio may comprise generating first graph data nodes from structured text data comprising a predetermined audio data model and generating second graph data nodes from unstructured data. The first and second graph data nodes may be associated with the audio. The one or more first graph data nodes may be linked to the one or more corresponding second graph data nodes using a natural language processing model.;1
338;10884710;2019;2021;Accenture Global Solutions Limited;Accenture Global Solutions Limited;G06F8/34,G06F8/38,G06F8/20,G06N3/08,G06F8/64;System and method for generating unified experiences on digital platforms;A system and method for intelligently and automatically generating deployable code for target platforms and frameworks based on images or other graphical inputs is disclosed. The system and method leverage artificial intelligence to automatically identify and classify elements of a design as feature patterns. Identification is performed using convolutional neural networks, while classification is done using a Softmax classifier. The intelligent system can then automatically generate code for target platforms and frameworks that reproduce the feature patterns. Target platforms may include web platforms, mobile platforms (such as mobile phones), wearable platforms (such as smart watches), and extended reality platforms (which includes augmented reality (AR), virtual reality (VR), and/or combinations of AR/VR).;4
339;10885423;2019;2021;UiPath, Inc.;UiPath, Inc.;G06N3/004,G06F3/04817,G06F9/451;Systems and methods of activity target selection for robotic process automation;A software robot is designed to carry out an activity (e.g., a mouse click, a text input, etc.) on a target element (e.g., a button, an input field, etc.) of a user interface. The robot's code specification is configured to include an on-screen image of the target element and a text displayed by the target element. The robot is configured to automatically identify the target element at runtime according to an element ID specified in the source code of the user interface, and when such identification fails, to identify the target element according to the text and image stored in the robot's code.;0
340;10885436;2020;2021;GOOGLE LLC;GOOGLE LLC;G06F40/56,G06N3/08,G06F40/284,G06F40/30,G06N3/045,G06N3/084;Training text summarization neural networks with an extracted segments prediction objective;"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for training a text summarization neural network. One of the methods includes pre-training the text summarization neural network including learning values of a plurality of network parameters through self-supervised learning using unlabeled data comprising unlabeled first texts, the pre-training including: obtaining an unlabeled first text comprising a plurality of segments; selecting one or more of the plurality of segments; processing a masked first text that excludes the one or more selected segments to generate a prediction of the one or more selected segments; and determining, based on a difference between the prediction and the one or more selected segments, an update to the current values of the plurality of network parameters; adapting the pre-trained text summarization neural network for a specific text summarization task using labeled data comprising second texts and respective summaries of the second texts.";2
341;10885707;2020;2021;FUDAN UNIVERSITY;FUDAN UNIVERSITY;G06T17/205,G06V10/82,G06N3/08,G06T7/50,G06T7/60,G06T17/00,G06T17/20,G06V10/454,G06V20/64,H04N13/275,G06T2207/20084;Network, system and method for multi-view 3D mesh generation via deformation;A network for generating 3D shape includes a perceptual network and a Graphic Convolutional Network (GCN). The GCN includes a coarse shape generation network for generating a coarse shape, and a Multi-View Deformation Network (MDN) for refining the coarse shape. The MDN further comprises at least one MDN unit, which in turn comprises a deformation hypothesis sampling module, a cross-view perceptual feature pooling module and a deformation reasoning module. Systems and methods are also provided.;1
342;10891723;2018;2021;Snap Inc.;Snap Inc.;H04L67/04,G06N3/045,G06N3/08,G06N3/088,G06N20/00,G06T5/00,G06T5/40,G06T5/92,G06T7/90,G06T11/60,G06V10/7753,G06V10/82,G06V40/161,G06V40/175,H04L67/10,H04N1/60,G06N3/047,G06Q50/01,G06T2207/10024,G06T2207/20132;Realistic neural network based image style transfer;A mobile device can implement a neural network-based style transfer scheme to modify an image in a first style to a second style. The style transfer scheme can be configured to detect an object in the image, apply an effect to the image, and blend the image using color space adjustments and blending schemes to generate a realistic result image. The style transfer scheme can further be configured to efficiently execute on the constrained device by removing operational layers based on resources available on the mobile device.;0
343;10897675;2019;2021;Sonova AG;Sonova AG;G10L25/30,H04R25/507,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G10L21/0208,G10L25/84,H04R1/1083,H04R25/43,G10L25/69,H04R2225/021,H04R2225/025,H04R2225/43,H04R2225/67;Training a filter for noise reduction in a hearing device;The disclosed technology relates to training a neural network and using that training to design a filter for the hearing device. The training can include training a generative adversarial network (GAN) to determine whether a signal includes intelligible speech, wherein the GAN includes a generator and discriminator. The generator can provide a mixed noise and speech signal to the discriminator can output a signal that reduces noise in the mixed signal. The technology can then determine whether the output signal is intelligible based on a speech metric function or based on feedback from a hearing care expert. The generator and the discriminator can continue to train based on this feedback. And then the trained network can use the settings of the discriminator or information from the design of the discriminator to generate a noise filter for a hearing device.;0
344;10902191;2019;2021;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F16/345,G06F40/166,G06F16/93,G06F40/20,G06F40/279,G06F40/30,G06F40/56,G06N3/04,G06N3/045,G06N3/08,G06N3/044;Natural language processing techniques for generating a document summary;A system for generating a summary of a text document is disclosed. In some examples, the system includes a processor configured to generate an initial summary of an original document. The initial summary includes a selection of extracted sentences copied from the original document. For each extracted sentence of the initial summary, the processor processes the extracted sentence to generate an abstracted sentence, and generates vector representations of the extracted sentence, the abstracted sentence, the original document, and the current summary. The vector representations are then input to a decision network to compute an editing decision. The editing decision is selected from a group of possible decisions that includes a decision to add the extracted sentence and a decision to add the abstracted sentence. The processor also updates the current summary based on the editing decision.;2
345;10904212;2018;2021;VeriSign, Inc.;VeriSign, Inc.;H04L61/3025,G06F16/90332,G06F16/9535,G06N20/00,H04L51/02,H04L51/046,H04L51/08,H04L51/52,H04L61/302,H04L63/083,H04L67/306,H04L67/535,H04L51/18;Domain name suggestion and registration via chatbot;"Techniques for providing domain name suggestions to a user that is a prospective registrant via chatbot are disclosed. The techniques include providing a publicly available online chatbot to the user; requesting domain name generation data from the user via the chatbot and during a chatbot session with the user; receiving domain name generation data based on the requesting; generating a plurality of generated domain names using the domain name generation data; filtering registered domain names out of the plurality of generated domain names to produce a plurality of unregistered generated domain names; offering to register at least one of the unregistered generated domain names to the user; receiving an offer acceptance from the user; directing the user, via the chatbot, to provide information sufficient to register the at least one of the unregistered generated domain names; and facilitating registration of the at least one of the unregistered generated domain names.";2
346;10908950;2018;2021;Automation Anywhere, Inc.;Automation Anywhere, Inc.;G06F9/4881;Robotic process automation system with queue orchestration and task prioritization;A robotic process automation (RPA) system receives task prioritization inputs that specify prioritization for processing of a set of RPA tasks. The tasks are performed in accordance with the specified priorities. The RPA system also receives queue orchestration commands that specify conditions under which tasks processed from a first queue are sent to another queue for subsequent processing. The RPA system also provides service level automation in accordance with specified parameters. Further task prioritization may be specified to provide quality of service performance.;0
347;10915818;2020;2021;NotCo Delaware, LLC;NotCo Delaware, LLC;G06N3/088,G06N3/044,G06N3/045,G06N3/047,G06N3/084,G06N5/01;Latent space method of generating food formulas;Techniques to mimic a target food item using artificial intelligence are disclosed. A formula generator learns from open source and proprietary databases of ingredients and recipes. The formula generator is trained using features of the ingredients and using recipes. Given a target food item, the formula generator determines a formula that matches the given target food item and a score for the formula. The formula generator may generate numerous formulas that match the given target food item and may select an optimal formula from the generated formulas based on score.;2
348;10916050;2019;2021;TENCENT AMERICA LLC;TENCENT AMERICA LLC;G06T15/503,G06N3/045,G06N3/047,G06N3/08,G06T7/13,G06T7/70,G06T7/90,G06T19/00,G06T2207/20081,G06T2207/20084,G06T2207/30196,G06T2219/004,H04L67/10;Method and apparatus for synthesizing realistic hand poses based on blending generative adversarial networks;"A method of synthesizing an image of a hand using a blending generative adversarial network (BlendGAN) includes obtaining a synthetic 3-dimensional (3D) hand pose including a 3D model of a hand; obtaining a real background image; combining the synthetic 3D hand pose with the real background image to create a synthetic hand image; and blending the synthetic hand image using the BlendGAN to create a blended synthetic hand image.";3
349;10916351;2020;2021;KOREA INTERNET & SECURITY AGENCY;KOREA INTERNET & SECURITY AGENCY;H04L63/1416,G06F18/2413,G06F18/2431,G06N3/04,G06N3/045,G06N3/084,G06N3/088,G16Y40/50,H04L63/1425,H04L63/1441,H04W4/70;Method and apparatus for identifying the type of cyber-attack against IoT devices;Provided is a method for classifying a cyber-attack performed in a computing device having an artificial neural network. The method comprises obtaining a plurality of features extracted from collected packets and inputting the plurality of features into the artificial neural network and using data output from the artificial neural network to determine a type of cyber-attack indicated by the collected packet.;0
350;10922788;2020;2021;STRADVISION, INC.;STRADVISION, INC.;G06T1/20,G06F18/214,G06F18/24317,G06N3/045,G06N3/063,G06N3/084,G06N3/088,G06T3/4046,G06V10/764,G06V10/774,G06V10/82;Method for performing continual learning on classifier in client capable of classifying images by using continual learning server and continual learning server using the same;"A method for performing continual learning on a classifier, in a client, capable of classifying images by using a continual learning server is provided. The method includes steps of: a continual learning server (a) inputting first hard images from a first classifier of a client into an Adversarial Autoencoder, to allow an encoder to output latent vectors from the first hard images, allow a decoder to output reconstructed images from the latent vectors, and allow a discriminator and a second classifier to output attribute and classification information to determine second hard images to be stored in a first training data set, and generating augmented images to be stored in a second training data set by adjusting the latent vectors of the reconstructed images determined not as the second hard images; (b) continual learning a third classifier corresponding to the first classifier; and (c) transmitting updated parameters to the client.";1
351;10929485;2016;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/9535,G06F16/90332,G06F16/9038,G06N20/00,G06N3/08;Bot search and dispatch engine;Techniques for improving access to and interactions with bots are described. In an example, a first bot, hosted on a computing system, may identify an action to be performed for a user associated with a computing device. The action may be identified based on a user interaction with the first bot, where the user interaction may be provided from the computing device. The first bot may select a second bot based on the action. The second bot may be hosted on a same or a different computing system. The first bot may determine, based on a previous user input to the first bot, a parameter value for an execution of the action and may send the parameter value to the second bot. In response, the first bot may receive a result of an execution of the action and may provide information about the result to the computing device.;0
352;10929781;2019;2021;Capital One Services, LLC;Capital One Services, LLC;G06N20/00,G06F3/16;Systems and methods for determining training parameters for dialog generation;A method for determining machine learning training parameters is disclosed. The method can include a processor receiving a first input. The processor may receive a first response to the first input, determine a first intent, and identify a first action. The processor can then determine first trainable parameter(s) and determine whether the first trainable parameter(s) is negative or positive. Further, the processor can update a training algorithm based on the first trainable parameter(s). The processor can then receive a second input and determine a second intent for the second input. The processor can also determine a second action for the second intent and transmit the second action to a user. The processor can then determine second trainable parameter(s) and determine whether the second trainable parameter(s) is positive or negative. Finally, the processor can further update the training algorithm based on the second trainable parameter(s).;0
353;10930066;2020;2021;Mythical, Inc.;Mythical, Inc.;G06T17/10,G06F40/14,G06F40/40,G06T9/00,G06F40/20,G06T2200/24;Systems and methods for using natural language processing (NLP) to automatically generate three-dimensional objects in a virtual space;"Systems and methods for using natural language processing (NLP) to automatically generate three-dimensional objects in a virtual space are disclosed. Exemplary implementations may: obtain three-dimensional objects using a three-dimensional voxelized format; encode those objects, using a variational autoencoder, into pairs of vectors that are subsequently sampled; decode the sampled vectors; determine loss information for the decoded voxelized three-dimensional objects; use the loss information to train the variational autoencoder; fine-tune a pretrained text-based system; receive user input describing a three-dimensional object; generate a vector from the user input; decode the vector into a voxelized three-dimensional object; present the voxelized three-dimensional object to the user.";4
354;10930263;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/033,H04N21/233,G06N3/006,G06N3/045,G06N3/088,G10L13/00,G10L13/027,G10L13/086,G10L15/005,G10L15/16,G10L25/30,H04N21/234336,H04N21/251,H04N21/25841,H04N21/4856,H04N21/8106,H04N21/854,G06N3/044,G10L2013/105;Automatic voice dubbing for media content localization;This disclosure describes techniques for replicating characteristics of an actor or actresses voice across different languages. The disclosed techniques have the practical application of enabling automatic generation of dubbed video content for multiple languages, with particular speakers in each dubbing having the same voice characteristics as the corresponding speakers in the original version of the video content.;2
355;10930272;2020;2021;Drift.com, Inc.;Drift.com, Inc.;G10L15/1815,G06F16/3344,G06N20/00,G10L15/063;Event-based semantic search and retrieval;A technique for semantic search and retrieval that is event-based, wherein is event is composed of a sequence of observations that are user speech or physical actions. Using a first set of conversations, a machine learning model is trained against groupings of utterances therein to generate a speech act classifier. Observation sequences therein are organized into groupings of events and configured for subsequent event recognition. A set of second (unannotated) conversations are then received. The set of second conversations is evaluated using the speech act classifier and information retrieved from the event recognition to generate event-level metadata that comprises, for each utterance or physical action within an event, one or more associated tags. In response to a query, a search is performed against the metadata. Because the metadata is derived from event recognition, the search is performed against events learned from the set of first conversations. One or more conversation fragments that, from an event-based perspective, are semantically-relevant to the query, are returned.;0
356;10931999;2016;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/1822,H04N21/439,G06F3/167,G10L15/22,G10L15/26,G10L25/51,H04N21/233,H04N21/25833,H04N21/25875,H04N21/25891,H04N21/26208,H04N21/42203,H04N21/43079,H04N21/4415,H04N21/4751,H04N21/6587,H04N21/8106,H04N21/835,H04N21/8586,G06F16/9038,G06F40/30,G10L13/00,G10L15/30,G10L2015/223;Systems and methods for routing content to an associated output device;Apparatuses and methods for routing content are provided herein. In some embodiments, a method for routing content include receiving audio data representing a command from a first electronic device, determining content that is associated with the command, sending responsive audio data to the first electronic device, and sending instructions to the second electronic device to output the content associated with the command. In some embodiments, a method for routing contents includes determining a state of the second electronic device and sending instructions to output the content to a selected one of the first and second electronic devices based on the state of the second electronic device.;0
357;10937237;2020;2021;ADOBE INC.;ADOBE INC.;G06T17/205,G06N3/045,G06N3/047,G06N3/084,G06T7/593,G06T7/75,G06T7/77,G06T19/20,G06T2200/08,G06T2207/20081,G06T2207/20084;Reconstructing three-dimensional scenes using multi-view cycle projection;Methods, systems, and non-transitory computer readable storage media are disclosed for reconstructing three-dimensional object meshes from two-dimensional images of objects using multi-view cycle projection. For example, the disclosed system can determine a multi-view cycle projection loss across a plurality of images of an object via an estimated three-dimensional object mesh of the object. For example, the disclosed system uses a pixel mapping neural network to project a sampled pixel location across a plurality of images of an object and via a three-dimensional mesh representing the object. The disclosed system determines a multi-view cycle consistency loss based on a difference between the sampled pixel location and a cycle projection of the sampled pixel location and uses the loss to update the pixel mapping neural network, a latent vector representing the object, or a shape generation neural network that uses the latent vector to generate the object mesh.;3
358;10937542;2020;2021;VENT CREATIVITY CORPORATION;VENT CREATIVITY CORPORATION;G16H30/40,G06T7/0012,G06T7/11,G06T19/20,G16H30/20,G16H50/50,G06T2207/10028,G06T2207/20081,G06T2207/20084,G06T2207/20101,G06T2219/2012,G16H20/30,G16H20/40;Patient specific treatment planning;"A method of patient specific treatment planning is described herein. The method includes receiving an image file of a region of interest of an anatomy; extracting co-ordinates information and density information of a plurality of points of an image of the image file; pre-training a neural network based on the co-ordinates information, the density information and collective information of a database; performing at least one of a virtual action, and a treatment, via a user, on the region of interest based on collective information in the database; and training the neural network based on a user input, and the collective information from the database. The collective information comprises a plurality of clusters of different physiological states. The plurality of clusters comprises a plurality of sub-clusters. The virtual action and the treatment are performed through at least one of a virtual reality and an augmented reality.";1
359;10943072;2020;2021;ConverSight.ai, Inc.;ConverSight.ai, Inc.;G06F40/30,G06F3/0482,G06F16/288,G06F16/3329,G06F16/9024,G06F40/279,G06N3/044,G06N3/045,G06N3/08,G06N5/022,G06N5/041,G06F16/243;Contextual and intent based natural language processing system and method;Methods and systems are disclosed for an artificial intelligence (AI)-based, conversational insight and action platform for user context and intent-based natural language processing and data insight generation. Using artificial intelligence and semantic analysis techniques, a knowledge graph is generated from structured data, and a word embedding is generated from unstructured data. A semantic meaning is extracted from a user request, and at least one user attribute and context are determined. One or more entities and relationships on the knowledge graph that match the semantic meaning are determined, based on the user attribute, context, and the word embedding. A sequence of analytical instructions is generated from the matching results, and applied to the structured data to generate a data insight response to the user request. If no matches are found, similar entities and relationships are presented to the user, and user selections are used to further train the system.;3
360;10943309;2017;2021;INTUIT INC.;INTUIT INC.;G06Q40/123,G06N5/04,G06N7/01;System and method for providing a predicted tax refund range based on probabilistic calculation;A method and system provide estimated tax refund data to a user of a tax return preparation system throughout personalized tax return preparation interview. The method and system receive current user tax related data associated with the user, retrieve tax rules data, and gather historical tax related data associated with historical users of the tax return preparation system. The method and system further generate probabilistic inference data including inferences about tax related characteristics of the user based on the historical tax related data and the tax rules data. The method and system provide estimated tax refund data to the user based on the probabilistic inference data.;0
361;10943583;2018;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/18,G10L15/183,G06F16/313,G06F16/3347,G10L15/063,G10L25/27;Creation of language models for speech recognition;A system to perform automatic speech recognition (ASR) using a dynamic language model. Portions of the language model can include a group of probabilities rather than a single probability. At runtime individual probabilities of the group are weighted and combined to create an adjusted probability for the portion of the language model. The adjusted probability can be used for ASR processing. The weights can be determined based on a characteristic of the utterance, for example an associated speechlet/application, the specific user speaking, or other characteristic. By applying the weights at runtime the system can use a single language model to dynamically adjust to different utterance conditions.;0
362;10944867;2020;2021;GOOGLE LLC;GOOGLE LLC;H04M3/432,H04M3/436,G06Q10/06311,G06Q10/103,G10L15/22,H04M3/42042,H04M3/42059,H04M3/42204,H04M3/4283,H04M3/46,H04M3/4936,H04M3/5158,H04M3/5166,H04M3/5183,G10L13/00,H04M2201/39,H04M2201/40,H04M2203/1058;Reducing telephone network traffic through automated telephone calls;Implementations are directed to using an assistant to initiate automated telephone calls with entities. Some implementations identify an item of interest, identify a group of entities associated with the item, and initiate the calls with the entities. During a given call with a given entity, the assistant can request a status update regarding the item, and determine a temporal delay before initiating another call with the given entity to request a further status update regarding the item based on information received responsive to the request. Other implementations receive a request to perform an action on behalf of a user, identify a group of entities that can perform the action, and initiate a given call with a given entity. During the given call, the assistant can initiate an additional call with an additional entity, and generate notification(s), for the user, based on result(s) of the given call and/or the additional call.;1
363;10945040;2019;2021;ADOBE INC.;ADOBE INC.;H04N21/47,G10L15/26,G11B27/102,G11B27/11,G11B27/309,G11B27/322,H04N21/4316,H04N21/440245,H04N21/8547,H04N21/8549;Generating and providing topic visual elements based on audio content and video content of a digital video;The present disclosure relates to methods, systems, and non-transitory computer-readable media for generating a topic visual element for a portion of a digital video based on audio content and visual content of the digital video. For example, the disclosed systems can generate a map between words of the audio content and their corresponding timestamps from the digital video and then modify the map by associating importance weights with one or more of the words. Further, the disclosed systems can generate an additional map by associating words embedded in one or more video frames of the visual content with their corresponding timestamps. Based on these maps, the disclosed systems can identify a topic for a portion of the digital video (e.g., a portion currently previewed on a computing device), generate a topic visual element that includes the topic, and provide the topic visual element for display on a computing device.;6
364;10945051;2020;2021;Bank of America Corporation;Bank of America Corporation;H04N21/44008,H04N21/8358,G06N3/045,G06N3/047,G06N3/088,G06N20/20,H04N21/4666,H04N21/8146,G06N20/00;System and method for intentionally distorting digital media to reduce the accuracy of generative machine learning algorithms;An apparatus includes a processor that monitors transmissions destined for an external network, determines that a transmission includes original media associated with a subject, and intercepts the transmission before it reaches the external network. The processor generates modified media by selecting a subset of data elements of the original media and replacing a value of each data element of the subset with a new value. At least one of the subset of data elements and the set of new values is chosen such that an accuracy metric calculated for a first generative algorithm, trained to generate synthetic representations of the subject based on modified media, is less than, by a given factor, the accuracy metric calculated for a second generative algorithm, trained to generate synthetic representations of the subject based on original media. The processor replaces the transmission with a new transmission that includes the modified media.;3
365;10951554;2019;2021;;;H04L51/02,G06Q30/0641,H04L51/046,H04L51/04,H04L51/52;Systems and methods facilitating bot communications;"A method for delivering messages from customers to bots that includes providing a bot gateway and, pursuant to a process, formatting and sending the messages. The bot gateway includes bot schemas that each defines a data field arrangement for sending requests to a particular bot. The process includes receiving a first message and determining therefrom a first customer, a first tenant, a first bot, and a text message from the first customer to the first bot. The process includes: providing a bot configuration data set; selecting a first bot schema pertaining to the first bot; creating a formatted request via mapping the text message and the data values defined in the bot configuration data set to corresponding data fields defined within the data field arrangement; and sending the formatted request to the first bot.";1
366;10956808;2020;2021;Fractal Analytics Private Limited;Fractal Analytics Private Limited;G06N3/045,G06F16/24568,G06N3/044,G06N3/047,G06N3/088;System and method for unsupervised anomaly detection;Some embodiments are associated with a system and method for deep learning unsupervised anomaly detection in Internet of Things (IoT) sensor networks or manufacturing execution systems. The system and method use an ensemble of a plurality of generative adversarial networks for anomaly detection.;0
367;10957017;2019;2021;Shutterstock, Inc.;Shutterstock, Inc.;G06V10/764,G06T1/60,G06T3/4076,G06T5/00,G06V10/82,G06T2207/20081;Synthetic image detector;A method including receiving a first image file in a network server is provided. The method also includes selecting multiple criteria in a test ensemble for the first image file and evaluating a synthetic value for the first image file according to the test ensemble. The method includes storing the first image file and the synthetic value for the first image file in a database, and providing for display the first image file and the synthetic value for the first image file in response to a search query from a user. A system and a non-transitory, computer-readable medium storing instructions to perform the above method are also provided.;0
368;10962473;2020;2021;NotCo Delaware, LLC;NotCo Delaware, LLC;G01N21/35,G06N5/04,G06N20/00,G01N2021/3595,G06Q30/0201;Protein secondary structure prediction;An artificial intelligence model receives a FTIR spectrum of a given ingredient to predict its protein secondary structure. The model includes three artificial modules, which generate three predicted values corresponding to structural categories (e.g., Î±-helix, Î²-sheet, and other) of the predicted secondary structure. Proteins may be compared for similarity based on predicted values corresponding to the structural categories of the predicted secondary structure.;2
369;10963231;2019;2021;UiPath, Inc.;UiPath, Inc.;G06F8/60,G06F8/35,G06F11/3466,G06N20/00,G06Q10/063,G06Q10/103;Using artificial intelligence to select and chain models for robotic process automation;Using artificial intelligence (AI) to select and/or chain robotic process automation (RPA) models a given problem is disclosed. A model of models (e.g., an RPA robot or an ML model) may serve as an additional layer on an existing system that makes the existing models more effective. This model of models may incorporate AI that learns an improved or best set of rules or an order from existing models, potentially taking certain activities from a model, feeding input from one model into another, and/or chaining models in some embodiments.;0
370;10963493;2018;2021;AIBrain Corporation;AIBrain Corporation;G06F40/211,G05D1/0016,G05D1/021,G05D1/0221,G06F16/3329,G06F18/254,G06F40/289,G06F40/30,G06F40/40,G06F40/56,G06N3/008,G06N3/042,G06N3/045,G06N3/08,G06N3/088,G06N5/022,G06N5/041,G06V10/764,G06V10/809,G06V10/82,G06V20/10,G06V40/168,G06V40/174,G06N5/01,G06N20/00,G06V40/161;Interactive game with robot system;A system comprises a motor and a base. The motor is configured to control the physical movement of the system and the base is configured to be coupled with a computing device. The computing device is configured to receive an indication of a target object and detect an obstacle object in a physical environment of the system. In response to detecting an obstacle object, the computing device provides to a user an inquiry associated with the detected obstacle object and receives a natural language response. Based at least in part on the natural language response, the computing device controls the motor to navigate the system around the detected obstacle object. Once the target object is detected, the computing device provides an indication that the target object has been located.;0
371;10963497;2016;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/3334,G06F16/3347,G06F16/3344;Multi-stage query processing;A query parsing system uses a multi-stage process to parse the text of incoming queries before attempting to answer the queries. The multi-stage configuration involves a first trained classifier to determine the query type, or intent, of the text and a plurality of second trained classifiers, where each of the second trained classifiers is configured particularly for one specific respective query type. During query processing, the first trained classifier is used on the text to identify the query type. A second trained classifier for that specific identified query type is then found and used on the text to identify what strings in the text correspond to specific entities needed to resolve the query. The identified text strings and query type are then placed into a form understandable by a knowledge base and sent to the knowledge base for resolution. The classifiers may be trained using queries and answers previously processed by the knowledge base using a rules/template resolution process.;0
372;10963748;2018;2021;Snap Inc.;Snap Inc.;G06N3/088,G06F18/2148,G06F18/2185,G06N3/045,G06N3/047,G06N3/08,G06V10/764,G06V10/7747,G06V10/7788,G06V10/82;Generative neural network distillation;A compact generative neural network can be distilled from a teacher generative neural network using a training network. The compact network can be trained on the input data and output data of the teacher network. The training network train the student network using a discrimination layer and one or more types of losses, such as perception loss and adversarial loss.;1
373;10963812;2017;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N20/00,G06N5/04,G06N7/01,G06Q30/0643;Model-based artificial intelligence data mining system for dimension estimation;Some aspects of the present disclosure relate to computer processes for generating and training a generative machine learning model to estimate the true sizes of items and users of an electronic catalog and subsequently applied to determine fit recommendations, as well as confidence values for the fit recommendations, for how a particular item may fit a particular user. During training, the disclosed generative model can implement Bayesian statistical inference to calculate estimated true sizes of both items and users of an electronic catalog using both (1) a prior distribution of sizes for items and users and (2) a distribution based on obtained evidence regarding how items actually fit users. The resulting posterior distribution can be approximated using a proposal distribution used to generate the fit recommendations and associated confidence values.;1
374;10963819;2017;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06Q10/02,G06F40/35,G06F40/44,G06F40/56,G06N3/044,G06N3/045,G06N3/047,G06N3/084,H04L51/02;Goal-oriented dialog systems and methods;"A goal-oriented dialog system interacts with a user over one or more turns of dialog to determine a goal expressed by the user; the dialog system may then act to fulfill the goal by, for example, calling an application-programming interface. The user may supply dialog via text, speech, or other communication. The dialog system includes a first trained model, such as a translation model, to encode the dialog from the user into a context vector; a second trained model, such as another translation model, determines a plurality of candidate probabilities of items in a vocabulary. A language model determines responses to the user based on the input from the user, the context vector, and the plurality of candidate probabilities.";1
375;10963823;2018;2021;Massachusetts Mutual Life Insurance Company;Massachusetts Mutual Life Insurance Company;G06Q10/063,G06Q10/107,G06Q10/109;Systems and methods for chatbot applications performing tasks based on user stress levels;"A system comprising: a server configured to: generate an interface based on a first user profile stored in a database; present the interface on a first client associated with the first user profile, wherein the interface depicts a set of input elements; receive a set of inputs from the set of input elements; generate a macro based on the set of inputs; associate the macro with a range of values based on the set of inputs; host a chatbot application; generate a value based on the chatbot application accessing a PIM application on a second client associated with a second user profile stored in the database; and enable the macro to be performed based on the value being within the range of values.";0
376;10965948;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04N19/20,G06T9/002,H04N19/124,H04N19/13,H04N19/167,H04N19/17,H04N19/33,H04N19/439,H04N19/91;Hierarchical auto-regressive image compression system;The present application relates to a multi-stage encoder/decoder system that provides image compression using hierarchical auto-regressive models and saliency-based masks. The multi-stage encoder/decoder system includes a first stage and a second stage of a trained image compression network, such that the second stage, based on the image compression performed by the first stage, identify certain redundancies that can be removed from the bit string to reduce the storage and bandwidth requirements. Additionally, by using saliency-based masks, distortions in different sections of the image can be weighted differently to further improve the image compression performance.;0
377;10970188;2020;2021;HoxHunt Oy;HoxHunt Oy;G06F11/3438,H04L63/20,G06F9/44526,G06N20/00,G06Q10/0635,G06Q10/06398,G06Q50/2057,H04L63/1416,H04L63/1441;System for improving cybersecurity and a method therefor;A system for improving cybersecurity includes a server configured to define a group of users associated with corresponding user devices, send calibration vectors to user devices, receive actions performed calibration vectors by users, allocate users in the group to one of sub-groups based on received actions, send a set of simulated vectors to user devices associated with users allocated to a particular subgroup, receive actions performed on the simulated vectors by each of the users in the subgroups, define a threshold expertise level for the subgroups, receive, one or more threat vectors reported by one or more users allocated to the sub-groups having the expertise level above the threshold expertise level, update the set of simulated vectors based on the received threat vector, and use the updated set of simulated vectors as training vectors for the each of the users allocated to a particular subgroup to improve cybersecurity.;0
378;10977783;2019;2021;Ford Global Technologies, LLC;Ford Global Technologies, LLC;G06N3/088,G06F18/214,G06F18/217,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G06T7/0002,G06V10/70,G06V10/764,G06V10/82,G06V20/56,G06T2207/20081,G06T2207/20084,G06T2207/30168;Quantifying photorealism in simulated data with GANs;The present disclosure discloses a system and a method. In an example implementation, the system and the method can receive a synthetic image at a first deep neural network, and determine, via the first deep neural network, a prediction indicative of whether the synthetic image is machine-generated or is sourced from the real data distribution. The prediction can comprise a quantitative measure of photorealism of synthetic image.;0
379;10978054;2020;2021;Accenture Global Solutions Limited;Accenture Global Solutions Limited;G10L15/1815,G06F16/43,G06N7/01,G06N20/00,G06N20/10,G10L15/22,G10L15/30,H04L51/02,G06N3/006,G10L15/26;Utilizing machine learning models for determining an optimized resolution path for an interaction;In some implementations, a device may receive unstructured interaction data identifying an interaction of a user with a user device. The device may receive historical unstructured interaction data identifying historical interactions of users and historical unstructured resolution data identifying historical resolutions to the historical interactions. The device may process the historical unstructured interaction data and the historical unstructured resolution data to determine historical structured interaction data and historical structured resolution data. The device may process the unstructured interaction data and the historical structured interaction data to determine pretext identifiers for the interaction of the user. The device may process the pretext identifiers and the historical structured resolution data to generate a resolution network identifying possible resolutions to the interaction of the user. The device may process the pretext identifiers and the resolution network to determine a resolution path identifying a resolution to the interaction of the user.;0
380;10979377;2020;2021;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06Q10/10,H04L51/216;Multi-message conversation summaries and annotations;The disclosure herein describes a system for generating customizable summaries of multi-message conversations in email threads and other multi-message and multi-participant messaging applications. A summary icon is displayed if the number of messages in a thread exceeds a threshold number. Message data is cleaned and analyzed by ML models to generate feature vectors representing the messages content. Multiple loglinear regression models compute similarity between message sentences. Clustering algorithm(s) to generate a multi-message conversation thread summary based on the values. If the user selects the summary icon, the multi-message conversation thread summary is displayed. The summary includes summary sentences and/or annotations describing content of the messages in the thread, participants contributing to the messages in the thread, links back to the original messages in the thread and/or a count of the number of messages contributed by each participant.;2
381;10983446;2019;2021;UNITED MICROELECTRONICS CORP.;UNITED MICROELECTRONICS CORP.;G03F7/70641,G03F7/70808,G03F7/70266,G03F7/70491,G03F7/705,G03F7/70525,G03F7/70558,G03F7/70775;Control equipment and control method of stepper;A control equipment and a control method of a stepper are provided. The control equipment of the stepper includes an input device, a generating device and a processing device. The input device is configured to input a plurality of sample development patterns. The sample development patterns are obtained according to a plurality of sample focal length values. The generating device is configured to generate a plurality of generative categories corresponding to a plurality of generative focal length values by using a depth learning algorithm. The processing device is configured to analyze an estimated focal length value of the online development pattern according to the generative categories.;0
382;10984225;2020;2021;Accenture Global Solutions Limited;Accenture Global Solutions Limited;G06V40/172,G06N3/08,G06V40/161,G06V40/168,G06V40/171,G06V40/40;Masked face recognition;Embodiments of the present disclosure provide systems and methods for recognizing a masked face. According to the present disclosure, the disclosed systems and methods include features that provide augmentation of existing face recognition databases, real-time mask detection, and real-time masked face recognition. In embodiments, masked face recognition includes a multi-layered approach, which includes finding matching simulated masked faces in the database that match the masked face being analyzed, comparing the unmasked portion of the masked face to stored unmasked faces in a database to identify any matches, and executing face restoration algorithms in which the masked portion is reconstructed to generate an unmasked representation which may then be matched against unmasked faces in the database.;0
383;10990619;2018;2021;Luka, Inc.;Luka, Inc.;G06F16/3329,G06F16/31,G06F16/313,G06F40/35,G10L15/28,H04L51/02;Multi-tier conversational architecture with prioritized tier-driven production rules;A chatbot maintains a conversation with a user by providing a plurality of separate conversational tiers that allow the chatbot to interact with the user, each of the conversational tiers having a set of production rules that are independent of production rules for other ones of the tiers. The production rules indicate a flow of conversation between the user and the chatbot. The chatbot selects one of the conversational tiers based on previous conversational tiers used by the chatbot in connection with conversing with the user and based on content of conversations between the user and the chatbot. The chatbot responds to statements of the user according to a specific production rule that is chosen by the chatbot based on a particular one of the conversational tiers that has been selected and based on other factors. A scripting engine may match statements of the user with specific production rules.;1
384;10990848;2019;2021;SAP SE;SAP SE;G06N3/088,G06F16/53,G06F18/214,G06F18/22,G06N3/045,G06N3/047,G06N3/08,G06T15/04,G06T17/20,G06T19/20,G06V10/764,G06V10/774,G06V10/776,G06V10/82,G06T2219/2016;Self-paced adversarial training for multimodal and 3D model few-shot learning;A method for generating synthetic data is provided. The method includes retrieving, from a database, a set of authentic base class images. The method further includes generating a three dimensional mesh of a base class. The method further includes retrieving, from the database, a set of textual descriptions. The method further includes retrieving a set of authentic novel class images. The method further includes generating, at a first neural network, a set of synthetic novel class images, the generating based on at least the three dimensional mesh, the set of textual descriptions, and/or the set of authentic novel class images. The method further includes training, based on at least the set of synthetic novel class images, a second neural network, the second neural network ranking the set of synthetic novel class images and outputting a set of highest ranked synthetic images from the set of synthetic novel class images.;6
385;10990852;2019;2021;SAMSUNG SDS CO., LTD.;SAMSUNG SDS CO., LTD.;G06V20/56,G06F18/2115,G06F18/2148,G06F18/2155,G06F18/254,G06V10/7747,G06V10/809,G06V2201/07;Method and apparatus for training model for object classification and detection;A method of training a model for object classification and detection includes training a first classification model including a shared feature extractor shared by classification models and a first classifier for outputting a result of an object in a first input image based on feature values of the first input image, training a second classification model including the shared feature extractor and a second classifier for outputting a result about authenticity of a second input image based on feature values of the second input image, and training a third classification model including the shared feature extractor and a third classifier for outputting a classification result about a rotation angle of a third input image on the basis of feature values of the third input image extracted by the shared feature extractor, using a third training image set including images rotated at one or more angles.;0
386;10991154;2019;2021;Ping An Technology (Shenzhen) Co., Ltd.;Ping An Technology (Shenzhen) Co., Ltd.;G06T17/00,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G06T19/20,G06T2219/2021;Method for generating model of sculpture of face with high meticulous, computing device, and non-transitory storage medium;A method for generating a model for facial sculpture based on a generative adversarial network (GAN) includes training a predetermined GAN based on a three-dimensional (3D) face dataset of multiple 3D face images to obtain a curvature map generation model and training a predetermined image translation model based on dataset of multiple image pairs to obtain a height map generation model. Target 3D face data is received, and the target 3D face data is inputted into the curvature map generation model to generate a target curvature map, and the target curvature map is inputted to the height map generation model to generate a target height map. The target height map is performed a 3D reconstruction to obtain a facial sculpture model corresponding to the target 3D face data. A computing device using the method is also provided.;3
387;10991369;2019;2021;;;G06F40/35,G10L15/22,G06F16/3329,G06F16/3344,G06F40/30,G10L15/26,G10L2015/223,G10L2015/225;Cognitive flow;"A system and method obtaining structured information from a conversation including receiving a first input from a user, determining a first set of slots filled based on the first input using natural language processing and a non-linear slot filling algorithm, determining first conversation based on the first set of slots filled, determining a first empty slot associated with the first conversation, prompting the user for a second input, the second input associated with the first empty slot, filling the first empty slot using natural language processing and the non-linear slot filling algorithm, determining that the slots associated with the first conversation are filled; and, responsive to determining that the slots associated with the first conversation are filled, initiating an action associated with the conversation.";0
388;10991373;2018;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L17/00,G06F3/167,G06F21/31,G06F21/52,G10L15/22,G10L17/22,G10L17/24,H04M1/724,H04M1/724631,G06F2221/2105,H04M2250/74;Voice command processing for locked devices;Techniques for processing voice commands from a locked device are described. A voice command received by a locked device is stored, a prompt requesting that the device be unlocked is generated, and the voice command is processed automatically after the device is unlocked. Thus, the system processes the voice command without the user repeating the voice command. In addition, the system may process certain voice commands even when the device is locked. For example, a whitelist filter compares an intent associated with the voice command to whitelisted intents from a whitelist database before the intent is dispatched to a speechlet, and intents included in the whitelist database are processed normally. Thus, the system performs certain voice commands while the device is locked, while other voice commands may be automatically processed after the device is unlocked without the user repeating the voice command.;0
389;10992604;2018;2021;Massachusetts Mutual Life Insurance Company;Massachusetts Mutual Life Insurance Company;H04L51/02,G06F40/58,H04L51/04,H04L65/1069,H04L67/01;Systems and methods for chat sessions involving multiple chatbots;Generally, this disclosure enables a chatbot to host a chat session with a user. In some implementations, when the chatbot is not able to or does not know an answer to a query from the user, then the chatbot can import another chatbot into the chat session such that the user is aware of such importation and such that the other chatbot can output the answer to the query into the chat session. In other implementations, when the chatbot is not able to or does not know the answer to the query from the user, then the chatbot can query another chatbot, in background, without notifying the user, and when the response is received from the other chatbot, the chatbot can output that response to the user seamlessly such that the user is not aware of such querying.;2
390;10999434;2020;2021;Bank of America Corporation;Bank of America Corporation;H04M3/5166,G06F3/167,G06N5/04,G06N20/00,G10L15/22,G10L15/26,H04M3/5183,G06N5/045,G10L15/1822,H04M7/0021,H04M2201/40,H04M2203/558;Artificial intelligence (âAIâ) integration with live chat;When a caller initiates an interaction with an interactive voice response (âIVRâ) system, the caller may be transferred to a live agent. Apparatus and methods are provided for integrating automated tools into the interaction after the caller been transferred to the agent. The agent may determine which AI responses are appropriate for the caller. AI may be leveraged to suggest responses for both caller and agent while they are interacting with each other. Such human-computer interaction may shorten response time of human agents and improve efficiency of IVR systems.;1
391;10999566;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F16/738,G06F16/7867,G06F40/166,G06N3/044,G06N3/045,G06N5/022,G06V10/454,G06V10/764,G06V10/82,G06V20/41,G06V20/46,G06V40/16,G06V40/172,G10L13/00,G10L17/00,H04N9/8715,G06V20/44,G10L25/30,G10L25/48;Automated generation and presentation of textual descriptions of video content;Systems, methods, and computer-readable media are disclosed for systems and methods for automated generation of textual descriptions of video content. Example methods may include determining, by one or more computer processors coupled to memory, a first segment of video content, the first segment including a first set of frames and first audio content, determining, using a first neural network, a first action that occurs in the first set of frames, and determining a first sound present in the first audio content. Some methods may include generating a vector representing the first action and the first sound, and generating, using a second neural network and the vector, a first textual description of the first segment, where the first textual description includes words that describe events of the first segment.;6
392;11003703;2018;2021;ZIGNAL LABS, INC.;ZIGNAL LABS, INC.;G06F16/345,G06F16/906,G06N3/044,G06N3/047,G06N3/08,G06N7/01;System and method for automatic summarization of content;Embodiments disclose a method for automatic summarization of content. The method includes accessing a plurality of stories from a plurality of data sources for a predefined time. Each story is associated with a media item. The method includes plotting the plurality of stories over the predefined time for determining one or more peaks and extracting a set of stories from the one or more peaks. The method includes detecting one or more themes from the set of stories using LDA algorithm. Each theme is associated with a group of stories. The method further includes determining at least one subset of stories for each theme from the group of stories representing the set of stories in the one or more peaks using RBM algorithm. The method includes generating a summarized content for each user based on an associated user profile and the at least one subset of stories.;2
393;11003865;2020;2021;GOOGLE LLC;GOOGLE LLC;G06F40/284,G06F18/2155,G06F40/49,G06F40/56,G06N3/045,G06N3/084,G06N5/022,G06N5/025;Retrieval-augmented language model pre-training and fine-tuning;Systems and methods for pre-training and fine-tuning of neural-network-based language models are disclosed in which a neural-network-based textual knowledge retriever is trained along with the language model. In some examples, the knowledge retriever obtains documents from an unlabeled pre-training corpus, generates its own training tasks, and learns to retrieve documents relevant to those tasks. In some examples, the knowledge retriever is further refined using supervised open-QA questions. The framework of the present technology provides models that can intelligently retrieve helpful information from a large unlabeled corpus, rather than requiring all potentially relevant information to be stored implicitly in the parameters of the neural network. This framework may thus reduce the storage space and complexity of the neural network, and also enable the model to more effectively handle new tasks that may be different than those on which it was pre-trained.;0
394;11011176;2020;2021;United Services Automobile Association (USAA);United Services Automobile Association (USAA);G10L13/027,G10L17/00,G06N3/08,G06N20/00,G10L13/033,G10L15/063,G10L17/04,H04M3/523,H04M2203/408;Voice synthesis for virtual agents;Techniques are described for generating a custom voice for a virtual agent. In one implementations, a method includes receiving information identifying a customer contacting a call center. The method includes selecting a voice for a virtual agent based on information about the customer. The method also includes assigning the voice to the virtual agent during communications with the customer.;0
395;11017123;2020;2021;Mores, Inc.;Mores, Inc.;G06F21/64,G06F21/6254,H04L9/0643,H04L9/0852,H04L9/3239,H04L9/50,H04L2209/42,H04L2209/46,H04L2209/56,H04L2209/88;System for anonymizing data for use in distributed ledger and quantum computing applications;A system and methods for anonymizing data for distribution on a distributed ledger arrangement is provided. The design includes receiving initial data at a computing device, the initial data relating to an initiating party, removing, at the computing device, personal identifying information from the initial data, thereby creating personal identifying information scrubbed data, anonymizing the personal identifying information scrubbed data on the computing device using DNA processing, thereby creating DNA processed scrubbed data, and providing the DNA processed scrubbed data from the computing device to the distributed ledger arrangement.;0
397;11024276;2020;2021;;;G10H1/0025,G06N20/00,G10H2210/061,G10H2210/076,G10H2210/105,G10H2210/111,G10H2210/115,G10H2210/125,G10H2210/131,G10H2210/151,G10H2210/571,G10H2220/036,G10H2250/211;Method of creating musical compositions and other symbolic sequences by artificial intelligence;"A method of creating AI-composed music having a style that reflects and/or augments the personal style of a user includes selecting and/or composing one or more seed compositions; applying variation and/or mash-up methods to the seed compositions to create training data; training an AI using the training data; and causing the AI to compose novel musical compositions. The variation methods can include methods described in the inventor's previous patents. A novel method of creating mash-ups is described herein. Variation and mash-up methods can further be applied to the A compositions. The AI compositions, and/or variations and/or mash-ups thereof, can be added to the training data for re-training of the AI. The disclosed mash-up method includes parsing the seed compositions into sequences of elements, which can be of equal length, beat-matching the seed compositions to make corresponding elements of equal beat length, and combining the elements to form a mash-up.";3
398;11030726;2019;2021;Shutterstock, Inc.;Shutterstock, Inc.;G06T5/73,G06F16/51,G06F16/532,G06F18/213,G06F18/214,G06F18/22,G06T3/4023,G06T5/60,G06V10/267,G06V10/764,G06V10/7715,G06V10/82,G06V20/00,G06T2207/20084;Image cropping with lossless resolution for generating enhanced image databases;A method including selecting, in a server, a first image portion from an image is provided. The method also includes identifying one or more known similar images associated with the first image portion, and determining a first score for enhancing the first image portion based on the known similar image(s). The method includes increasing a pixel resolution in the first image portion according to the scale to form an enhanced image portion. The method also includes identifying a synthetic value for the enhanced image portion and storing the enhanced image portion in a database when the synthetic value is below a tolerance value.;0
399;11037328;2019;2021;Lyft, Inc.;Lyft, Inc.;G06T7/564,G06T7/90,G06T7/74,G06T2207/10024,G06T2207/20021;Overhead view image generation;"The present invention relates to a method of generating an overhead view image of an area. More particularly, the present invention relates to a method of generating a contextual multi-image based overhead view image of an area using ground map data and field of view image data.Various embodiments of the present technology can include methods, systems and non-transitory computer readable media and computer programs configured to receive a plurality of images of the geographical area, determine a ground map of the geographical area, divide the ground map into a plurality of sampling points of the geographical area; and determine a color for each of the plurality of sampling points, wherein the color of each of the sampling points is determined by determining a correlation between the sampling points of the geographical area and color of the sampling points captured in at least one of the plurality of images.";2
400;11037554;2017;2021;Wells Fargo Bank, N.A.;Wells Fargo Bank, N.A.;G06F40/35,G10L15/22,G10L15/16,G10L15/1815,H04L51/02;Network of domain knowledge based conversational agents;The innovation disclosed and claimed herein, in one aspect thereof, comprises systems and methods of supporting a conversation with a user. The systems and methods of the innovation can include a bot consumer that receives a first dialogue input from a user, the first dialogue input requesting a response and having an intent. A super-agent selects a worker thread from a pool of worker threads, wherein the worker thread is delegated a task to perform on the first dialogue input. A registrar determines a conversational agent to respond to the user based on the intent. The conversational agent retrieves data to facilitate a response to the first dialogue input and renders a response to the user based on the retrieved data. A registrar custodian can perform create, read, update, and delete operations on the information in the registrar.;1
402;11043026;2018;2021;Pointivo Inc.;Pointivo Inc.;G06T17/20,G06F30/10,G06F30/13,G06N20/00,G06T17/05,G06T2207/20081;Systems and methods for processing 2D/3D data for structures of interest in a scene and wireframes generated therefrom;The inventions herein relate generally to improvements in the generation of wireframe renderings derived from 2D and/or 3D data that includes at least one structure of interest in a scene. Such wireframe renderings and similar formats can be used in, among other things, 2D/3D CAD drawings, designs, drafts, models, building information models, augmented reality or virtual reality, and the like. Measurements, dimensions, geometric information, and semantic information generated according to the inventive methods can be accurate in relation to the actual structures. The wireframe renderings can be generated from a combination of a plurality of 2D images and point clouds, processing of point clouds to generate virtual/synthetic views to be used with the point clouds, or from 2D image data that has been processed in a machine learning process to generate 3D data. In some aspects, the wireframe renderings are accurate in relation to the actual structure of interest, automatically generated, or both.;3
403;11049481;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10H1/0066,G06F16/116,G06F16/61,G06F16/638,G06N3/04,G06N3/045,G06N3/047,G06N3/08,G10H1/0008,G10H2210/036,G10H2220/005,G10H2220/126,G10H2240/081,G10H2240/141,G10H2250/311;Music generation system;Methods and apparatus for providing metrics for the quality, attributes, and relationships of music including AI-generated music. Music classification and visualization methods are described that involve transforming music files into graphical representations, generating a similarity matrix for the music files using structural similarity techniques, and generating visualizations of the relationships among the music files using multidimensional scaling techniques. Qualitative scoring methods for AI-generated music are described that involve classifying the AI-generated music using a multi-genre classifier, generating a similarity metric for the AI-generated music to other genres using structural similarity techniques and multidimensional scaling techniques, and generating a qualitative score for the music using confidence in the classification in combination with the similarity metric.;6
404;11049605;2020;2021;Cortery AB;Cortery AB;G16H20/70,A61B5/024,A61B5/165,A61B5/4088,A61B5/4836,A61B5/749,A61B10/0051,G06N3/02,G16H10/20,G16H20/10,G16H40/63,G16H40/67,G16H50/20,G06N3/045,G06N3/047,G06N3/08;Computer-implemented systems and methods for generating tailored medical recipes for mental health disorders;"Computer-implemented systems and methods for generating tailored medical recipes for mental health disorders. The systems include a processor, a memory, and a server. The memory is configured to register a user over a communication application through a registration module; receive demographic data through a demography module; receive voice data of the user through a voice module; receive bio-sample data of the user through a bio-sample module; receive face image data of the user through a camera module; receive mental health questionnaire data from the user through a questionnaire module; and transmit a final dataset through a data transmission module. The server is configured to process the final dataset received from the data transmission module by applying a machine learning module; generate the tailored medical recipes; and transmit the tailored medical recipes to one or more computing devices of the user over the network.";6
405;11050885;2020;2021;Bank of America Corporation;Bank of America Corporation;H04M3/5166,G06F16/252,G06F16/907,G06N5/02,G06N5/04,G06N20/00,G06Q30/016,G10L15/1807,G10L15/1815,H04M3/4936,H04M3/5183,H04M7/0021,H04M2201/40,H04M2203/355,H04M2203/558;Call interception heuristics;When a customer initiates an interaction with an interactive voice response (âIVRâ) system, the customer may need to be transferred to a live agent. Apparatus and methods may formulate timing information for integrating a live agent into an interaction controlled by an artificial intelligence (âAIâ) engine. The system may integrate machine generated responses into a customer interaction controlled by a live agent. The system may formulate timing information for intercepting the live agent with responses generated by the AI engine. The system may formulate the timing information using interactional analytics and preferences of a specific customer.;1
406;11055514;2018;2021;Snap Inc.;Snap Inc.;G06V10/764,G06V10/82,G06V40/161,G06V40/168,G06V40/172,G06V40/175;Image face manipulation;"Aspects of the present disclosure involve a system comprising a computer-readable storage medium storing a program and a method for synthesizing a realistic image with a new expression of a face in an input image by receiving an input image comprising a face having a first expression; obtaining a target expression for the face; and extracting a texture of the face and a shape of the face. The program and method for generating, based on the extracted texture of the face, a target texture corresponding to the obtained target expression using a first machine learning technique; generating, based on the extracted shape of the face, a target shape corresponding to the obtained target expression using a second machine learning technique; and combining the generated target texture and generated target shape into an output image comprising the face having a second expression corresponding to the obtained target expression.";2
407;11064113;2019;2021;GoPro, Inc.;GoPro, Inc.;H04N1/2145,G06V20/47,H04N23/60,H04N23/61,H04N23/64,H04N23/667,G06V10/507,G06V10/56;Image capture device with an automatic image capture capability;An image capture device may automatically capture images. An image sensor may generate visual content based on light that becomes incident thereon. A depiction of interest within the visual content may be identified, and one or more images may be generated to include one or more portions of the visual content including the depiction of interest.;0
408;11064252;2020;2021;;;H04N21/44218,G06Q10/0637,G06Q10/0639,G06Q10/10,H04N21/4316,H04N21/6587,H04N21/84,H04N21/854;Service, system, and computer-readable media for generating and distributing data- and insight-driven stories that are simultaneously playable like videos and explorable like dashboards;A service, system, and computer-readable media to create and distribute stories generated from data and insights, that are simultaneously played like videos and can be explored like dashboards and interactive charts. Such generative stories or casts are personalized to individuals and distributed using various push mechanisms including podcast-like and video cast-like distribution channels âstorycastsâ to deliver stories or casts to subscribed users.;6
409;11068661;2018;2021;Narrative Science LLC;Narrative Science LLC;G06F40/237,G06F40/30,G06F40/295,G06F40/35,G06F40/56,G06N5/022,G06N5/041,G06N20/00;Applied artificial intelligence technology for narrative generation based on smart attributes;Artificial intelligence (AI) technology can be used in combination with composable communication goal statements to facilitate a user's ability to quickly structure story outlines in a manner usable by an NLG narrative generation system without any need for the user to directly author computer code. This AI technology permits attribute structures within an ontology can include an explicit model for the subject attribute, regardless of whether that model is used to compute the value of the subject attribute itself. This explicit model can then be leveraged to support an investigation of drivers of the value for the subject attribute. Narrative analytics that perform driver analysis can then be used to support narrative generation for communication goals relating to explanations, predictions, recommendations, and the like.;3
410;11068952;2020;2021;;;G06Q30/0605,G06F16/26,G06F16/29,G06F16/9537,G06Q10/109,G06Q30/0205,G06T11/001,G06T11/60;System and computer-implemented method of identifying tattoo providers;A storage medium having software instructions configured to cause a processor to receive a series of user preferences, a user's requested dates of availability, transmit a request for tattoo service providers matching the series of user preferences, and receive a series of tattoo service providers matching the user preferences. The instructions are also configured to cause the processor to display an image of a map including the user's desired geographic location, and to display the series of tattoo service providers with a series of visual indicia. The series of visual indicia are overlaid on the image of the map based on geographic coordinates of the series of tattoo service providers.;0
411;11069353;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L15/005,G10L15/08,G10L15/142,G10L15/16,G10L25/78,G06F40/263,G10L15/02,G10L2015/088;Multilingual wakeword detection;A system and method performs multilingual wakeword detection by determining a language corresponding to the wakeword. A first wakeword-detection component, which may execute using a digital-signal processor, determines that audio data includes a representation of the wakeword and determines a language corresponding to the wakeword. A second, more accurate wakeword-detection component may then process the audio data using the language to confirm that it includes the representation of the wakeword. The audio data may then be sent to a remote system for further processing.;0
412;11070671;2020;2021;Zendesk, Inc.;Zendesk, Inc.;H04M3/5141,G06Q10/063114,G06Q10/10,H04L51/02,H04L51/046,H04L51/212,H04L51/214,H04M3/42382,H04M3/5175,G06Q30/016;Middleware pipeline that provides access to external servers to facilitate customer-support conversations;The disclosed embodiments relate to a system that facilitates accessing external servers to process messages during customer-support conversations in an online customer-support system. During operation, the system receives a message from a sender while the message is in transit between the sender and a receiver during a customer-support conversation, wherein the customer-support conversation is between a customer and a responsive entity, and wherein the customer-support conversation relates to an issue the customer has with a product or a service used by the customer. Next, the system feeds the message through a pipeline of processors, wherein each processor in the pipeline is configured to make a call to an associated external server to perform an operation on the message before forwarding the message to a subsequent stage of the pipeline. Finally, when the message finishes transiting the pipeline, the system forwards the message to the receiver.;0
413;11073975;2019;2021;Shutterstock, Inc.;Shutterstock, Inc.;G06F3/04847,G06F3/0482,G06F3/04845,G06F16/532,G06N3/045,G06N3/08,G06T11/60,G06T2200/24;Synthetic image generation in response to user creation of image;Various aspects of the subject technology relate to systems, methods, and machine-readable media for generating a user-created synthetic image. A method includes receiving input from a user onto a search field, the input relating to a desired image of the user, the search field including a user interface for specifying components of the desired image for display to the user. The method also includes identifying the components of the desired image in stock images, the stock images stored in a database. The method also includes generating the components of the desired image based on the stock images, the components located in user-specified locations of the search field. The method also includes generating the user-created synthetic image comprising the components located in user-specified locations in response to the input from the user relating to the desired image.;6
414;11074507;2020;2021;STRADVISION, INC.;STRADVISION, INC.;G06N3/088,G06N3/045,G06N3/084;Method for performing adjustable continual learning on deep neural network model by using selective deep generative replay module and device using the same;"A method of adjustable continual learning of a deep neural network model by using a selective deep generative replay module is provided. The method includes steps of: a learning device (a) (i) inputting training data from a total database and a sub-database into the selective deep generative replay module to generate first and second low-dimensional distribution features, (ii) inputting binary values, random parameters, and the second low-dimensional distribution features into a data generator to generate a third training data, and (iii) inputting a first training data into a solver to generate labeled training data; (b) inputting the training data, the low-dimensional distribution features, and the binary values into a discriminator to generate a first and a second training data scores, a first and a second feature distribution scores, and a third training data score; and (c) training the discriminator, the data generator, the distribution analyzer and the solver.";3
415;11077367;2020;2021;Mythical, Inc.;Mythical, Inc.;A63F13/42,G06F40/216,A63F13/215,A63F13/35,A63F13/63,A63F13/67,G06F40/289,G06F40/30,G06N3/045,G06N20/00,A63F13/56,A63F2300/1081,A63F2300/6045,G06N3/08;Systems and methods for using natural language processing (NLP) to control automated gameplay;"Systems and methods for using natural language processing (NLP) to control automated gameplay in one or more online games within an online gaming platform are disclosed. Exemplary implementations may: train a pretrained text-based system to generate sequences indicating one or more activities, wherein activities are associated with characteristics of user-controllable characters such that a particular user-controllable character having a particular characteristic may perform, within the online gaming platform, one or more activities from a particular set that is associated with the particular characteristic; receive user input describing one or more commands; generate, using the pretrained text-based system, a sequence indicating one or more activities based on the user input; transfer the generated sequence to the online gaming platform; effectuate automated execution of activities by the particular user-controllable character in accordance with the generated sequence.";5
416;11080607;2021;2021;RO5 INC.;RO5 INC.;G06V10/761,G06F16/951,G06F18/22,G06N3/044,G06N3/045,G06N3/08,G06N5/022,G16B15/00,G16B40/00,G16B45/00,G16B50/10,G06N5/02,G06V30/40;Data platform for automated pharmaceutical research using knowledge graph;A system and method for an automated pharmaceutical research data platform comprising a data curation platform which searches for and ingests a plurality of unstructured, heterogenous medical data sources, extracts relevant information from the ingested data sources, and creates a massive, custom-built and intricately related knowledge graph using the extracted data, and a data analysis engine which receives data queries from a user interface, conducts analyses in response to queries, and returns results based on the analyses. The system hosts a suite of modules and tools, integrated with the custom knowledge graph and accessible via the user interface, which may provide a plurality of functions such as statistical and graphical analysis, similarity based searching, and edge prediction among others.;1
417;11082369;2019;2021;Figure Eight Technologies, Inc.;Figure Eight Technologies, Inc.;G06F40/35,H04L51/02,G06F40/169,G06F40/30,G06N3/04,G06N20/00,G06F40/166,G06N3/006,G06N3/044;Domain-specific chatbot utterance collection;A set of utterances collected from a plurality of contributors is received. Semantically irrelevant utterances are removed from the set of utterances to obtain a processed set of utterances, including by applying a machine learning model to the set of utterances. An annotation user interface is provided to a plurality of human annotators to perform annotation on the processed set of utterances to obtain an annotated set of utterances. A curation user interface is provided to one or more domain experts to perform curation of the annotated set of utterances to obtain a curated set of utterances. The curated set of utterances is outputted as a training set for an automated dialogue agent.;0
418;11087095;2015;2021;STATS LLC;STATS LLC;G06F40/40,G06F40/56,G06F16/353,G06F40/103,G06F40/169,G06F40/18,G06F40/51;Platform for quality assurance in natural language generation frameworks;The present invention is a system and method for optimizing the narrative text generated by one or more narrative frameworks that utilize data input from one or more data sources to drive the creation of a narrative text output. Narrative text is generated in accordance with sets of data that provide the scope of text to be generated. A Quality Assurance module presents the narrative text output to a user that reviews both the condition and the logic evaluation associated with the scope, and the quality of the generated text. A log of Quality Assurance items is created upon review of the generated text. These items are then later resolved by locating them in a narrative text generation data structure to resolve the identified issues.;1
419;11087528;2020;2021;Apple Inc.;Apple Inc.;G06T15/205,G06T17/00,G06F30/13,G06T15/04,G06T19/003,G06T19/20,G06F2111/18,G06T2200/24,G06T2210/04,G06T2219/2021;3D object generation;In one implementation, a method of generating a three-dimensional object is performed by a device including one or more processors, non-transitory memory, one or more input devices, and a display. The method includes detecting a first set of one or more user inputs indicative of a two-dimensional profile and detecting a second set of one or more user inputs indicative of a two-dimensional floor plan. The method includes generating, based on the two-dimensional profile and the two-dimensional floor plan, a three-dimensional object and displaying the three-dimensional object.;4
420;11087877;2020;2021;Omniscient Neurotechnology Pty Limited;Omniscient Neurotechnology Pty Limited;G16H40/63,G16H50/20,G16H30/20;Identifying anomalous brain data;"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for determining anomalous brain data. One of the methods includes obtaining brain data characterizing brain activity of a patient; for each of a plurality of pairs of parcellations comprising a first parcellation and a second parcellation, processing the brain data to generate a correlation between the brain activity of the first and second parcellations; obtaining second connectivity data that characterizes, for each of the plurality of pairs of parcellations, a normal range of correlations between the brain activity of the first and second parcellations; identifying one or more of the plurality of pairs of parcellations for which the correlation between brain activity of the first and second parcellations is outside of the corresponding normal range of correlations; and providing data characterizing the one or more identified pairs of parcellations for display to a user on a graphical interface.";0
421;11093718;2021;2021;Rammer Technologies, Inc.;Rammer Technologies, Inc.;G06F40/35,G06F40/289;Determining conversational structure from speech;Embodiments are directed to organizing conversations. Words may be provided from a conversation stream. Each word may be mapped to a graph model based on characteristics of each word. The graph model may be partitioned based on one or more attributes of a nodes and edges included in the graph model such that nodes associated with relationship strength that exceeds a threshold value may be grouped together. Sentence models may be generated based on sentences included in the conversation stream. Combined models may be generated based on the sentence models and the graph such that each sentence model may be associated with one or more partitions of the graph model. A conversation digest may be generated based on the combined model such that the conversation digest identifies one or more dominant portions of the conversation that include key subject matter.;0
422;11094007;2018;2021;STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY;STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY;G06Q40/03,G06Q40/02,G06Q50/16,H04L9/0637,H04L9/3239,H04L9/50,H04L2209/56;Continuously updating mortgage ready data;A system and computer-implemented method of continuously updating information about one or more of a customer approved for a mortgage and a real estate property identified as mortgage ready. The method includes monitoring information accessed from a memory storage location corresponding to a customer identification number, the information used to determine the customer is approved for a mortgage, and receiving new information about the customer, the new information used to determine the customer is approved for a mortgage. The method also includes updating, at a memory coupled to the one or more processors, the memory storage location to include the new information. The method still further includes recalculating the amount in which the customer is approved for a mortgage based upon the new information received.;0
423;11094134;2020;2021;Booz Allen Hamilton Inc.;Booz Allen Hamilton Inc.;G06T15/20,G06T19/006,G06T11/00,G06T15/503,G06T15/506;System and method for generating synthetic data;"Exemplary systems and methods are directed to generating synthetic data for computer vision. A processing device generates a synthetic three-dimensional (3D) image of an object. A background image is selected, and a composite image is generated by combining the 3D image of the object and the background image. The processing device simulates: reflection or emission of at least one type of radiant energy from the surface of the object and/or the background according to a set of parameters associated with at least one of the object and the background image; and a reflectance or emittance measurement of the at least one type of radiant energy from the surface of the object by a sensor device configured for detecting the at least one type of radiant energy. The processing device generates a plurality of two-dimensional (2D) simulated images of different perspectives of the object based on simulation data.";3
424;11095579;2020;2021;YSEOP SA;YSEOP SA;H04L51/02,G06F40/40,H04L51/216,G06F40/35;Chatbot with progressive summary generation;Methods and apparatus for summarizing a chatbot interaction with a user are provided. The method comprises using at least one computer hardware processor to perform generating an initial summary based, at least in part, on user data, receiving first input from the user during the chatbot interaction, processing the first input with a natural language processing engine, updating the initial summary based, at least in part, on an output of the processing by the natural language processing engine to generate an updated summary, wherein the updating is performed prior to completion of the chatbot interaction, and outputting a final summary of the chatbot interaction, wherein the final summary of the chatbot interaction is based, at least in part, on the updated summary.;2
425;11106903;2018;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06V20/64,G06F18/2132,G06F18/217,G06V10/143,G06V10/56,G06V10/82,G06V40/103,G06V10/58;Object detection in image data;Techniques are generally described for object detection in image data. A first frame of image data associated with a first domain is received by a detector executing on at least one computing device. The detector generates a first feature data in the first domain. The first feature data is transformed from the first domain into a second feature data in a second domain. The detector may be effective to detect objects in the second domain. A location of an object in the first frame of image data is determined based at least in part on the second feature data.;0
426;11107228;2020;2021;Ford Global Technologies, LLC;Ford Global Technologies, LLC;G06T7/50,G06T7/10,G06T11/00,G06V10/774,G06V10/82,G06T2207/10024,G06T2207/10028,G06T2207/20084,G06T2207/30244;Realistic image perspective transformation using neural networks;"The present disclosure discloses a system and a method. In example implementations, the system and the method can include receiving an image having a first perspective; generating, via a deep neural network, a depth map corresponding to the image having the first perspective; generating, via the deep neural network, a point cloud representation based on the depth map; projecting the point cloud representation onto a point cloud representation corresponding to an image having a second perspective; generating a depth map corresponding to the image having the second perspective; and generating a synthetic image having the second perspective based on the depth map corresponding to the image having the second perspective and a semantic segmentation map corresponding to the image having the first perspective, wherein the second perspective is different from the first perspective.";3
427;11107557;2018;2021;;;G16C20/30,G16C20/80,G16C10/00;Force field based molecular structure and conformer generation;Systems and methods for molecular structure generation and conformer elaboration, in which natural physical molecular movements can be combined with a molecular force field that constrains those movements, to rapidly produce conformational variants with relatively low energy. Construction and energy minimization of initial and subsequent 3D molecular models using force field parameters, are combined with alteration of the molecular model using biophysical transformations, to generate one or more conformations thereof. The biophysical transformations each include natural physical movements of parts of the molecule, such as rotation of atoms or bonds about a selected axis in the 3D molecular model. The selected axis can define ring components, selected bonds within a macrocyclic ring, selected bonds joining substituents or other portions of the molecule, or axes or lines defining one or more geometric features of the molecular model. Once altered using biophysical transformations, the 3D molecular model can also have energy minimization performed with respect to a molecular force field model. A subset of the generated conformers can be collected, compressed from time to time, and selected for output.;0
428;11107591;2020;2021;ROM Technologies, Inc.;ROM Technologies, Inc.;G06F40/58,G16H80/00,G06F40/205,G06N3/02,G06N20/00,G16H10/60,G16H20/10,G16H20/30,G16H20/60,G16H20/70,G16H40/67,G16H50/20,G06N3/08;Method and system for describing and recommending optimal treatment plans in adaptive telemedical or other contexts;A method is disclosed for providing, by an artificial intelligence engine, an optimal treatment plan to use with a treatment apparatus. The method includes receiving, from a data source, clinical information pertaining to results of using the treatment apparatus to perform particular treatment plans for people having certain characteristics. The clinical information has a first data format. The method also includes translating a portion of the clinical information from the first data format to a medical description language used by the artificial intelligence engine, determining, based on the portion of the clinical information described by the medical description language and a plurality of characteristics pertaining to a patient, the optimal treatment plan for the patient to follow using the treatment apparatus to achieve a desired result, and providing the optimal treatment plan to be presented on a computing device of a medical professional.;2
429;11109099;2020;2021;Disney Enterprises, Inc.;Disney Enterprises, Inc.;H04N21/44218,H04N21/458,H04N21/4665,H04N21/6373,H04N21/64792;Techniques for streaming a media title based on user interactions with an internet of things device;In various embodiments, an interactive streaming application plays back a media title via a client device. In operation, the interactive streaming application causes the client device to playback a first chunk of the media title. While the client device plays back the first chunk, the interactive streaming application determines a movement of an internet of things (âIoTâ) device that is controlled by the user. The interactive streaming application performs reinforcement-learning operation(s) based on the first chunk and the movement to determine a second chunk of the media title to playback. The interactive streaming application then causes the client device to playback the second chunk of the media title. Advantageously, the interactive streaming application can automatically personalize the playback of the media title for the user based, at least in part, on movements of the IoT device.;0
430;11113578;2020;2021;ADOBE INC.;ADOBE INC.;G06V10/82,G06F18/213,G06F18/214,G06F18/2411,G06N3/006,G06N3/045,G06N3/08,G06V10/7715,G06N3/044;Learned model-based image rendering;A non-photorealistic image rendering system and related techniques are described herein that train and implement machine learning models to reproduce digital images in accordance with various painting styles and constraints. The image rendering system can include a machine learning system that utilizes actor-critic based reinforcement learning techniques to train painting agents (e.g., models that include one or more neural networks) how to transform images into various artistic styles with minimal loss between the original images and the transformed images. The image rendering system can generate constrained painting agents, which correspond to painting agents that are further trained to reproduce images in accordance with one or more constraints. The constraints may include limitations of the color, width, size, and/or position of brushstrokes within reproduced images. These constrained painting agents may provide users with robust, flexible, and customizable non-photorealistic painting systems.;2
431;11113771;2015;2021;INTUIT INC.;INTUIT INC.;G06Q40/123,G06F16/2246,G06F16/9024;Systems, methods and articles for generating sub-graphs of a tax calculation graph of a tax preparation system;Systems, methods and articles of manufacture for generating a sub-graph of a tax calculation graph usable by a tax calculation engine to perform tax calculation operations. The system includes a computing device, a data store in communication with the computing device and a tax preparation software application executable by the computing device. The system comprises a tax calculation graph and a sub-graph engine which executes on the computing device. The sub-graph engine is configured to generate a sub-graph of the tax calculation graph based upon a target node and one or more user enterable nodes selected from the nodes of the tax calculation graph. The sub-graph engine analyzes the tax calculation graph in view of the target node and user enterable nodes and generates a sub-graph which excludes all nodes which are not necessary to calculate the target node.;1
432;11113862;2020;2021;Sony Interactive Entertainment Inc.;Sony Interactive Entertainment Inc.;G06T13/80,A63F13/26,A63F13/30,A63F13/355,A63F13/55,A63F13/58,G06T13/40,A63F2300/5553,A63F2300/6607;Simulating motion of computer simulation characters to account for simulated injuries to the characters using current motion model;An injury to a computer game character is simulated by changing the length or weight of an injured body part, or by adding additional links to the skeleton of the character to make the character harder to control. The facial expression of the character also can change. A generative adversarial network (GAN) may be used to learn motion under circumstances of injury to better mimic motion of an injured human as may be derived from, e.g., motion capture (MOCAP) video.;0
433;11115530;2020;2021;Bank of America Corporation;Bank of America Corporation;H04M3/4936,H04M3/5166,G06F40/40,H04M3/42382,H04M3/4933,G06F40/30,H04M3/58;Integration of human agent and automated tools for interactive voice response (IVR) systems;When a caller initiates a conversation with an interactive voice response (âIVRâ) system, the caller may be transferred to a live agent. Apparatus and methods are provided for integrating automated tools and artificial intelligence (âAIâ) into the interaction with the IVR system. The automated tools and AI may track the conversation to decipher when to transfer the caller to the agent. The agent may determine which machine generated responses are appropriate for the caller. AI may be leveraged to suggest responses for both caller and agent while they are interacting with each other. The agent may transfer back the caller to the IVR system along with the appropriate machine generated response to maintain efficiency and shorten time of human agent interaction.;1
434;11120812;2020;2021;CRESTA INTELLIGENCE INC.;CRESTA INTELLIGENCE INC.;G10L21/007,G10L21/013,G06N20/00,G06Q30/01,G10L15/063,G10L15/22,G10L25/51,H04M3/5133,G06N3/084,G06N5/047,G10L25/48,G10L25/63,G10L25/90,G10L2015/228,G10L2021/0135,H04M2203/2061;Application of machine learning techniques to select voice transformations;Techniques for monitoring a conversation in real-time to detect attributes of a conversation, identifying a desired outcome of the conversation, and identifying voice modulations that may be applied to the agent's voice to help accomplish the desired outcome are disclosed. The system may identify voice modulations by comparing a current conversation to one or more prior conversations having desired outcomes similar to that of the current conversation. A trained machine learning model may select and apply voice modulations associated with accomplishing a desired outcome.;0
435;11120839;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G11B27/102,G06F18/23,G06V10/762,G06V20/49,G06V40/161,G10L15/26,G10L25/57,G11B27/28,G11B27/32,G10L25/78,G10L25/81;Segmenting and classifying video content using conversation;Disclosed are various embodiments for segmenting and classifying video content using conversation. In one embodiment, a plurality of segments of a video content item are generated by analyzing audio accompanying the video content item. A subset of the plurality of segments that correspond to conversation segments are selected. Individual segments of the subset of the plurality of segments are processed to determine whether a classification applies to the individual segments. A list of segments of the video content item to which the classification applies is generated.;0
436;11121987;2020;2021;Octane AI, Inc.;Octane AI, Inc.;H04L51/02,H04L51/216;Conversational support for user journey;Interacting with a user of an electronic device includes presenting information to the user on a screen of the electronic device, a first chatbot that corresponds to the information presented to the user engaging in a conversation with the user based on an agenda that is related to the information presented to the user, the first chatbot recommending an action to the user based at least in part on the conversation and on the agenda, and updating the agenda and presenting new information on the electronic device in response to an action performed by the user. The agenda may be customized for the user based on available information about the user. Information about the user may include information about user behavior, previous actions performed by the user, previous user conversations with chatbots, and/or user personal data. The first chatbot may answer questions by the user about the information.;1
437;11122165;2020;2021;Verizon Patent and Licensing Inc.;Verizon Patent and Licensing Inc.;H04M3/5191,H04L51/02,H04L51/216,H04M3/5133,H04M3/5175,H04M7/0045,H04M3/2281,H04M2201/14,H04M2203/403,H04M2203/551;Systems and methods for customer service agent-guided chat session digital assistant;"A method, device, and computer-readable medium provide for receiving, via a chatbot access channel, a chat message from a user device associated with a customer chat session; determining that the chat message includes a customer intent that corresponds to a chat flow for the customer chat session; generating one or more suggested response messages based on the chat message, wherein at least one of the one or more suggested response messages includes a previously stored chat message response corresponding to the customer intent and approved by a service agent; presenting, via a display, a transcript of a messaging sequence for the customer chat session concurrently with a user interface that enables the service agent to perform an action with respect to the one or more suggested response messages; and sending, via the chatbot access channel, a selected one of the one or more suggested response messages to the user device.";2
438;11122238;2018;2021;Twitter, Inc.;Twitter, Inc.;G06T3/4053,H04N7/0137,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G06T3/4007,H04N7/0127;Frame interpolation with multi-scale deep loss functions and generative adversarial networks;A method includes selecting two or more frames from a plurality of frames of a video, downscaling the two or more frames, estimating a flow data based on an optical flow associated with the downscaled two or more frames, upscaling the flow data, generating a refined flow data based on the upscaled flow data and the downscaled two or more frames, upscaling the refined flow data, and synthesizing an image based on the upscaled refined flow data and the two or more frames.;0
439;11126826;2020;2021;shallow.AI Inc.;shallow.AI Inc.;G06V40/165,G06F18/21,G06N3/045,G06N3/047,G06N3/08,G06V10/454,G06V10/764,G06V10/82,G06V40/171,G06V40/174,G06N5/022;Machine learning system and method for recognizing facial images;A machine learning system extracts features from images. A system and method predicts appearance type, attractiveness, gender, or ethnicity from a facial image. In a machine learning approach, predictor training is performed by using known data sets containing the classification information for a given facial image into the desired categories. When a new image is presented, the system applies this info to the new image. The training can be performed continuously while the system is being presented with new images and corrective results in a feedback loop. In another machine learning approach, a generative adversarial network is used to predict the desired categories based on the given facial image.;0
440;11127225;2020;2021;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06T17/00,G06T19/20,G06N3/045,G06N3/088,G06T15/04,G06T19/006,G06T2200/04,G06T2219/2008;Fitting 3D models of composite objects;"A method of fitting a three dimensional (3D) model to input data is described. Input data comprises a 3D scan and associated appearance information. The 3D scan depicts a composite object having elements from at least two classes. A texture model is available which, given an input vector, computes, for each of the classes, a texture and a mask. A joint optimization is computed to find values of the input vector and values of parameters of the 3D model, where the optimization enforces that the 3D model, instantiated by the values of the parameters, gives a simulated texture which agrees with the input data in a region specified by the mask associated with the 3D model; such that the 3D model is fitted to the input data.";0
441;11127488;2021;2021;Accenture Global Solutions Limited;Accenture Global Solutions Limited;G16C20/50,G06N20/00,G16C20/70,G16H20/10,G16H70/40;Machine learning systems for automated pharmaceutical molecule screening and scoring;Aspects of the present disclosure provide systems, methods, and computer-readable storage media that leverage artificial intelligence and machine learning to screen candidate pharmaceutical molecules or compounds for pharmaceutical uses (e.g., treating diseases or conditions). The screening may be based on structural similarity between the candidate pharmaceutical molecules and various pharmaceutical targets (e.g., proteins, nucleic acids, etc.). In aspects, one or more machine learning (ML) models may be trained to assign a candidate pharmaceutical molecule to one of multiple clusters based on chemical/physical properties of the candidate pharmaceutical molecule and chemical/physical properties of pharmaceutical targets associated with the clusters. The pharmaceutical targets associated with the cluster may be scored based on comparisons between the candidate pharmaceutical molecule and the pharmaceutical target, and a subset of the pharmaceutical targets may be identified based on the scores. In some implementations, the subset may be ranked using conjoint analysis and machine learning.;0
442;11132598;2021;2021;NEURAVILLE, LLC;NEURAVILLE, LLC;G06N3/045,B25J9/163,G06N3/049,G06N3/08,G06N3/082,G06N3/088;System and method for humanoid robot control and cognitive self-improvement without programming;"A method for automatically generating a neural network architecture includes: loading a first computer-readable representation of a first neural network architecture including first genes representing parameters of the first neural network architecture; generating a first neural network including neurons in accordance with the first genes; deploying the first neural network in a robotic controller; training the first neural network by supplying inputs to an input processing unit connected to the first neural network and receiving outputs from an output processing unit connected to the first neural network, the training including updating synaptic weights of connections between the neurons based on responses to the inputs supplied to the input processing unit; evaluating a performance of the first neural network architecture; and generating, by the computer system, an updated computer-readable representation of an updated neural network architecture based on the evaluation of the performance the first neural network.";0
443;11138249;2018;2021;REALPAGE, INC.;REALPAGE, INC.;G06F16/3347,G06F16/90332,G06F16/35,G06F16/951,G06F16/9537,G06F40/30,G06N5/01,G06N5/02,G06N5/025,G06N20/00;Systems and methods for the creation, update and use of concept networks to select destinations in artificial intelligence systems;Systems and methods for concept based searching or recommendation are disclosed. More particularly, embodiments of a concept based approach to the search and analysis of data, including the creation, update or use of concept networks in searching and analyzing data are disclosed, including embodiments of the usage of such concept networks in artificial intelligence systems that are capable of utilizing concepts expressed by users to return or evaluate associated destinations.;0
444;11138781;2020;2021;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06T11/00,G06T13/40,G06N3/045,G06N3/08,G06T3/4053,G06T17/20,G10L13/027,G10L13/00;Creation of photorealistic 3D avatars using deep neural networks;"A computer-implemented method for the automatic generation of photorealistic 3D avatars. The method includes generating, using machine learning network, a plurality of 2D photorealistic facial images; generating, using a model agnostic meta-learner, a plurality of 2D photorealistic facial expression images based on the plurality of 2D photorealistic facial images; and generating a plurality of 3D photorealistic avatars based on the plurality of 2D photorealistic facial expression images.";5
445;11144846;2020;2021;Bank of America Corporation;Bank of America Corporation;G06N20/00,H04L41/16,H04L12/1813,H04L12/1818,H04L51/02,H04L51/04,H04L51/046,H04L51/216,H04L65/1069,G06N5/041,H04L41/5093;Complex human-computer interactions;"Methods for leveraging a plurality of machine-learning algorithms to improve a chat interaction are provided. The methods may include monitoring for initiation of a live chat session; alerting and assigning a chat responder to the live chat session; engaging one or more of a plurality of automated chat tools, the tools loaded with artificial intelligence (AI), in order to improve the response of the responder during the session; reviewing and retrieving, using the AI, from a machine learning (ML) library in electronic communication with the AI, historical information; presenting, on a chat responder screen, selected actionable information generated based on the historical information, to the responder; integrating, based on pre-determined conditions, chat responses into the ML library; and integrating into the ML library, based on the same or other pre-determined conditions, chat comments. The chat comments are generated by a chat initiator.";2
446;11146598;2020;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04L65/1069,H04L51/02,H04L51/066,H04L51/18,H04L51/52;Establishing a bot-to-bot communication protocol;Systems and methods allow bots to collaborate using a bot-to-bot communication protocol. In some instances, a first bot may request to establish a bot-to-bot channel with a second bot. The first bot may be authorized to issue the request for establishing the bot-to-bot channel. To communicate with one another, the first bot may have a first application programming interface (API) that is configured to communicate over the bot-to-bot channel with the second bot in natural language Additionally, the second bot may have a second API configured to communicate over the bot-to-bot channel with the first bot in natural language. An invitation to be sent to the second bot associated with joining the bot-to-bot channel. If accepted, the first bot and the second bot may exchange messages over the bot-to-bot channel.;0
447;11152031;2021;2021;CLIPr Co.;CLIPr Co.;G11B27/031,G06F3/0482,G06F16/738,G06V20/47,G06V20/49,G11B27/005,G11B27/06,G11B27/10;System and method to compress a time frame of one or more videos;"System and method to compress multiple videos are provided. the system includes a video retrieve module configured to retrieve videos from a video library database; a video selection module configured to select videos by a user upon viewing the plurality of videos; a video clipping module configured to clip each of the multiple videos into a multiple video clips based on features with assigned importance levels, and to select one or more video clips by the user upon viewing the multiple video clips; a video summarization module configured to create a video summary, a video editing module configured to operate multiple video clips from the video summary on receiving a feedback from the user, to generate a modified video summary to compress the multiple videos. a video time calculator module configured to calculate total time eliminated; a video timer module configured to set a time frame enable the modified video summary to be played.";0
448;11152123;2021;2021;Omniscient Neurotechnology Pty Limited;Omniscient Neurotechnology Pty Limited;A61B5/7253,G16H50/70,A61B5/0044,A61B5/055,A61B5/4064,A61B5/4848,A61B5/7267,G01R33/4806,G01R33/56341,G06N3/045,G06N3/088,G16H20/30,G16H40/67,G16H50/20,G16H50/30,A61B5/0042,A61B5/14553,A61B5/372,A61B2505/09,A61B2576/026,G01R33/5608,G16H20/10,G16H20/40,G16H30/20,G16H30/40;Processing brain data using autoencoder neural networks;"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for processing brain data using autoencoder neural networks. One of the methods includes obtaining brain data captured by one or more sensors characterizing brain activity of a patient; processing the brain data to generate modified brain data that characterizes a predicted local effect of a future treatment on the brain of the patient; processing the modified brain data using an autoencoder neural network to generate reconstructed brain data; and determining, using the reconstructed brain data, a predicted global effect of the future treatment on the brain of the patient.";3
449;11152785;2019;2021;X Development LLC;X Development LLC;G06Q50/06,G06N3/045,G06N3/047,G06N3/08,G06N3/084,H02J3/0073,H02J3/14,H02J2203/20,Y02B70/3225,Y04S20/222;Power grid assets prediction using generative adversarial networks;"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for using a neural network to predict locations of feeders in an electrical power grid. One of the methods includes training a generative adversarial network comprising a generator and a discriminator; and generating, by the generator, from input images, output images with feeder metadata that represents predicted locations of feeder assets, including receiving by the generator a first input image and generating by the generator a corresponding first output image with first feeder data that identifies one or more feeder assets and their respective locations, wherein the one or more feeder assets had not been identified in any input to the generator.";3
450;11153566;2021;2021;TSINGHUA UNIVERSITY;TSINGHUA UNIVERSITY;H04N19/124,G06N3/045,G06N3/047,G06N3/084,G06N3/088,H04N19/184,G06N3/048;Variable bit rate generative compression method based on adversarial learning;"A variable bit rate generative compression method based on adversarial learning is provided. According to the method, a variance of a feature map of an encoding-decoding fill convolutional network is quantized to train a single generative model to perform variable bit rate compression. The method includes the following implementation steps of: constructing training and testing data sets through an image acquisition device; constructing a generative compression network based on an auto-encoder structure; according to a rate-distortion error calculation unit, alternately training a generative network; according to a target compression rate, calculating a mask threshold; based on a feature map channel redundancy index, calculating a mask; and performing lossless compression and decoding on the mask and the feature map. According to the invention, only a single model is trained, but compression results with different bit rates can be generated, and on a limit compression rate below 0.1 bpp.";0
451;11157339;2020;2021;UiPath, Inc.;UiPath, Inc.;G06F9/546,B25J9/1661,B25J9/1689,G06Q10/10;Automation of a process running in a first session via a robotic process automation robot running in a second session;Automation of a process running in a first session via robotic process automation (RPA) robot(s) running in a second session is disclosed. In some aspects, a form is displayed in a user session, but one or more attended RPA robots that retrieve and/or interact with data for an application in the first session run in one or more other sessions. In this manner, the operation of the RPA robot(s) may not prevent the user from using other applications or instances when the RPA robot(s) are running, but the data modifications made or facilitated by the RPA robot(s) may be visible to the user in the first session window.;0
452;11158207;2017;2021;Proofpoint, Inc.;Proofpoint, Inc.;G09B19/00,A63F13/80,G06F21/55,G06F21/552,G06F21/554,G06F21/56,G06F21/562,G06F21/563,G06F21/564,G06F21/565,G06F21/566,G06F21/567,G09B5/00,G09B5/065,G09B7/02,G09B19/0053,H04L63/1408,H04L63/1416,H04L63/1425,H04L63/1433,H04L63/1441,H04L63/145,H04L63/1458,H04L63/1466,H04L63/1475,H04L63/1483,H04L63/1491;Context-aware cybersecurity training systems, apparatuses, and methods;A system assesses the susceptibility of an electronic device user to a cybersecurity threat by sensing a user action with respect to the electronic device. The system maps the sensed data to a training needs model to determine whether the sensed data corresponds to a pattern associated with a threat scenario in the training needs model. When the system determines that the sensed data corresponds to a pattern associated with a threat scenario in the training needs model, identify a cybersecurity threat scenario for which the user is at risk, and use the training needs model to estimate susceptibility of the user to the cybersecurity threat scenario.;0
453;11165725;2020;2021;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L51/02,H04L51/04,H04L67/306,H04L67/535,H04L51/222;Messaging in a real-time chat discourse based on emotive cues;Aspects of the present invention disclose a method, computer program product, and system for generating messages in a real-time chat discourse. The method includes one or more processors identifying user profile data associated with a user engaged in a real-time chat discourse with a chatbot. The method further includes one or more processors analyzing the real-time chat discourse between the user and the chatbot. The method further includes one or more processors determining a baseline emotive level for the real-time chat discourse based on the user profile data and the analysis of the real-time chat discourse. The method further includes one or more processors determining real-time emotion information of messages of the user in the real-time chat discourse. The method further includes one or more processors determining whether the real-time emotion information deviates from the determined baseline emotive level.;0
454;11165735;2020;2021;Accenture Global Solutions Limited;Accenture Global Solutions Limited;H04L51/42,G06F40/30,G06N3/04,G06N3/045,G06N3/047,G06N3/088,G06Q10/107,H04L51/066;Email validation;Systems and methods for email validation are disclosed. The email validation includes transforming format of emails to a predefined format understandable the present system and application of text mining component on the transformed format. The email validation further includes obtaining details from a repository related to a historical pattern associated with an email validation requirement and a cognitive learning operation employed for the historical email validation to ascertain an outcome of the historical validation for similar emails. The email validation also includes predicting misdirection of the email and change in configuration of the email account based on the validation of the email.;0
455;11166035;2020;2021;Wangsu Science & Technology Co., Ltd.;Wangsu Science & Technology Co., Ltd.;H04N19/40,H04N19/115,H04N19/119,H04N19/167,H04N19/174,H04N19/176,H04N19/1883;Method and device for transcoding video;"The present disclosure discloses a method and device for transcoding a video, which belongs to the video processing technology. The method includes: acquiring a target frame image of a video to be transcoded, and generating a global feature map of the target frame image based on a feature extraction module of a semantic segmentation model; performing feature segmentation on the global feature map based on a feature segmentation module of the semantic segmentation model, and determining a multi-level ROI of the target frame image; and using different transcoding rates to transcode the multi-level ROI and other regions of the target frame image respectively.";2
456;11170038;2018;2021;Narrative Science LLC;Narrative Science LLC;G06F16/51,G06F16/904,G06F16/9024,G06F40/143,G06F40/157,G06F40/186,G06F40/56,G06T11/206;Applied artificial intelligence technology for using narrative analytics to automatically generate narratives from multiple visualizations;Narrative generation techniques can be used in connection with data visualization tools to automatically generate narratives that explain the information conveyed by a visualization of a data set. In example embodiments, new data structures and artificial intelligence (AI) logic can be used by narrative generation software to map different types of visualizations to different types of story configurations that will drive how narrative text is generated by the narrative generation software.;4
457;11171904;2020;2021;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L51/18,G06F3/0482,G06F3/04842,G06F3/0488,G06F3/04883,G06F3/04886,G06N3/02,G06N3/045,G06N3/047,G06N3/084,H04L51/046,H04L51/48,H04L63/08,H04L63/126,G06F21/316;Message authentication using generative adversarial networks;Authenticating a message by receiving a first message from a source, generating a touch activation matrix from the first message, the touch activation matrix comprising touchscreen data associated with the first message, generating a second message from the touch activation matrix, determining a first legitimacy state by comparing the first message and the second message, determining a second legitimacy state using a discriminator network, determining a categorization for the first message according to the first and second legitimacy states, and sending the categorization for the first message to the source.;0
458;11176443;2019;2021;Automation Anywhere, Inc.;Automation Anywhere, Inc.;G06V30/414,G06F18/2148,G06F18/217,G06F18/24,G06F18/25,G06N3/045,G06N3/08,G06N3/084,G06V10/82,G06V30/19147,G06V30/19173,G06N3/044,G06N20/00,G06V30/10;Application control and text detection from application screen images;Automation controls and associated text in images displayed by a computer application are automatically detected by way of region-based R-FCN and Faster R-CNN engines. Datasets comprising images containing application controls, where the application controls include images of application where width is greater than height, width is equal to height and height is greater than width are retrieved and each dataset is processed with the R-FCN and Faster R-CNN engines to generate a software robot configured to recognize corresponding application controls. Text is recognized by an optical character recognition system that employs a deep learning system trained to process a plurality of images to identify images representing text within each image and to convert the images representing text to textually encoded data. The deep learning system is trained with training data generated from a corpus of real-life text segments that are generated by a plurality of OCR modules.;0
459;11176930;2016;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/30,G10L15/22,G06F3/167,G06F40/40,G10L13/08,G10L15/02,G10L15/142,G10L15/183,G10L15/26,G06F40/279,G10L13/00,G10L2015/025,G10L2015/223;Storing audio commands for time-delayed execution;Systems including physical devices, such as button and switches, that receive audio when a user performs a specific interaction are described. The audio may correspond to a particular spoken command to be executed by a system in a time-delayed fashion. At a later time, when another interaction is performed with the physical device, the device may send the stored audio to a server for processing to determine a command associated with the audio. A device may store multiple audio data segments corresponding to multiple different commands, and each piece of audio data corresponding to a command may be associated with a specific physical operation of a device. If audio data is determined as corresponding to a multiple-input command, additional information needed to perform the multiple-input command may be audibly gathered from a user.;0
460;11183169;2019;2021;OBEN, INC.;OBEN, INC.;G10L13/047,G10L13/02,G10L13/033,G10L13/08,G10H2210/041,G10H2240/325,G10H2250/455;Enhanced virtual singers generation by incorporating singing dynamics to personalized text-to-speech-to-singing;A technique to enhance the quality of Text-to-Speech (TTS) based Singing Voice generation is disclosed. The present invention efficiently preserves the speaker identity and improves sound quality by incorporating speaker-independent natural singing information into TTS-based Speech-to-Singing (STS). The Template-based Text-to-Singing (TTTS) system merges qualities of a singing voice generated from a TTS system with qualities of a singing voice generated from an actual voice singing the song. The qualities are represented in terms of Mel-generalized cepstrum (MGC) coefficients. In particular, low-order MGC coefficients from the TTS-based singing voice with high-order MGC coefficients from the voice of an actual singer.;4
461;11194973;2019;2021;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/35,G06F40/289,G06N3/044,G06N3/045,G06N3/08,G06N20/00,G10L15/26,H04L51/02,G10L13/00,G10L15/1822;Dialog response generation;A system that can engage in a dialog with a user may select a system response to a user input based on how the system estimates a user may respond to a potential system response. Models may be trained to evaluate a potential system response in view of various available data including dialog history, entity data, etc. Each model may score the potential system response for various qualitative aspects such as whether the response is likely to be comprehensible, on-topic, interesting, likely to lead to the dialog continuing, etc. Such scores may be combined to other scores such as whether the potential response is coherent or engaging. The models may be trained using previous dialog/chatbot evaluation data. At runtime the scores may be used to select a system response to a user input as part of the dialog.;0
462;11196867;2021;2021;STEREO APP LIMITED;STEREO APP LIMITED;H04M3/566,H04W4/21,H04M3/567,H04M3/569,H04W4/16,H04M2203/205,H04M2203/655;Complex computing network for improving establishment and broadcasting of audio communication among mobile computing devices and for improving switching from listening mode to conversation mode on a mobile application;"Systems, methods, and computer program products are provided for improving establishment and broadcasting of audio communication among mobile computing devices and for improving switching from listening mode to conversation mode on a mobile application. For example, a method comprises: receiving a selection of a conversation mode option from the user interface of the mobile application on a first mobile device of a first user displaying the conversation mode option simultaneously with both a first visual representation of a second user involved in the first audio conversation, and the second visual representation of a third user involved in the first audio conversation; and in response to receiving selection of the conversation mode option, modifying a first image of the conversation mode option, and placing the first user in the first audio conversation with the second user and the third user, or in a second audio conversation with a fourth user.";0
463;11210062;2021;2021;Chief Chief Technologies Oy;Chief Chief Technologies Oy;G06F3/167,G06F3/0484,G06F40/166,G06F40/30,G10L15/1815,G10L15/22,H04R1/08,G10L2015/088,G10L2015/223,H04R2499/15;Methods, apparatuses and systems for modifying user-interface using voice commands;"A method for modifying user-interface using voice commands. Method includes receiving first and second voice commands of first type respectively at first and second moments of time for creating first and second user-interface elements including first and second information having at least one information attribute associated therewith; receiving third voice command of second type at third moment of time for initiating modification process for modifying first or second user-interface elements; and rendering modified user-interface element on user-interface. Modification process includes extracting, from third voice command, first or second information or information attribute associated therewith; identifying corresponding first or second user-interface element based on extracted information or information attribute; extracting, from third voice command, modifying-intent or modifying-intent attribute; and modifying identified first or second user-interface element based on modifying-intent or modifying-intent attribute. Disclosed also is apparatus and system for modifying user-interface using voice commands.";5
464;11210775;2020;2021;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06T5/70,G06T7/0002,G06N3/045,G06N3/047,G06N3/08,G06T5/50,G06T5/60,G06T7/269,G06T2207/10016,G06T2207/20081,G06T2207/20084,G06T2207/20182,G06T2207/20221;Gradient-embedded video anomaly detection;A sequence of frames of a video can be received. For a given frame in the sequence of frames, a gradient-embedded frame is generated corresponding to the given frame. The gradient-embedded frame incorporates motion information. The motion information can be represented as disturbance in the gradient-embedded frame. A plurality of such gradient-embedded frames can be generated corresponding to a plurality of the sequence of frames. Based on the plurality of gradient-embedded frames, a neural network such as a generative adversarial network is trained to learn to suppress the disturbance in the gradient-embedded frame and to generate a substitute frame. In inference stage, anomaly in a target video frame can be detected by comparing it to a corresponding substitute frame generated by the neural network.;1
465;11223580;2018;2022;Octane AI, Inc.;Octane AI, Inc.;G06F40/35,H04L51/02,G06F16/3329,G06F40/20,H04L51/222,H04L51/52;Optimized conversation routing for unified multi-platform chatbots;Providing a consistent conversational experience to a user having multiple platform devices includes assigning each of a plurality of platform specific artificial conversational entities to at least one of the multiple platform devices using a chatbot integration platform of a particular messaging platform running on each of the devices or available conversational, interaction and integration capabilities of a messaging platform running on each of the devices, determining preferences of the user regarding timing and use on the multiple platform devices, and initially selecting a conversational venue corresponding to one of the multiple platform devices and a corresponding one of the platform specific artificial conversational entities according to routing logic that uses the preferences of the user and other factors. The preferences of the user may be determined using contact information of the user and social media profiles of the user.;0
466;11227047;2018;2022;FireEye Security Holdings US LLC;FireEye Security Holdings US LLC;G06F21/552,G06F21/554,G06F21/56,G06N5/01,G06N20/00,G06N20/20,G06F2221/034,G06N3/045,G06N3/082,G06N20/10;System and method for improved end-to-end cybersecurity machine learning and deployment;The presently disclosed subject matter includes an apparatus that receives a dataset with values associated with different digital resources captured from a group of compute devices. The apparatus includes a feature extractor, to generate a set of feature vectors, each feature vector from the set of feature vectors associated with a set of data included in the received dataset. The apparatus uses the set of feature vectors to validate multiple machine learning models trained to determine whether a digital resource is associated with a cyberattack. The apparatus selects at least one active machine learning model and sets the remaining trained machine learning models to operate in an inactive mode. The active machine learning model generates a signal to alert a security administrator, blocks a digital resource from loading at a compute device, or executes other remedial action, upon a determination that the digital resource is associated with a cyberattack.;0
467;11232266;2020;2022;Verizon Patent and Licensing Inc.;Verizon Patent and Licensing Inc.;G06F40/35,G06F40/56,G06F16/313,G06F40/253,G06F40/279,G06F40/289,G06F40/30,G10L15/26,H04M3/5175,G06F16/36,H04M3/2218;Systems and methods for generating a summary of a multi-speaker conversation;"A system may separate a transcript of a conversation into a first section corresponding to a first speaker in the conversation, and a second section corresponding to a second speaker in the conversation. The system may process, based on delimiters and punctuation marks, the first section and the second section to form a first speaker transcript and a second speaker transcript; determine, based on one or more topic models, a first set of key terms associated with the first speaker transcript and a second set of key terms associated with the second speaker transcript. The system may generate, based on a common set of key terms associated with the first set and the second of key terms, a first transcript summary of the first speaker transcript and a second transcript summary of the second speaker transcript; and generate a summary based on the first and the second transcript summaries.";2
468;11232268;2017;2022;Narrative Science LLC;Narrative Science LLC;G06F40/56,G06F16/248,G06F40/205;Applied artificial intelligence technology for using narrative analytics to automatically generate narratives from line charts;Disclosed herein are example embodiments that describe how a narrative generation techniques can be used in connection with data visualization tools to automatically generate narratives that explain the information conveyed by a visualization of a data set. In example embodiments, new data structures and artificial intelligence (AI) logic can be used by narrative generation software to map different types of visualizations to different types of story configurations that will drive how narrative text is generated by the narrative generation software.;4
469;11232793;2021;2022;Chief Chief Technologies Oy;Chief Chief Technologies Oy;G10L15/1822,G10L15/22,G10L15/10,G10L15/1815;Methods, systems and voice managing servers for voice recognition to perform action;A method for voice recognition to perform an action. The method includes receiving a voice command, identifying a first action intended word from the voice command, assigning a confidence score to the first action intended word, altering the confidence score of the first action intended word in a temporal manner, based on confidence scores of second action intended words following the first action intended word in the voice command, identifying the action when the confidence scores of the first and second action intended words reach a pre-determined confidence score associated therewith, and performing the identified action. Disclosed also is a system for voice recognition to perform an action. The system includes one or more voice-controlled devices, and a voice managing server communicably coupled to the one or more voice-controlled devices. The voice managing server for voice recognition to perform an action using the aforementioned method.;0
470;11238090;2016;2022;Narrative Science LLC;Narrative Science LLC;G06F16/51,G06F40/30,G06F16/248,G06F16/9024,G06F40/56,G06T11/206;Applied artificial intelligence technology for using narrative analytics to automatically generate narratives from visualization data;Disclosed herein are example embodiments that describe how a narrative generation techniques can be used in connection with data visualization tools to automatically generate narratives that explain the information conveyed by a visualization of a data set. In example embodiments, new data structures and artificial intelligence (AI) logic can be used by narrative generation software to map different types of visualizations to different types of story configurations that will drive how narrative text is generated by the narrative generation software.;4
471;11238197;2021;2022;;;G06F30/23,G06T19/00,G06F17/18,G06N20/20,G06T7/0012,G06T7/10,G06T15/08,G06T17/205,G16H20/40,G16H30/40,G16H50/20,G16H50/50,G06N3/045,G06N3/088,G06T2207/20081,G06T2210/41;Generating a 3D dataset containing a simulated surgical device;This patent includes a method and apparatus for the generation of a simulated, realistic medical device, which can be inserted into a 3D radiological dataset from CT, MRI, PET, SPECT or DTS examinations. This simulated dataset can be segmented, filtered, manipulated, used with artificial intelligence algorithms and viewed in conjunction with head display units and geo-registered tools.;1
472;11240181;2020;2022;Jiseki Health, Inc.;Jiseki Health, Inc.;H04L51/02,G06N3/006,G06N5/02,G06N5/027,G06N5/041,G06N20/00,G16H10/20,G16H10/60,G16H40/20,G16H40/67,G16H50/20,G16H50/30,G16H80/00,H04L41/16,H04L67/306,H04L67/51,G06N3/043,G06N3/044,G06N3/045,G06N3/049,G06N3/084,G06N3/088,G06N3/126,G06N7/01,G06N7/023,G06N20/10,H04L51/18,H04L51/222;Artificial intelligence assisted service provisioning and modification for delivering message-based services;"Systems and methods involving AI-assisted service provisioning and modification for delivering message-based services. One exemplary method includes: receiving an input sequence from a user in relation to a request for a service, the input sequence including one or more inputs; processing the input sequence to determine a service type; associating a workflow with the request based at least in part on the service type and a profile of the user, the workflow including a set of one or more steps, a step of the set of one or more steps corresponding to a set of attributes including at least one of: a communication mode, a communication type, or a communication priority, the workflow being performed by at least one of: a chatbot, an AI assistant, or a service professional; modifying the workflow based on a new input sequent from the user using a workflow engine comprising at least one of: an AI model, a machine learning model, and a ruleset; and interacting with the user based at least in part on the workflow to deliver the service.";0
473;11250635;2020;2022;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06T19/006,G06T7/75,G06T19/20;Automated provisioning of three-dimensional (3D) printable images;"Methods, computer program products, and/or systems are provided that perform the following operations: obtaining image data, wherein the image data includes a plurality of images; generating visualizations for one or more of the plurality of images included in the image data; obtaining a selection of one of the plurality of images for which a visualization was generated; generating three-dimensional (3D) model data based on the selected one of the plurality of images; and providing the 3D model data of the selected one of the plurality of images for generation of a 3D printable object.";4
474;11252113;2021;2022;Drift.com, Inc.;Drift.com, Inc.;H04L51/02,G06F3/167,G06F40/35,G06N5/01,G06N7/01,G06N20/00,G06Q10/02,G06Q30/016,G10L15/22,G10L15/28,G06N3/006,G06N5/041;Proactive and reactive directing of conversational bot-human interactions;Method and system to control a conversational bot uses a directed acyclic graph to specify a desired conversation flow. A graph node has synthetic conversation transcripts annotated with events, wherein an event in a synthetic conversation transcript has preconfigured event expressions that represent ways in which dialogue at the node can unfold. During an on-going conversation with an actor, the system provides a data model uniquely associated with the conversation and that specifies a linear sequence of observations. The data model includes events representing semantically-related conversation fragments located in annotated historical conversation transcripts. In response to receipt of an input in association with a current graph node, the system determines whether the input extends an event in the synthetic conversation transcript associated with the node. If so, a response that continues a current conversation flow in the graph is provided. If not, a response that interrupts a current conversation flow in the graph is provided.;1
475;11256402;2020;2022;FACEBOOK, INC.;FACEBOOK, INC.;G06F3/04845,G06F3/0482,G06T11/60,G06T2200/24;Systems and methods for generating and broadcasting digital trails of visual media;A computer-implemented method for generating and broadcasting digital trails of visual media may include (i) receiving user input initiating a digital trail, the user input including a selection of a trail mode from a list of candidate modes, (ii) creating a digital trail container configured to contain a series of thematically related visual media works generated by users invited to contribute to the digital trail, (iii) providing, to one or more users, a creation prompt that corresponds to the trail mode, (iv) adding, to the digital trail container, at least one visual media work received in response to providing the creation prompt, and (v) displaying the resulting digital trail by sequentially presenting each visual media work added to the digital trail container in the order added. Various other methods, systems, and computer-readable media are also disclosed.;4
476;11256995;2021;2022;RO5 INC.;RO5 INC.;G06N5/022,G16B15/30,G06F16/951,G06F18/22,G06F18/2413,G06N3/047,G06N3/08,G06N3/084,G16B40/20,G06N3/045,G06N5/02;System and method for prediction of protein-ligand bioactivity using point-cloud machine learning;A system and method that predicts whether a given protein-ligand pair is active or inactive, the ground-truth protein-ligand complex crystalline-structure similarity, and an associated bioactivity value. The system and method further produce 3-D visualizations of previously unknown protein-ligand pairs that show directly the importance assigned to protein-ligand interactions, the positive/negative-ness of the saliencies, and magnitude. Furthermore, the system and method make enhancements in the art by accurately predicting protein-ligand pair bioactivity from decoupled models, removing the need for docking simulations, as well as restricting attention of the machine learning between protein and ligand atoms only.;0
477;11263407;2021;2022;Rammer Technologies, Inc.;Rammer Technologies, Inc.;G06F40/35,G06F3/0481,G06F3/04842,G06F40/166,G06F40/253,G06F40/268,G06F40/279,G06F40/289,G06N5/04,G06N20/00,G06F40/211,G06F40/284,G10L15/1815,G10L15/1822,G10L15/22,G10L15/30;Determining topics and action items from conversations;Embodiments are directed to organizing conversation information. Two or more machine learning (ML) models and a plurality of sentences provided from a conversation may be employed to generate insight scores for each sentence such that each insight score correlates to a probability that its sentence includes one or more of an action or a question. In response to one or more sentences having insight scores that exceed a threshold value an information score and a definiteness score may be determined for the one or more sentences. And one or more insights associated with the conversation may be generated based on the one or more sentences. A report may be generated that associates the one or more insights with one or more portions of the conversation that include the one or more sentences that are associated with the insights.;2
478;11263534;2021;2022;RO5 INC.;RO5 INC.;G06N5/022,G16C20/50,G06F16/951,G06F18/22,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G16B15/00,G16B40/00,G16B45/00,G16B50/10,G16C20/70,G06N3/084,G16C20/30,G16C20/90;System and method for molecular reconstruction and probability distributions using a 3D variational-conditioned generative adversarial network;A system and method that produces an accurate probability distribution representative of a target molecule that may be used in pharmacokinetics and analogous applications. A generator is seeded from a variational autoencoder during training and is then used after training in series with a second variational autoencoder to produce the probability distributions from molecular tensors.;0
479;11264140;2021;2022;RO5 INC.;RO5 INC.;G16H70/40,G06F16/24575,G16H20/10,G16H50/20,G16H50/50,G16H50/70;System and method for automated pharmaceutical research utilizing context workspaces;A system and method for an automated pharmaceutical research utilizing contextual workspaces comprising a workspace drive engine, a data analysis engine, one or more machine and deep learning modules, a knowledge graph, and a workspace interface, which can create a virtual research workspace where data files containing biochemical data related to current research can be uploaded, which automatically processes and analyzes the uploaded data file to autonomously extract a plurality of information related to the uploaded data file, which performs various similarity searches on the uploaded data, and which formats and displays all the extracted information in the workspace, such that the workspace may provide a deeper contextualized view of the uploaded biochemical data.;0
480;11270164;2020;2022;Ford Global Technologies, LLC;Ford Global Technologies, LLC;G06F18/217,G06V10/82,G06F18/214,G06V10/774,G06V10/776,G06V20/56;Vehicle neural network;A system, including a processor and a memory, the memory including instructions to be executed by the processor to train a deep neural network based on a plurality of real-world images, determine the accuracy of the deep neural network is below a threshold based on identifying one or more physical features by the deep neural network, including one or more object types, in the plurality of real-world images and generate a plurality of synthetic images based on the accuracy of the deep neural network is below a threshold based on identifying the one or more physical features using a photo-realistic image rendering software program and a generative adversarial network. The instructions can include further instructions to retrain the deep neural network based on the plurality of real-world images and the plurality of synthetic images and output the retrained deep neural network.;3
481;11271984;2021;2022;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L65/764,G06N3/063,G06N20/00,H04L65/612,H04L65/80,H04L67/02,G06N3/045,G06N3/047,G06N3/088;Reduced bandwidth consumption via generative adversarial networks;"A computer-implemented method comprises determining a desired resolution quality of a media file; determining available resources of a computing device; selecting one of a plurality of Generative Adversarial Network (GAN) plugins based on the determined available resources of the computing device; and estimating a second resolution quality of the media file corresponding to the selected GAN plugin. The second resolution quality is less than the desired resolution quality. The method further comprises communicating with a server to receive the media file at the estimated second resolution quality; converting the received media file from the second resolution quality to the desired resolution quality by using the selected GAN plugin; and outputting the converted media file with the desired resolution quality.";0
482;11275903;2021;2022;Retain Health, Inc;Retain Health, Inc;G06F40/35,G06F40/284,G06F40/56,G06N7/01,G06N20/00,G16H10/20,G16H50/20,G06F40/205,G06N3/045,G06N5/01,G16H20/70,G16H50/30,G16H50/70;System and method for text-based conversation with a user, using machine learning;"In an aspect, systems and methods for text-based conversation with a user, using machine learning, include receiving, using a computing device, at least a feature associated with the user's condition and at least a preference input; generating, using the computing device, a probabilistic output by operating a probabilistic machine learning model input with the at least a feature; classifying, using the computing device, an intervention class by operating a classifying machine learning model input with the probabilistic output and the at least a feature; and, interfacing conversationally, using the computing device, with the user by text generated as a function of the intervention class and the at least a preference input.";1
483;11276389;2019;2022;OBEN, INC.;OBEN, INC.;G10L13/02,G10L13/033,G10L13/047,G10L15/02,G10L15/16,G10L25/18,G10L25/90,G10L2013/105,G10L2015/025,G10L2015/027;Personalizing a DNN-based text-to-speech system using small target speech corpus;A personalized text-to-speech system configured to perform speaker adaption is disclosed. The TTS system includes an acoustic model comprising a base neural network and a differential neural network. The base neural network is configured to generate acoustic parameters corresponding to a base speaker or voice actor, while the differential neural network is configured to generate acoustic parameters corresponding to differences between acoustic parameters of the base speaker and a particular target speaker. The output of the acoustic model is then a weighted linear combination of the output from the base neural network and differential neural network. The base neural network and differential neural network share a first input layer and first plurality of hidden layers. Thereafter, the base neural network further comprises a second plurality of hidden layers and output layer. In parallel, the differential neural network further comprises a third plurality of hidden layers and separate output layer.;4
484;11276434;2020;2022;Rovi Guides, Inc.;Rovi Guides, Inc.;G11B27/34,G06F18/214,G06V20/40,G06V20/46,G06V20/49,G10L25/78,G11B27/031,G11B27/06,H04N21/23418,H04N21/251,H04N21/25883,H04N21/26603,H04N21/8405,G06V2201/10,G10L25/57,H04N21/2668,H04N21/4666,H04N21/8456,H04N21/8549;System and method for generating personalized video trailers;Systems and methods for generating individualized content trailers. Content such as a video is divided into segments each representing a set of common features. With reference to a set of stored user preferences, certain segments are selected as aligning with the user's interests. Each selected segment may then be assigned a label corresponding to the plot portion or element to which it belongs. A coherent trailer may then be assembled from the selected segments, ordered according to their plot elements. This allows a user to see not only segments containing subject matter that aligns with their interests, but also a set of such segments arranged to give the user an idea of the plot, and a sense of drama, increasing the likelihood of engagement with the content.;3
485;11288542;2020;2022;SAP SE;SAP SE;G06F18/214,G06V20/70,G06F18/2413,G06F18/29,G06N3/04,G06N3/045,G06N3/047,G06N3/08,G06V10/40,G06V10/766,G06V10/774,G06V10/82,G06V10/84,G06N7/01,G06N20/00;Learning graph-based priors for generalized zero-shot learning;An image is received for classification. Thereafter, features are extracted from the image which are used by a machine learning model to classify the image. Thereafter, data is provided that characterizes the classification. The machine learning model can be trained using a training data set labeled, in part, using a generative model conditioned on label attribute information in combination with a directed relation graph having a plurality of nodes in which each node without images at training time are given predefined probability distributions. Related apparatus, systems, techniques and articles are also described.;0
486;11289082;2019;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G10L13/02,G10L15/02,G10L2015/225,G10L2015/227;Speech processing output personalization;Described herein is a system for adapting an output to a user input over a period of time based on how often the user interacts with the system. The system may determine a user's level of familiarity of the system, and may determine to personalize the output to a user request based on his level of familiarity. The user's level of familiarity may be determined by analyzing historical interactions between the user and the system. The level of personalization applied to the output may be determined based on the user's level of familiarity. As user becomes more familiar with the system, the output may be more personalized.;1
487;11294756;2019;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F11/079,G06F11/3006,G06F11/0709,G06F11/0751,G06F11/3452,G06F11/3466,G06N3/044,G06N3/045,G06N3/047,G06N3/08,H04L41/064,H04L41/16;Anomaly detection in a network;Anomaly detection for one or more streams of time-series data can use an encoder/decoder pair, such as in a variational autoencoder (VAE) in combination with an aggregator or classifier, such as a random isolation forest (RIF). A particular application relates to detecting anomalies in network updates in a large number of network devices that can transmit the updates to a collector for analysis. The encoder/decoder pair can include a neural network with long short-term memory cells or similar type cells. Using the combination, a single anomaly score can be produced from multiple streams of the time-series data.;0
488;11301269;2020;2022;UiPath, Inc.;UiPath, Inc.;G06F9/451,G06N3/082,G06F11/3438,G06F17/18,G06N20/00,G06F9/45529,G06N3/044,G06N3/047,G06N3/084,G06Q10/06;Determining sequences of interactions, process extraction, and robot generation using artificial intelligence / machine learning models;Use of artificial intelligence (AI)/machine learning (ML) models is disclosed to determine sequences of user interactions with computing systems, extract common processes, and generate robotic process automation (RPA) robots. The AI/ML model may be trained to recognize matching n-grams of user interactions and/or a beneficial end state. Recorded real user interactions may be analyzed, and matching sequences may be implemented as corresponding activities in an RPA workflow.;0
489;11302310;2019;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/065,G10L15/1822,G10L15/01,G10L15/063,G10L15/197,G10L15/083;Language model adaptation;Exemplary embodiments relate to adapting a generic language model during runtime using domain-specific language model data. The system performs an audio frame-level analysis, to determine if the utterance corresponds to a particular domain and whether the ASR hypothesis needs to be rescored. The system processes, using a trained classifier, the ASR hypothesis (a partial hypothesis) generated for the audio data processed so far. The system determines whether to rescore the hypothesis after every few audio frames (representing a word in the utterance) are processed by the speech recognition system.;0
490;11302329;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L25/51,G06F3/167,G10L15/08,G10L25/72,G10L25/03,G10L25/30,G10L25/60,G10L2015/088;Acoustic event detection;A system may include an acoustic event detection component for detecting acoustic events, which may be non-speech sounds. Upon detection of a command to detect a new sound, a device may prompt a user to cause occurrence of the sound one or more times. The acoustic event detection component may then be reconfigured, using audio data corresponding to the occurrences, to detect future occurrences of the event.;0
491;11308657;2021;2022;Neon Evolution Inc.;Neon Evolution Inc.;G06N3/084,G06T11/00,G06F18/214,G06N3/04,G06N3/045,G06N3/088,G06T5/50,G06T5/60,G06T5/77,G06V40/174,G06T2207/10016,G06T2207/20081,G06T2207/20084,G06T2207/20221,G06T2207/30196,G06T2207/30201,G10L21/013,G10L25/18,G10L25/30,G10L2021/0135;Methods and systems for image processing using a learning engine;Systems and methods are disclosed configured to train an autoencoder. A data training set is generated comprising images of different faces. A first autoencoder configuration is generated, comprising a first encoder, and a first decoder. The first autoencoder configuration is trained using dataset images, wherein weights associated with the first encoder and weights associated with the first decoder are modified. A second autoencoder configuration is generated comprising the first encoder and a second decoder. The second decoder is trained using a plurality of images of a first target face. First encoder weights are substantially maintained, and weights associated with the second decoder are modified. An autoencoder comprising the trained first encoder and the trained second decoder is used to generate an output using a source image of a first face having a facial expression, where the facial expression of the first face from the source image is applied to the first specific target face.;2
492;11314922;2021;2022;MOORE AND GASPERECZ GLOBAL, INC.;MOORE AND GASPERECZ GLOBAL, INC.;G06F40/10,G06F40/30,G06F40/20,G06F40/284,G06N3/045,G06N3/08;System and method for generating regulatory content requirement descriptions;A computer-implemented method for generating regulatory content requirement descriptions is disclosed and involves receiving requirement data including a plurality of requirements including hierarchical information extracted from regulatory content. The method involves identifying parent requirements based on the existence of child requirements on a lower hierarchical level and generating requirement pairs including the parent requirement and at least one child requirement. The method also involves feeding each of the pairs through a conjunction classifier which has been trained to generate a classification output indicative of the pair being not a conjunction (NC), a single requirement conjunction (CSR), or a multiple requirement conjunction (CMR). The method involves generating a set of requirement descriptions based on the classification output generated for each parent requirement.;2
493;11314945;2021;2022;Capital One Services, LLC;Capital One Services, LLC;G06F40/42,H04L51/48,H04L67/306;Profile-based natural language message generation and selection;In some embodiments, text for user consumption may be generated based on an intended user action category and a user profile. In some embodiments, an action category, a plurality of text seeds, and a profile comprising feature values may be obtained. Context values may be generated based on the feature values, and text generation models may be obtained based on the text seeds. In some embodiments, messages may be generated using the text generation models based on the action category and the context values. Weights associated with the messages may be determined, and a first text message of the messages may be sent to an address associated with the profile based on the weights. Based on a reaction value obtained in response to the first message, a first expected allocation value may be updated based on the reaction value.;6
494;11315692;2019;2022;Vitalchat, Inc.;Vitalchat, Inc.;G16H80/00,G06F3/017,G06N3/08,G06V20/40,G06V40/20,G16H10/20,G16H20/13,G16H40/67,H04L65/1069,H04L65/1089,H04L65/40,G06F3/167,G06N3/044,G06V30/10,G06V40/28;Systems and methods for video-based user-interaction and information-acquisition;Devices, systems, and methods for real-time video processing and video-based user-interfacing via a videobot are provided. In an aspect, a first compute device receives a videobot deployment request with respect to a communication session with a user at a second compute device. A videobot is provisioned in response to the videobot deployment request to provide a real-time, video-based user-interface between the user and the videobot via the second compute device. A multimedia stream associated with the videobot is sent to the second compute device to cause the second compute device to render the videobot during the communication session. A live multimedia stream associated with the user is received from the second compute device. A user gesture event indicative of patient intake information is detected in connection with an act by the user during the communication session based on at least one video frame of the live multimedia stream.;6
495;11321729;2020;2022;INMAR CLEARING, INC.;INMAR CLEARING, INC.;G06Q30/0222,G06Q30/0239,G06Q30/0633,H04L51/02;Promotion processing system for digital promotion insertion into retailer chatbot conversation and related methods;A system may include a chatbot communications server executing a given chatbot conversation associated with a given retailer from among retailers. The system may also include a remote device associated with a given user, and a promotions processing server. The promotions processing server may be configured to store digital promotions for the retailers, and communicate at least one digital promotion corresponding to the given retailer to the chatbot communications server. The chatbot communications server may be configured to insert the at least one digital promotion into the given chatbot conversation to be displayed on the remote device. The chatbot conversation may permit the given user to clip at least one selected digital promotion, and add a product for purchase corresponding to the at least one selected digital promotion to a virtual shopping cart.;0
496;11328480;2020;2022;Radical Convergence Inc.;Radical Convergence Inc.;G06T17/20,G06T7/74,G06T15/04,G06T2200/04,G06T2207/20081;Rapid generation of three-dimensional characters;Aspects described herein relate to three-dimensional (3D) characters and rapidly generating 3D characters from a plurality of 3D source models. In generating the 3D characters, one or more 3D source models may be standardized, applied to a base mesh with material ID assignments, and decomposed to isolate particular polygonal mesh pieces and/or texture map feature selections denoted by the material IDs of the base mesh. The disparate isolated polygonal mesh pieces and/or texture map feature selections may be assembled in a modular fashion to compose unique 3D characters unlike any of those of the one or more 3D models. The 3D characters may be further refined through the addition of features and/or accessories, and may also be processed through machine learning algorithms to further ascertain uniqueness.;3
497;11329933;2020;2022;Drift.com, Inc.;Drift.com, Inc.;H04L51/02,G10L15/183,H04L51/56;Persisting an AI-supported conversation across multiple channels;A method and computing platform to imitate human conversational response as a context transitions across multiple channels (e.g., chat, messaging, email, voice, third party communication, etc.) where inputs to the system are categorized into identified speech acts and physical acts, and a conversational bot is associated to the channels. In this approach, a data model associated with a multi-turn conversation is provided. The data model comprises an observation history, wherein an observation in the observation history includes an identification of a channel in which the observation originates. As turns are added to the multi-turn conversation, a conversational context across multiple channels is persisted using the data model. Using this approach, an AI-supported conversation started in one channel can move to another conversation channel while maintaining the context of the conversation intact and coherent.;6
500;11341698;2020;2022;Tiliter Pty Ltd.;Tiliter Pty Ltd.;G06T11/60,G06F18/214,G06N3/08,G06T3/40,G06T3/608,G06T5/20,G06T5/50,G06T5/70,G06T11/001,G06V10/25,G06V10/82,G06N20/00,G06T2207/20081,G06T2207/20084,G06T2207/20212,G06T2210/22,G06V20/68;Methods and apparatus for simulating images of produce with markings from images of produce and images of markings;In some embodiments, a method includes receiving an image of produce and an image of marking. The image of produce has a set of pixels, each associated with a position and a color value. The method further includes generating a grayscale image from the image of produce. The method further includes cropping out a portion from the grayscale image. The method further includes locating a marking position pixel on the image of produce by: (a) producing a list of pixels that are part of the cropped portion, (b) selecting, from the list of pixels, a subset of pixels having grayscale pixel values above a threshold, and (c) randomly selecting the marking position pixel from the subset of pixels. The method further includes overlaying the image of marking on the image of produce by coinciding a pixel of the image of marking with the marker position pixel.;0
501;11341699;2021;2022;CARMAX ENTERPRISE SERVICES, LLC;CARMAX ENTERPRISE SERVICES, LLC;G06T11/60,G06F18/2178,G06F18/2433,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G06N7/01,G06N20/00,G06V10/774,G06V10/82,G06T2200/24,G06V20/56;Systems and methods for synthetic image generation;A system includes memory devices storing instructions, and one or more processors configured to execute instructions performing method steps. The method may include training a generator, encoder, and discriminator of a synthetic image generation system to enable creation of synthetic images that comply with one or more image classification requirements. A generator and discriminator may be trained in an adversarial relationship. Training may be completed when the generator outputs a synthetic image that matches a target image beyond a first predetermined threshold of accuracy and the encoder outputs a latent feature vector that matches an input latent feature vector beyond a second predetermined threshold of accuracy. After training the system may be configured to generate synthetic images that comply with one or more image classification requirements.;3
502;11343650;2020;2022;HUAWEI TECHNOLOGIES CO. LTD.;HUAWEI TECHNOLOGIES CO. LTD.;G06N3/08,H04W4/029,G06F18/2135,G06N3/045,G06N20/00,H04L67/52,G06N3/047;Computation of a saddle-point;An unconstrained saddle point of a function is obtained by computing a combination of a first subspace for minimization, and a second subspace for maximization. A combination of a current location including a first and second current location within the first and second subspace is iteratively selected. From the current location, a combination of a step-size including a first and second step-size along a first and second direction of the first and second subspace, is computed. The first and second step-size is to a next first and second location within the first and second subspace. The current location is set to a next location including the next first and second location. The combination of the first and second subspace is according to the next location. The iterations terminate when the next location meets a requirement denoting the unconstrained saddle point. The location indicating the unconstrained saddle point is provided.;0
503;11348577;2019;2022;;;G10L15/22,G06F3/16,G06F3/167,G09B5/04,G09B19/04,G10L13/00,G10L13/08,G10L15/26,G10L25/48,G10L2015/223;Methods, systems, and media for presenting interactive audio content;"Methods, systems, and media for presenting interactive audio content are provided. In some embodiments, the method includes: receiving narrative content that includes action points, wherein each of the action points provides user actions and a narrative portion corresponding to each of the user actions; determining a user engagement density associated with the narrative content, wherein the user engagement density modifies the number of the action points to provide within the narrative content; causing the narrative content to be presented to a user based on the user engagement density; determining that a speech input has been received at one of the action points in the narrative content; converting the speech input to a text input; determining whether the user action associated with the text input corresponds to one of the user actions; selecting the narrative portion corresponding to the text input in response to determining that the user action corresponds to one of the user actions; converting the selected narrative portion to an audio output; and causing the narrative content with the converted audio output of the selected narrative portion to be presented to the user.";0
505;11354164;2018;2022;Automation Anywhere, Inc.;Automation Anywhere, Inc.;G06F9/5055,G06F9/5038,G06F9/505,G06F9/5077,G06F9/5088;Robotic process automation system with quality of service based automation;A computerized method for processing a set of robotic process automation (RPA) tasks receives service level requirement inputs that specify a first set of RPA tasks to be performed within a specified period of time. A response to the service level requirement inputs is computed to determine a number of computing resources required to perform the first set of RPA tasks in the specified period of time. Availability of computing resources from a set of computing resources is determined to generate an allocated set of computing resources. The allocated set of computing resources are deployed. A subset of the first set of RPA tasks is queued for each computing resource and each computing resource is monitored and redeployed as it completes tasks in its queue. Quality of Service (QOS) is achieved by prioritizing certain tasks above others.;0
506;11354490;2020;2022;Intellectual Property Demonstratives, Inc.;Intellectual Property Demonstratives, Inc.;G06F40/169,G06F3/0482,G06F3/0486,G06F40/103,G06F40/106,G06F40/117,G06F40/14,G06F40/143,G06F40/166,G06F40/186;Systems, methods, and computer readable media for creating slide presentations;Systems, methods, and computer readable medium are provided that relates to the field electronic slide presentation applications. Improvements to add new features and functionality in that field are illustratively described. In some embodiments, an automated feature is provided that generates high-quality presentation slides from source content. Other features are contemplated including features related to security, GUIs, smart templates, and others.;5
507;11354582;2021;2022;RO5 INC.;RO5 INC.;G06N5/022,G16B15/30,G06F16/951,G06F18/214,G06F18/22,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G06N5/01,G06N7/01,G06N20/00,G06V30/10,G16B15/00,G16B40/00,G16B40/20,G16B45/00,G16B50/10,G16C20/10,G16C20/70,G06N3/006;System and method for automated retrosynthesis;A system and method for automated retrosynthesis which can reliably identify valid and practical precursors and reaction pathways. The methodology involves a k-beam recursive process wherein at each stage of recursion, retrosynthesis is performed using a library of molecule disconnection rules to identify possible precursor sets, validation of the top k precursor sets is performed using a transformer-based forward reaction prediction scoring system, the best candidate of the top k precursor sets is selected, and a database is searched to determine whether the precursors are commercially available. The recursion process is repeated until a valid chain of chemical reactions is found wherein all precursors necessary to synthesize the target molecule are found to be commercially available.;0
508;11354599;2021;2022;BRYTER GmbH;BRYTER GmbH;G06N20/00,G06F16/9027,G06F18/24,G06F40/20,G06F40/30,G06N5/01,G06N5/02,G06N7/01,G06N7/02;Methods and systems for generating a data structure using graphical models;A system for generating a data structure using graphical models includes a computing device configured to provide a visual interface configured to provide a plurality of graphical models of a plurality of rule modules and receive a relational identification of at least a graphical representation of the plurality of graphical models, the relational indication including at least an entry indication and at least an exit indication, to convert the relational identification into at least a decision tree having at least a root node corresponding and at least a terminal node, to train a machine-learning model to match execution parameters to the at least a root node, and to generate an execution result interface configured to receive at least an execution parameter, map it to the at least a root node using the machine-learning model, and generate an execution result at the at least a terminal node.;0
509;11354803;2021;2022;Omniscient Neurotechnology Pty Limited;Omniscient Neurotechnology Pty Limited;G06T7/0012,G16H30/20,G16H30/40,G06T2207/10092,G06T2207/20024,G06T2207/20081,G06T2207/20084,G06T2207/30016;Identifying spurious tracts in neuro-networks;Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for filtering a set of tracts that are predicted to be included in a selected neuro-network. In one aspect, a method comprises: receiving selection data selecting a network of a brain, processing magnetic resonance image data of the brain to identify a set of tracts that are predicted to be included in the selected network, determining a blocking surface for the selected network, comprising: obtaining a set of parcellations for the selected network and determining, based on a respective position in the brain of each parcellation in the set of parcellations, a set of parameters defining the blocking surface, and generating filtered tract data by filtering the set of tracts that are predicted to be included in the selected network to remove tracts that intersect the blocking surface.;0
510;11355112;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G06N3/08,G06N7/01,G06N20/00,G10L13/08,G10L15/083,G06N3/047,G10L15/32,G10L2015/088,G10L2015/223;Speech-processing system;A system may include first and second speech-processing systems with corresponding first and second wakewords. An utterance may contain two or more wakewords. The system determines which speech-processing system to use to perform further audio processing and to determine a response to the utterance.;0
511;11355155;2021;2022;CLIPr Co.;CLIPr Co.;G06F16/739,G11B27/031,G06F18/2178,G06N20/00,G06V20/47,G10L15/22,G10L15/26,G10L21/10,G10L25/57,H04N21/233,H04N21/23418,H04N21/234336,H04N21/251,H04N21/25891,H04N21/4756,H04N21/8549,H04N21/8586;System and method to summarize one or more videos based on user priorities;"System and method to summarize one or more videos are provided. The system includes a data receiving module configured to receive videos; a video analysis module configured to analyse the one or more videos to generate one or more transcription text output; a building block data module configured to create a building block model and to apply the building block model on analysed videos; a video presentation module configured to present contents of the videos using elements and to present the one or more transcription texts; a video prioritization configured to generate one or more ranking formulas for the videos, to prioritize building block models, upon receiving feedback from users, based on contents and transcription texts; a video summarization module configured to generate a video summary; a video action module configured to choose an action to be performed on the videos based on the feedback received from the corresponding users.";6
512;11356390;2021;2022;Bank of America Corporation;Bank of America Corporation;H04L51/02,G06F40/30,G06F40/35,G06N5/04,G06Q30/016,G06Q30/0202,G06N20/00;Multi-dimensional/multi-actor conversational bot;A method for encouraging dialogue between, and promoting transparency of said dialogue, a plurality of telecommunicators is provided. The plurality of telecommunicators may include an end user and an agent. The method includes receiving a request for a dialogue from the end user. The request includes a set of end user log-in credentials associated with the end user. The method further includes identifying the agent associated with a set of agent log-in credentials. The agent may respond to the request. The method also distinguishes the end user from the agent based on the set of end user log-in credentials and the set of agent log-in credentials and then instantiates an instance of telecommunications between the end user and the agent based, at least in part, on the distinguishing. Finally, the method includes monitoring the instantiation.;0
514;11361571;2021;2022;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F40/30,G06V30/414,G06F40/166,G06F40/268,G06F40/279,G06N3/08,G06V30/19093,G06V30/19147,G06V30/1985,G06N3/045,G06N5/02;Term extraction in highly technical domains;A language model is fine-tuned by extracting terminology terms from a text document. The method comprises identifying a text snippet, identifying candidate multi-word expressions using part of speech tags, and determining a specificity score value for each of the candidate multi-word expressions. Moreover, the method comprises determining a topic similarity score value for each of the candidate multi-word expressions, selecting remaining expressions from the candidate multi-word expressions using a function of a specificity value and a topic similarity value of each of the candidate multi-word expressions, adding a noun comprised in the text snippet to the remaining expressions depending on a correlation function, labeling the remaining multi-word expressions, and fine-tuning an existing pre-trained transformer-based language model using as training data the identified text snippet marked with the labeled remaining expressions.;3
515;11374952;2019;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04L63/1425,G06N3/045,G06N3/047,G06N3/08,G06N3/088,H04L63/1416,H04L63/0227;Detecting anomalous events using autoencoders;Techniques for monitoring a computing environment for anomalous activity are presented. An example method includes receiving a request to invoke an action within a computing environment, with the request including a plurality of request attributes and a plurality of contextual attributes. A normalcy score is generated for the received request by encoding the received request into a code in latent space of an autoencoder, reconstructing the request from the code, and generating a probability distribution indicating a likelihood that the reconstructed request attributes exist in a data set of non-anomalous activity. Based on the calculated normalcy score, one or more actions are taken to process the request such that execution of non-anomalous requests is allowed, and execution of potentially anomalous requests may be blocked pending confirmation.;0
516;11379189;2021;2022;salesforce.com, inc.;salesforce.com, inc.;G06F8/20,G06F8/38,G06F9/451,G06N20/00;Automatic user interface data generation;Techniques are disclosed relating to automatically synthesizing user interface (UI) component instances. In disclosed techniques a computer system receives a set of existing UI elements and a set of design rules for the set of existing elements, where design rules in the set of design rules indicate one or more allowed states for respective UI elements in the set of existing UI elements. The one or more allowed states may correspond to one or more visual characteristics. Using the set of existing UI elements, the computer system may then automatically generate a plurality of UI component instances based on the set of design rules, where a respective UI component instance includes a first UI element in a first allowed state. The computer system may then train, using the plurality of UI component instances, a machine learning model operable to automatically generate UI designs.;3
517;11379446;2021;2022;FMR LLC;FMR LLC;G06F16/22,G06F16/24552;Session-based data storage for chat-based communication sessions;Methods and apparatuses are described for session-based data storage for chat-based communication sessions. A computing device connects to a data storage area associated with an active chat-based communication session, the data storage area comprising data storage slots. The computing device identifies a first data event during the chat-based communication session, the first data event comprising one of a data capture event, a data retrieval event, or a data prefill event. The computing device identifies a data storage slot in the data storage area corresponding to the first data event, the data storage slot comprising a slot name and a slot value. The computing device selects data elements associated with the chat-based communication session for insertion into the data storage slot based upon the event type of the first data event. The computing device stores the selected data elements in the data storage slot for retrieval during the chat-based communication session.;0
518;11379747;2020;2022;BABYLON PARTNERS LIMITED;BABYLON PARTNERS LIMITED;G06N7/01,G16H50/20,G06N5/046,G06N20/00,G16H50/50,G16H50/70,G16H70/60;Counterfactual measure for medical diagnosis;A method for providing a computer-implemented medical diagnosis includes receiving an input from a user comprising at least one symptom of the user. The method also includes providing the at least one symptom as an input to a medical model, the medical model being retrieved from memory. The medical model includes a probabilistic graphical model comprising probability distributions and relationships between symptoms and diseases. The method also includes performing inference on the probabilistic graphical model to obtain a prediction of the probability that the user has that disease. The method also includes outputting an indication that the user has a disease from the Bayesian inference, wherein the inference is performed using a counterfactual measure.;0
520;11393451;2017;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G10L13/08,G06F40/134,G10L13/04,G10L15/22,G10L15/30,G10L13/00,G10L15/142,G10L15/16,G10L2015/223;Linked content in voice user interface;A speech processing system configured to convey linked content to a user in a voice user interface (VUI) environment is described. The system may create output audio data that includes an audible indicator (e.g., a beep) that indicates to a user linked content. An audible indicator may be located at the beginning and/or end of linked content in the output audio data. The system may also output non-linked content in a first voice while outputting linked content in a second voice. The system may further overlay audio atop the linked portion of the output audio data, with the overlaid audio indicating the linked content to the user. A user may invoke linked content in output audio by speaking the linked content, or a portion thereof, back to the system. The system may then output the linked, additional content to the user.;0
521;11393575;2021;2022;Paige.AI, Inc.;Paige.AI, Inc.;G16H30/20,G06N3/045,G06N3/088,G06N20/00,G06T7/0012,G06T11/008,G16H30/40,G16H50/20,G16H50/50,G16H50/70,G06T7/11,G06T2207/20081,G06T2207/20084,G06T2207/20112,G06T2207/20212,G06T2207/30004,G06T2211/441;Systems and methods to process electronic images for synthetic image generation;Systems and methods are disclosed for generating synthetic medical images, including images presenting rare conditions or morphologies for which sufficient data may be unavailable. In one aspect, style transfer methods may be used. For example, a target medical image, a segmentation mask identifying style(s) to be transferred to area(s) of the target, and source medical image(s) including the style(s) may be received. Using the mask, the target may be divided into tile(s) corresponding to the area(s) and input to a trained machine learning system. For each tile, gradients associated with a content and style of the tile may be output by the system. Pixel(s) of at least one tile of the target may be altered based on the gradients to maintain content of the target while transferring the style(s) of the source(s) to the target. The synthetic medical image may be generated from the target based on the altering.;3
522;11403800;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06T13/40,G06F18/2148,G06T15/20,G06T17/00,G06T19/00,G06V10/56,G06V10/82,G06V20/64,G06V20/653,G06V40/103;Image generation from 3D model using neural network;Systems and methods are provided for generating an image of a posed human figure or other subject using a neural network that is trained to translate a set of points to realistic images by reconstructing projected surfaces directly in the pixel space or image space. Input to the image generation process may include parameterized control features, such as body shape parameters, pose parameters and/or a virtual camera position. These input parameters may be applied to a three-dimensional model that is used to generate the set of points, such as a sparsely populated image of color and depth information at vertices of the three-dimensional model, before additional image generation occurs directly in the image space. The visual appearance or identity of the synthesized human in successive output images may remain consistent, such that the output is both controllable and predictable.;3
525;11409963;2019;2022;Pivotal Software, Inc.;Pivotal Software, Inc.;G06F40/30,G06N5/04,G06N20/00,G06N3/08;Generating concepts from text reports;"Methods, systems, and apparatus, including computer programs encoded on computer storage media, for automatic text report concept generation. Generating concepts from text reports includes receiving a collection of text reports; performing a clustering process for a plurality of different cluster sizes; evaluating each of the plurality of different cluster sizes to select an optimal cluster size; generating, from the collection of text reports, clusters using the selected optimal cluster size; aggregating text associated with text reports in each cluster; maintaining a training dataset comprising the aggregated text; and generating a predictive model from the training dataset to generate a concept for an input text report.";1
526;11410684;2019;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L25/78,G06F16/38,G06N3/044,G06N3/045,G06N3/08,G06N7/01,G10L13/027,G10L15/16,G10L15/187,G10L13/033,G10L13/04,G10L13/047,G10L13/07,G10L15/26;Text-to-speech (TTS) processing with transfer of vocal characteristics;Audio data from a first, source speaker is received and processed to determine linguistic units and vocal characteristics corresponding to those linguistic units. The linguistic units may either be determined from received text data or may be determined from the audio data using automatic speech recognition. A model is trained using training data from a second, target speaker. The trained model concatenates the linguistic units with the vocal characteristics to produce output speech that has the âvoiceâ of the target speaker and the vocal characteristics of the source speaker.;5
527;11410685;2022;2022;Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;G10L15/04,G10L25/87,G06N3/045,G06N3/08,G10L25/24,G10L25/30,G10L15/16;Method for detecting voice splicing points and storage medium;"Disclosed are a method for detecting speech concatenating points and a storage medium. The method includes: acquiring a speech to be detected, and determining high-frequency components and low-frequency components of the speech to be detected; extracting first cepstrum features and second cepstrum features corresponding to the speech to be detected according to the high-frequency components and the low-frequency components; splicing the first and the second cepstrum feature of speech per frame in the speech to be detected in units of frame so as to obtain a parameter sequence; inputting the parameter sequence into a neural network model so as to obtain a feature sequence corresponding to the speech to be detected, wherein the model has been trained, has learned and stored a correspondence between the parameter sequence and the feature sequence; and performing detection of speech concatenating points on the speech to be detected according to the feature sequence.";0
528;11416712;2021;2022;SAS INSTITUTE INC.;SAS INSTITUTE INC.;G06F18/2148,G06F18/23,G06F18/217,G06F18/2413,G06N3/045,G06N3/0475,G06N3/09,G06N3/048,G06N7/01,G06N20/00;Tabular data generation with attention for machine learning model training system;"A computing device generates synthetic tabular data. Until a convergence parameter value indicates that training of an attention generator model is complete, conditional vectors are defined; latent vectors are generated using a predefined noise distribution function; a forward propagation of an attention generator model that includes an attention model integrated with a conditional generator model is executed to generate output vectors; transformed observation vectors are selected; a forward propagation of a discriminator model is executed with the transformed observation vectors, the conditional vectors, and the output vectors to predict whether each transformed observation vector and each output vector is real or fake; a discriminator model loss value is computed based on the predictions; the discriminator model is updated using the discriminator model loss value; an attention generator model loss value is computed based on the predictions; and the attention generator model is updated using the attention generator model loss value.";3
529;11423616;2020;2022;Facebook Technologies, LLC;Facebook Technologies, LLC;G06T17/20,G06N3/0455,G06N3/0464,G06N3/09,G06N20/00,G06T7/50,G06T7/73,G06T15/04,G06N3/0475,G06N3/084,G06T2207/10028,G06T2207/20081;Systems and methods for rendering avatar with high resolution geometry;In one embodiment, a system may access an input image of an object captured by cameras, and the input image depicts appearance information associated with an object. The system may generate a first mesh of the object based on features identified from the input image of the object. The system may generate, by processing the first mesh using a machine-learning model, a position map that defines a contour of the object. Each pixel in the position map corresponds to a three-dimensional coordinate. The system may further generate a second mesh based on the position map, wherein the second mesh has a higher resolution than the first mesh. The system may render an output image of the object based on the second mesh. The system disclosed in the present application can render a dense mesh which has a higher resolution to provide details which cannot be compensated by texture information.;3
531;11430434;2017;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/167,G10L15/08,G10L15/30,H04L67/52,G10L15/142,G10L15/16,G10L15/18,G10L2015/088,G10L2015/223,G10L2015/227,H04L67/306;Intelligent privacy protection mediation;Systems, methods, and devices for privacy protection and user data obfuscation are disclosed. A speech-controlled device captures audio including a spoken command, and sends audio data corresponding thereto to a server(s). The server(s) 120 determines a user that spoke the command. The server(s) also determines, based on a profile of the user, user data (e.g., age, geographic location, etc.). The server(s) determines user group data encompassing the user data (e.g., including an age range encompassing the user's age, a geographic area encompassing the user geographic location, etc.). The server(s) determines a remote device(s) storing or having access to content responsive to the spoken command. The server(s) sends the user group data to the remote device(s), receives output content responsive to the spoken command and tailored to the user group data from the remote device(s), and causes the speech-controlled device to emit the output content.;0
532;11430467;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F40/30,G10L25/63,G09B19/04,G10L25/30,G10L15/26,G10L17/00,G10L2015/228;Interaction emotion determination;Disclosed are systems and methods for determining sentiment data representative of an emotional state of a user during an interaction with one or more non-users. The disclosed implementations may further determine, for frequent interactions between the user and a non-user, the identity of the non-user and provide information to the user relating to the sentiment of the user when interacting with the non-user.;0
533;11431660;2021;2022;Conversation Processing Intelligence, Corp.;Conversation Processing Intelligence, Corp.;H04L51/02,G06F3/04847,G06F40/279,G06F40/30,G06N3/006,G06N3/045,G06N20/20,G10L15/22,G06N5/045,G10L2015/223;System and method for collaborative conversational AI;A method for collaborative conversational artificial intelligence (CCAI). The invention discloses an architecture wherein members of the disclosed system participate in collaborative conversations with one or more AI and human âsubmindsâ connected via a forum, including conversing in natural language and facilitated by one or more âfacilitatorsâ. CCAI Applications include the creation of widely extensible evolving modular polylogical groups that are capable of collaboration with sentient beings, collaborative control of devices, service worker interfaces, hybrid representations of sentient beings (including via âreconveyanceâ of conversation segments), in collaborations that may include, exclude or require human or AI participation.;3
534;11436438;2021;2022;SAS INSTITUTE INC.;SAS INSTITUTE INC.;G06F18/2148,G06N3/084,G06N3/045,G06N3/047;Tabular data generation for machine learning model training system;(A) Conditional vectors are defined. (B) Latent observation vectors are generated using a predefined noise distribution function. (C) A forward propagation of a generator model is executed with the conditional vectors and the latent observation vectors as input to generate an output vector. (D) A forward propagation of a decoder model of a trained autoencoder model is executed with the generated output vector as input to generate a plurality of decoded vectors. (E) Transformed observation vectors are selected from transformed data based on the defined plurality of conditional vectors. (F) A forward propagation of a discriminator model is executed with the transformed observation vectors, the conditional vectors, and the decoded vectors as input to predict whether each transformed observation vector and each decoded vector is real or fake. (G) The discriminator and generator models are updated and (A) through (G) are repeated until training is complete.;3
536;11438457;2021;2022;UNIPHORE SOFTWARE SYSTEMS, INC.;UNIPHORE SOFTWARE SYSTEMS, INC.;H04M3/5175,G06N3/0475,G06N3/088,G06N20/00,H04M3/42221,H04M3/5191,H04M2203/401,H04M2203/403;Method and apparatus for coaching call center agents;A method and an apparatus for coaching call center agents is provided. The method includes analyzing a conversation of the agent with a first customer, determining a performance of the agent on at least one behavioral skill based on the analysis, generating automatically, a custom training package (CTP) based on the determined first performance, and sending the CTP for presentation on the agent device.;0
537;11445065;2020;2022;Wells Fargo Bank, N.A.;Wells Fargo Bank, N.A.;G06F21/45,H04M3/5166,G06F21/32,G10L17/00,H04M3/5175,H04M3/5183,G06F2221/2133,H04M2203/6072;Voice captcha and real-time monitoring for contact centers;A call screening computing system is described that is configured to perform voice captcha and real-time monitoring of calls into a contact center of an organization. The call screening computing system includes a chat bot configured to operate as an AI-based call screener. The chat bot is configured to perform voice captcha by sending a random question to a user device placing a call into the contact center, and analyzing the received answer to determine whether a user of the user device is human or a robot. The chat bot is configured to, based on the user being human, determine whether the user is a legitimate customer of the organization by generating and presenting authentication challenges to the user device. The chat bot may be configured to monitor and interact with a conversation between the user and an agent of the organization during the call into the contact center.;0
538;11445989;2021;2022;Omniscient Neurotechnology Pty Limited;Omniscient Neurotechnology Pty Limited;A61B6/4035,A61B5/055,A61B6/461,A61B6/466,A61B6/501,A61B6/5217,A61B6/5223,A61B8/0808,A61B8/461,A61B8/466,A61B8/5223,G01R33/5608,G06T7/0012,A61B5/4064,A61B2576/026,G01R33/4806,G01R33/56341,G06T2207/10088,G06T2207/30016;Interfaces for identifying and characterizing tracts in neuro-networks;"Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for generating visualization data for a selected neuro-network. In one aspect, a method comprises: receiving selection data selecting a network in a brain of a subject; processing magnetic resonance image data of the brain to identify a set of tracts that are predicted to be included in the selected network; processing the set of tracts to identify a proper subset of the set of tracts as being spurious tracts; generating a set of valid tracts by filtering the spurious tracts from the set of tracts that are predicted to be included in the selected network; providing visualization data for the selected network showing a three-dimensional spatial representation of both: (i) the valid tracts, and (ii) the spurious tracts, wherein the spurious tracts are visually distinguished from the valid tracts.";0
539;11455658;2021;2022;;;G06Q30/0255,G06N5/04,G06Q30/0201,G06Q30/0242,G06Q30/0276,H04L67/02,G06N20/00,G06Q30/0247,G06Q30/0273;Artificially intelligent campaign creation and deployment system;"An artificially intelligent network for ad campaign design and distribution having a first subsystem for receiving and storing permission to receive targeted content; a second subsystem for selecting recipients for the content; a third subsystem responsive to the second subsystem for distributing the content; and a fourth subsystem for confirming the viewing of the content. In the illustrative embodiment, the system further includes a subsystem for creating as well as receiving the content. The system provides an artificially intelligent campaign creation and deployment system with an advertiser interface; a subscriber interface; and a targeting system for creating, deploying and monitoring an advertising campaign using inputs received via the advertiser and subscriber interfaces and a unique targeting system. The targeting system comprises first arrangement for receiving a first data set with user permissions and preference data; second arrangement for receiving a second data set based on demographic data; and third arrangement for correlating the first data set with the second data set to provide a third data set optimized with respect to at least one parameter. In an illustrative embodiment, the second arrangement further includes arrangement for receiving and factoring in historical and trending data and the parameter is return on investment.";1
541;11461774;2022;2022;;;G06Q20/38215,G06Q20/065,G06Q20/223,G06Q20/36,G06Q20/367,G06Q20/3672,G06Q30/0209,H04L9/3213,G06Q2220/00,H04L9/50,H04L2209/56,H04L2209/603;Creating and managing artificially intelligent entities represented by non-fungible tokens on a blockchain;Some examples of the present disclosure relate to generating artificially intelligent entities represented on a blockchain using a non-fungible token (NFT). In one such example, a system can generate an NFT on a blockchain. The NFT can represent an artificially intelligent entity. The system can also generate a personality dataset on the blockchain, the personality dataset describing personality characteristics of the artificially intelligent entity. The system can then correlate the NFT to the personality dataset, thereby assigning the personality characteristics to the artificially intelligent entity. Once generated, the artificially intelligent entity may reside in a virtual ecosystem in which it can perform tasks and learn over time.;3
542;11461952;2022;2022;Attune Media Labs, PBC;Attune Media Labs, PBC;G06F40/30,G06T13/40,G06T13/205,G06V20/20,G06V40/174,G06V40/176,G06V40/193,G10L15/1815,G10L15/22,G10L25/63,G06F40/279,G10L13/033,G10L15/26;Systems and methods for automated real-time generation of an interactive attuned discrete avatar;Systems and methods enabling rendering an avatar attuned to a user. The systems and methods include receiving audio-visual data of user communications of a user. Using the audio-visual data, the systems and methods may determine vocal characteristics of the user, facial action units representative of facial features of the user, and speech of the user based on a speech recognition model and/or natural language understanding model. Based on the vocal characteristics, an acoustic emotion metric can be determined. Based on the speech recognition data, a speech emotion metric may be determined. Based on the facial action units, a facial emotion metric may be determined. An emotional complex signature may be determined to represent an emotional state of the user for rendering the avatar attuned to the emotional state based on a combination of the acoustic emotion metric, the speech emotion metric and the facial emotion metric.;6
544;11470279;2021;2022;Zoom Video Communications, Inc.;Zoom Video Communications, Inc.;H04L12/1831,H04N7/155,G06F40/166,G06F40/289,G11B27/031;Automated recording highlights for conferences;A transcript of a conference (e.g., a video conference, an audio conference, or a telephone call with two or more participants) is processed to extract a conference summary. The transcript includes strings that are associated with respective timestamps and respective speaker identifiers. Speaker segmentsâsequences of consecutive strings attributed to the same speakerâare identified in the transcript. A speaker segment is selected based on its duration in time and one or more strings are selected from with the selected speaker segment for inclusion in the conference summary. A short video conference summary or a short audio conference summary is then generated using timestamps from the transcript associated with strings (e.g., sentences) that have been selected for inclusion in the conference summary. The short video or audio summary may be presented to users to enable efficient storage and transmission of information from the conference within a unified communications system.;0
545;11475276;2017;2022;Apple Inc.;Apple Inc.;G06N3/045,G06V10/774,G06N3/0455,G06N3/08,G06N3/094,G06V10/82,G06V40/113,G06F18/217,G06F18/24,G06F18/28,G06N3/0464;Generating more realistic synthetic data with adversarial nets;A generative network may be learned in an adversarial setting with a goal of modifying synthetic data such that a discriminative network may not be able to reliably tell the difference between refined synthetic data and real data. The generative network and discriminative network may work together to learn how to produce more realistic synthetic data with reduced computational cost. The generative network may iteratively learn a function that synthetic data with a goal of generating refined synthetic data that is more difficult for the discriminative network to differentiate from real data, while the discriminative network may be configured to iteratively learn a function that classifies data as either synthetic or real. Over multiple iterations, the generative network may learn to refine the synthetic data to produce refined synthetic data on which other machine learning models may be trained.;3
546;11475334;2017;2022;HRL Laboratories, LLC;HRL Laboratories, LLC;G06N3/006,G06N5/043,G06N7/01,G06Q30/0201,G06Q30/0207,G06Q30/0241,G06Q50/01,G06Q50/265;System and method for event prediction using online social media;Described is a system for large-scale event prediction and a corresponding response. The system, using an agent-based model, predicts how many users (agent accounts) on a social media platform will become activists related to a large-scale event. This process is accomplished using both Before and During models. Before the large-scale event, the system operates to generate agent attributes and a posting network based on posts on the social media platform. During the large-scale event and based on the agent attributes and posting network, the system determines if a social media user (agent account) will become an activist of the large-scale event and a corresponding magnitude of the large-scale event. Depending on the magnitude, the system can implement a responsive measure and control a device based on the prediction of the activists.;0
547;11477142;2021;2022;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L51/02,G06F40/30,G06F40/35,G06Q30/016,H04L51/046,G06Q30/0201;Question suggestion and agent transfer to advance a customer support conversation;In an approach for identifying when transferring a real-time conversation on a chatbot application to a customer support agent would be more valuable than outputting suggested queries, a processor classifies a query inputted into a real-time conversation as an intention. A processor predicts a set of next intentions of the user. A processor filters out one or more intentions from the set of next intentions of the user that do not further the real-time conversation. A processor generates a set of suggested queries the user can ask to further the real-time conversation from the subset of next intentions. A processor builds a suggestion evaluation contextual bandit model that determines whether the real-time conversation will be resolved successfully if transferred to a customer support agent. A processor trains the suggestion evaluation contextual bandit model against a set of annotated historical interactions. A processor outputs a response recommendation.;0
548;11481559;2019;2022;Wells Fargo Bank, N.A.;Wells Fargo Bank, N.A.;G06F40/30,G06F9/543,G06F40/247,G06F40/295;Artificial intelligence natural language processing platform;Systems and techniques for an artificial intelligence natural language processing platform are described herein. A first dialogue configuration ruleset may be obtained for a first intent for a first entity. First user input may be received that includes a first text string from a chatbot session. The first intent may be identified from the chatbot session through evaluation of the first text string by a natural language processing engine using the first dialogue configuration ruleset. A dialogue stack may be generated for the chatbot session that includes a first state based on the first dialogue configuration ruleset. A first response to the first user input may be displayed based on the first dialogue configuration ruleset. The dialogue stack may be updated based on the first response.;2
549;11483494;2022;2022;Twyn Limited;Twyn Limited;G06F40/35,H04N5/2628,G06F16/7834,G06F16/90332,G10L15/22,H04L51/02,H04N5/272,H04N7/147,H04N7/15,H04N7/155,H04N23/63,G06F3/04817,G06F3/0482,G06F2203/04803,G06F2203/04804,H04L51/10;Video-based conversational interface;In an answer view, a first video segment is selected based on a first natural language input and displayed in a main display region, and a self-video image of a user is displayed in a peripheral display region having a smaller area than the main display region. To transition from the answer view to a question view, the self-video image is enlarged to replace the first video segment in the main display region. A second natural language input is received. To transition from the question view to the answer view, the self-video image is reduced to occupy the peripheral display region and the self-video image is replaced in the main display region with a second video segment selected based on the second natural language input. The video segments are pre-recorded video response segments spoken by the same person. Enlarging the self-video image masks the transition between the video segments.;0
550;11488288;2022;2022;AJOU UNIVERSITY INDUSTRY-ACADEMIC COOPERATION FOUNDATION;AJOU UNIVERSITY INDUSTRY-ACADEMIC COOPERATION FOUNDATION;G06T5/73,G06N3/045,G06N3/047,G06N3/0475,G06N3/088,G06V10/30,G06V10/82,G06V40/172,G06T2207/20081,G06T2207/20084;Method and apparatus for processing blurred image;Disclosed are a method and an apparatus for processing a blurred image. The method for processing a blurred image includes the steps of generating a first input feature map and a second input feature map with a feature distribution for blur removal from the blurred image, generating a prediction feature map from the first input feature map by using a self-spatial feature transform (SSFT) module which transforms the feature distribution for blur removal into a feature distribution for face recognition without external information, and generating a deblurred image based on the second input feature map and the prediction feature map.;3
551;11488634;2021;2022;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G11B27/028,G11B27/031,G09B5/06,G11B27/28,H04L12/1831;Generating video summaries based on notes patterns;A method, computer system, and a computer program product for generating a summary video from a plurality of presentation attendee notes is provided. Embodiments of the disclosure may include collecting the plurality of attendee notes from a plurality of attendees of a presentation and then analyzing the collected plurality of notes to identify a plurality of common note topics. The identified plurality of common note topics may be correlated to a plurality of portions in a presentation video of the presentation which is used to generate the summary video by combining the correlated plurality of portions in the presentation video.;4
552;11494486;2019;2022;HRL Laboratories, LLC;HRL Laboratories, LLC;G06F21/554,A63F13/422,A63F13/822,G06N3/045,G06N3/0475,G06N5/02,G06F2221/034,G06N3/044,G06N3/084,G06N3/086,G06N3/094,G06N5/041;Continuously habituating elicitation strategies for social-engineering-attacks (CHESS);Described is a system for continuously predicting and adapting optimal strategies for attacker elicitation. The system includes a global bot controlling processor unit and one or more local bot controlling processor units. The global bot controlling processor unit includes a multi-layer network software unit for extracting attacker features from diverse, out-of-band (OOB) media sources. The global controlling processing unit further includes an adaptive behavioral game theory (GT) software unit for determining a best strategy for eliciting identifying information from an attacker. Each local bot controlling processor unit includes a cognitive model (CM) software unit for estimating a cognitive state of the attacker and predicting attacker behavior. A generative adversarial network (GAN) software unit predicts the attacker's strategies. The global bot controlling processor unit and the one or more local bot controlling processor units coordinate to predict the attacker's next action and use the prediction to disrupt an attack.;6
553;11501079;2019;2022;X Development LLC;X Development LLC;G06F40/30,G06N3/045,G06N3/088,G06N3/0455,G06N3/047,G06N3/0475;Personalized content creation using neural networks;A technique for dynamic generation of a derivative story includes obtaining content preferences from a content consumer. The content preferences indicate preferences for characteristics of the derivative story. A content data structure is identified based at least in part on the content preferences. The content data structure specifies story elements of a preexisting story. The story elements are defined at one or more different levels of story abstraction and associated with metadata constraints that constrain modification or use of the story elements within the derivative story. At least some of the metadata constraints indicate whether associated ones of the story elements are mutable story elements. One or more of the mutable story elements are adapted to the content preferences of the content consumer as constrained by the metadata constraints to generate the derivative story. The derivative story is then rendered via a user interface.;3
554;11507059;2021;2022;;;G05B19/409,G05B19/4155,G05B2219/31229;System and computer-implemented method for seamless consumption of automations;A system and a method for accessing at least one automation from an automation store are provided. The method comprises receiving a user input indicative of selection of at least one automation for accessing from a plurality of automations displayed in the automation store, and automatically uploading, in response to receiving the user input, the selected automation to a personal workspace of the user from the automation store. The automations are accessed via one or more Application Programming Interface (API) calls directed to an automation cloud server. Further, the method comprises generating a notification indicative of upload of the selected automation for accessing the automation. The uploaded automation is displayed in a software robot assistant associated with the user. Furthermore, the method comprises displaying the generated notification in an application interface associated with the automation store and displaying the selected automation in the personal workspace in the application interface.;0
555;11508042;2020;2022;STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY;STATE FARM MUTUAL AUTOMOBILE INSURANCE COMPANY;G06T5/77,G06N3/045,G06N3/088,G06T5/60,G06T7/579,G06N3/084,G06T2207/10016,G06T2207/10024,G06T2207/10028,G06T2207/10032,G06T2207/20081,G06T2207/20084,G06T2207/30184;Imputation of 3D data using generative adversarial networks;A generative adversarial network (GAN) is manufactured by a process including obtaining a three-dimensional (3D) point cloud, extracting a region from the 3D point cloud, the region corresponding to a gap, analyzing the extracted region to generate a loss, backpropagating the loss, and updating weights of the GAN. A computer-implemented method for training a GAN includes obtaining a 3D point cloud, extracting a region from the 3D point cloud, the region corresponding to a gap, analyzing the extracted region to generate a loss, backpropagating the loss, and updating weights of the GAN. A server includes a processor and a memory storing instructions that, when executed by the processor, cause the server to obtain a 3D point cloud, extract a region from the 3D point cloud, the region corresponding to a gap, analyze the extracted region to generate a loss, backpropagate the loss, and update weights of the GAN.;3
556;11508045;2022;2022;CARL ZEISS MICROSCOPY GMBH;CARL ZEISS MICROSCOPY GMBH;G06T5/90,G06T3/40,G06T5/20,G06T5/60,G06T5/70,G06T5/73,G06T5/94,G06T2207/10024,G06T2207/10056,G06T2207/20081,G06T2207/20084,G06T2207/30072;Microscopy system and method for generating stylized contrast images;In a computer-implemented method for generating an image processing model that generates output data defining a stylized contrast image from a microscope image, model parameters of the image processing model are adjusted by optimizing at least one objective function using training data. The training data comprises microscope images as input data and contrast images, wherein the microscope images and the contrast images are generated by different microscopy techniques. In order for the output data to define a stylized contrast image, the objective function forces a detail reduction or the contrast images are detail-reduced contrast images with a level of detail that is lower than in the microscope images and higher than in binary images.;3
557;11509995;2021;2022;;;G10K11/17881,H04R1/1083,G10K11/178,G10K11/17827,G10K11/17885,H04R1/1016,G10K2210/1081,H04R2420/07,H04R2460/01;Artificial intelligence based system and method for generating silence in earbuds;An earbud system for reducing noise is disclosed. The earbud system comprises an analog to digital converter to convert analog audio signals of a sound captured by one or more sound sensors into digital audio signals, and one or more processors to filter one or more raw bits with a specific frequency band from the digital audio signals to obtain filtered digital audio signals and extract human audible frequency bands from the filtered digital audio signals. The processors predict and/or classify, using an artificial intelligence based mechanism, the extracted human audible frequency bands into ambient noise corresponding to a first set of frequencies and desired sound, to be heard by the user, corresponding to a second set of frequencies. The processors further generate anti-phase signals for a first set of frequencies to reduce noise, and audible phase signals for the second set of frequencies.;3
558;11514493;2020;2022;Overstock.com, Inc.;Overstock.com, Inc.;G06Q30/0617,G06F21/32,G06F40/216,G06F40/30,G06F40/35,G06F40/56,G06Q30/0281,G06Q30/0631,G06Q30/0641,G06N20/00;System and method for conversational commerce online;This disclosure relates generally to a system and method for creating computer-generated conversational commerce. A conversational AI interacts with a human being and guides that human being through a virtual interaction while communicating and humanizing the interaction. A user makes a request, whether through voice or through typing at the computer, and the system interprets that request while making additional, natural communications with the user, such as suggesting additional items, asking additional questions to gain more information when needed, and learning from interactions. The AI simulates a human being on the website, responding to the user, and asks for additional information, as if a user were actually at a brick-and-mortar store interacting with a salesperson.;6
559;11514633;2022;2022;Pointr Limited;Pointr Limited;G06T11/203,G06T11/60,G06F30/12,G06F30/13,G06F30/27,G06N3/045,G06N3/0475,G06N3/094,G06N20/00,G06T11/20,G06F2111/12,G06N3/088,G06N5/02,G06N5/04,G06N20/20;Systems and methods for automating conversion of drawings to indoor maps and plans;"Automating conversion of drawings to indoor maps and plans. One example is a computer-implemented method comprising: preprocessing an original CAD drawing to create a modified CAD drawing, a text database, a CAD vector-image, and a CAD raster-image; determining a floor depicted in the CAD drawing by applying the CAD raster-image, the CAD vector-image, and the text database to a floor-level machine-learning algorithm resulting in a floor-level outline; sensing furniture depicted on the floor by applying the floor-level outline, the CAD vector-image, and the text database to a furniture-level machine-learning algorithm resulting in a set of furniture entities; identifying each room depicted in the CAD drawing by applying the floor-level outline, the furniture entities, CAD vector-image, and the text database to room-level machine-learning algorithm resulting in a plurality of room-level outlines; and creating an indoor map by combining the furniture entities and the plurality of room-level outlines.";0
560;11514948;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G11B27/036,G06F16/2365,G06F16/7834,G06F40/30,G06F40/58,G06N3/045,G06N3/0475,G06N3/08,G06N3/088,G06N5/022,G10L13/00,G10L17/00,G11B27/10,H04N7/155,H04N21/2343,H04N21/234336,H04N21/23439,G06N3/04,G10L13/033;Model-based dubbing to translate spoken audio in a video;Model-based dubbing techniques are implemented to generate a translated version of a source video. Spoken audio portions of a source video may be extracted and semantic graphs generated that represent the spoken audio portions. The semantic graphs may be used to produce translations of the spoken portions. A machine learning model may be implemented to generate replacement audio for the spoken portions using the translation of the spoken portion. A machine learning model may be implemented to generate modifications to facial image data for a speaker of the replacement audio.;6
561;11515041;2021;2022;Omniscient Neurotechnology Pty Limited;Omniscient Neurotechnology Pty Limited;G16H50/20,G06F3/04847,G06T11/60,G16H30/20,G16H30/40,G16H50/50,A61B6/465,A61B6/469,A61B6/501,A61B8/0808,G06T2200/24,G06T2210/41;Display of subset brain graph by shading nodes;Disclosed herein are systems and methods for interactive graphical user interfaces (GUIs) that users (e.g., medical professionals) can use to interact with modelled versions of brains and easily and intuitively analyze deep and/or lateral structures in the brain. A user can, for example, selectively view structures and their connectivity data (e.g., nodes and edges) relative to other structures and connectivity data over a representation of a particular patient's brain. Emphasis can be minimized for certain foreground nodes and edges (e.g., lateral structures) to make it easier for the user to focus on and analyze deeper structures that otherwise can be challenging to visualize and understand. A method can include overlaying deep and non-deep nodes on a representation of a brain, displaying the representation of the brain in a GUI, receiving user input indicating interest in focusing on one or more deep nodes, and taking an action based on the input.;0
562;11537416;2021;2022;NTT DATA Services, LLC;NTT DATA Services, LLC;G06F9/451,G06F9/44505,G06F11/3438,G06N3/006,G06N20/00;Detecting and handling new process scenarios for robotic processes;In an embodiment, a method of real-time process monitoring includes initiating monitoring of user interface (UI) activity in a plurality of user environments in which a user-executed process is performed. The user-executed process is defined in a stored instruction set that identifies a plurality of steps of the user-executed process. The plurality of user environments include a first environment operated by a human worker and a second environment operated by a bot. The method also includes, responsive to the initiating, detecting a new process scenario for the user-executed process. The method also includes determining new bot logic for the new process scenario. The method also includes causing the bot to implement the new bot logic.;0
563;11537813;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F18/214,G06N3/0455,G06N3/0475,G06N20/00,G06V10/145,G06V10/147,G06V10/774,G06V10/82,G06V40/1318,G06V40/1347,G06V40/70,G06N3/047,G06N3/084,G06N3/094,G06V40/14;System for synthesizing data;During a training phase, a first machine learning system is trained using actual data, such as multimodal images of a hand, to generate synthetic image data. During training, the first system determines latent vector spaces associated with identity, appearance, and so forth. During a generation phase, latent vectors from the latent vector spaces are generated and used as input to the first machine learning system to generate candidate synthetic image data. The candidate image data is assessed to determine suitability for inclusion into a set of synthetic image data that may be used for subsequent use in training a second machine learning system to recognize an identity of a hand presented by a user. For example, the candidate synthetic image data is compared to previously generated synthetic image data to avoid duplicative synthetic identities. The second machine learning system is then trained using the approved candidate synthetic image data.;3
564;11537902;2020;2022;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N3/0455,G06N3/088,G06F16/245,G06N3/045,G06N3/047;Detecting anomalous events from categorical data using autoencoders;Systems, devices, and methods are provided for detecting anomalous events from categorical data using autoencoders. A system may receive a data set associated with actions requested within the computing environment, wherein the data set includes first categorical data indicative of anomalous activity in the computing environment. The system may train an autoencoder to reconstruct approximations of requests associated with the computing environment based on the received data set, wherein training the autoencoder includes using a beta divergence and a maximum mean discrepancy divergence. The trained system may receive a request to invoke an action within the computing environment, may generate a reconstruction of the request to invoke the action using the trained autoencoder, may determine a normalcy score based on a probability that the reconstruction of the request exists in the training data set, and, based on the calculated normalcy score, may determine whether requests indicate anomalous data.;0
565;11544460;2022;2023;INTUIT INC.;INTUIT INC.;G06F40/279,G06F40/151,G06F40/30,G06N3/044,G06N3/0442,G06N3/045,G06N3/0475,G06N3/094;Adversarial anonymization and preservation of content;Systems and methods for anonymizing content suggestive of a particular characteristic while preserving relevant content are disclosed. An example method may be performed by one or more processors of a protection system and include defining an anonymization loss indicative of an accuracy at which a trained discriminator model can predict a particular characteristic, defining a content loss indicative of a difference between latent representations of versions of a document, defining a combined objective function incorporating the anonymization and content losses, extracting and anonymizing suggestive content from training documents while preserving relevant content, and adversarially training, using the associated accuracies and differences in the combined objective function, a transformation model to transform a given document representative of credentials of a given person possessing the particular characteristic into an anonymized document maximizing a predicted uncertainty of the trained discriminator model while simultaneously maximizing an amount of relevant information about the person preserved.;2
567;11544881;2022;2023;DEEP RENDER LTD.;DEEP RENDER LTD.;G06T9/002,H04N19/124,G06N3/045,G06N3/0455,G06N3/0475,G06N3/0495,G06N3/08,G06N3/084,G06N3/094,G06N7/01,G06T3/4046,H04N19/136,H04N19/17,H04N19/186,G06N3/0464;Method and data processing system for lossy image or video encoding, transmission and decoding;"A method for lossy image or video encoding, transmission and decoding, the method comprising the steps of: receiving an input image at a first computer system; encoding the first input training image using a first trained neural network to produce a latent representation; performing a quantization process on the latent representation to produce a quantized latent; entropy encoding the quantized latent using a probability distribution, wherein the probability distribution is defined using a tensor network; transmitting the entropy encoded quantized latent to a second computer system; entropy decoding the entropy encoded quantized latent using the probability distribution to retrieve the quantized latent; and decoding the quantized latent using a second trained neural network to produce an output image, wherein the output image is an approximation of the input training image.";2
568;11546181;2021;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L12/1822,H04N7/147,H04L12/1845,H04L65/1069,H04L65/403,H04L67/303,H04N7/15,H04W4/021;Contextually driven diversion enhancement framework;An approach is provided in which the approach establishes a geo-fence around a user in response to the user invoking a video conversation corresponding to a conversational context. The approach determines an action in response to detecting an entity crossing the geo-fence. The action is based on the entity and the conversational context of the video conversation. The approach invokes the action by transmitting a message to a device in proximity to the entity.;0
569;11551652;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N3/0475,G10H1/0025,G06N3/045,G06N3/088,G06N3/0455,G06N3/0464,G06N3/047,G10H2210/105,G10H2210/111,G10H2220/106,G10H2220/116,G10H2220/221,G10H2240/081;Hands-on artificial intelligence education service;Indications of sample machine learning models which create synthetic content items are provided via programmatic interfaces. A representation of a synthetic content item produced by one of the sample models in response to input obtained from a client of a provider network is presented. In response to a request from the client, a machine learning model is trained to produce additional synthetic content items.;3
571;11551668;2020;2023;Meta Platforms, Inc.;Meta Platforms, Inc.;G10L15/063,G06F18/2155,G06F18/2413,G10L15/065,G10L15/16,G10L15/26;Generating representations of speech signals using self-supervised learning;In one embodiment, a method includes generating audio segments from a speech signal, generating latent representations that respectively correspond to the audio segments, the latent representations comprising a first subset and a second subset, generating quantized representations that respectively correspond to the latent representations, masking the second subset of the latent representations, using a machine-learning model to process the first subset of the latent representations and the masked second subset of the latent representations to generate contextualized representations that respectively correspond to the latent representations, pre-training the machine-learning model based on comparisons between (1) a subset of the contextualized representations that respectively correspond to the masked second subset of the latent representations and (2) a subset of the quantized representations that respectively correspond to the masked second subset of the latent representations, and training the pre-trained machine-learning model to perform a speech analysis task.;3
572;11551695;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/26,G10L15/063,G10L15/07,G10L2015/0638;Model training system for custom speech-to-text models;A transcription service may receive a request from a developer to build a custom speech-to-text model for a specific domain of speech. The custom speech-to-text model for the specific domain may replace a general speech-to-text model or add to a set of one or more speech-to-text models available for transcribing speech. The transcription service may receive a training data and instructions representing tasks. The transcription service may determine respective schedules for executing the instructions based at least in part on dependencies between the tasks. The transcription service may execute the instructions according to the respective schedules to train a speech-to-text model for a specific domain using the training data set. The transcription service may deploy the trained speech-to-text model as part of a network-accessible service for an end user to convert audio in the specific domain into texts.;1
573;11556335;2021;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F8/73,G06F8/71;Annotating program code;Annotating programming code by receiving a first version of programming code, determining annotations for the code according to a first machine learning model, identifying a user, altering a generative model according a persona of the user, and generating altered annotations using the altered generative model.;3
574;11556722;2022;2023;ONE AI, INC.;ONE AI, INC.;G06F40/51,G06F40/211,G06F40/232,G06F40/253,G06F40/35;System and method for editing transcriptions with improved readability and correctness;Disclosed are a computer implemented method, system and platform for improving the readability and/or coherency of a conversation transcript, which include the applying of a speech disfluency detection model to identify speech disfluencies in a text transcript and to provide a corrected and/or annotated version of the conversation transcript indicating the edits made vis-Ã -vis the inputted text transcript.;3
575;11557292;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06N3/044,G06N3/0464,G06N3/08,G06N3/084,G06N3/09,G10L15/02,G10L15/05,G10L15/16,G10L2015/088,G10L2015/223;Speech command verification;A system and method performs speech command verification to determine if audio data includes a representation of a speech command. A first neural network may process portions of the audio data before and after a representation of a wake trigger in the audio data. A second neural network may process the audio data using a recurrent neural network to determine if the audio data includes a representation of a wake trigger.;0
576;11561326;2019;2023;Acme Atronomatic, LLC;Acme Atronomatic, LLC;G01W1/10,G06N3/045,G06N3/0464,G06N3/047,G06N3/0475,G06N3/094,G06T7/174,G06T7/215,G06T7/248,G06V10/454,G06V10/82,G01W2203/00,G06T2207/10044,G06T2207/20081,G06T2207/20084,G06T2207/30192,Y02A90/10;System and method for generating accurate hyperlocal nowcasts;A computing system includes at least one processor, and a memory communicatively coupled to the at least one processor. The processor is configured to receive at least two successive radar images of precipitation data, generate a motion vector field using the at least two successive radar images, forecast linear prediction imagery of future precipitation using the motion vector field, and generate corrected output imagery corresponding to the forecasted linear prediction imagery of the future precipitation corrected by a first neural network. In addition, the processor is further configured to receive, by a second neural network, the linear prediction imagery, and one of observed imagery and the corrected output imagery, and distinguish, by the second neural network, between the corrected output imagery and the observed imagery to produce conditioned output imagery. The processor is also configured to display the conditioned output imagery on a display.;3
577;11564194;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;H04L41/0226,H04W60/02,H04L41/0213,H04L41/046,H04L41/145,H04L41/16,H04L69/08,H04L69/22,H04W4/70,H04W8/04,H04W8/06,H04W8/18,H04L67/02,H04L67/306,H04L67/34;Device communication;A device-communication system may receive, from a user device via a first network, communication data originating from a first device connected to the user device via a second network having a type different from that of the first network. The device-communication system may process the communication data to determine a corresponding device-management system, and may communicate further with the user device for additional identification information, if necessary. The device-communication system determines which of a plurality of device-management systems should receive the communication data and sends the data to the appropriate system.;0
578;11568148;2018;2023;Narrative Science LLC;Narrative Science LLC;G06F40/237,G06F40/30,G06F40/295,G06F40/35,G06F40/56,G06N5/022,G06N5/041,G06N20/00;Applied artificial intelligence technology for narrative generation based on explanation communication goals;Artificial intelligence (AI) technology can be used in combination with composable communication goal statements to facilitate a user's ability to quickly structure story outlines using âexplanationâ communication goals in a manner usable by an NLG narrative generation system without any need for the user to directly author computer code. This AI technology permits NLG systems to determine the appropriate content for inclusion in a narrative story about a data set in a manner that will satisfy a desired explanation communication goal such that the narratives will express various ideas that are deemed relevant to a given explanation communication goal.;3
579;11568576;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06T11/00,G06V10/82,G06F18/21,G06F18/24,G06T7/11,G06T7/70,G06T11/60,G06V20/36,G06T2207/10016,G06T2207/20084,G06T2207/20132;Generation of synthetic image data;Techniques are generally described for generation of photorealistic synthetic image data. A generator network generates first synthetic image data. A first class of image data represented by a first portion of the first synthetic image data is detected and the first portion is sent to a first discriminator network. The first discriminator network generates a prediction of whether the first portion of the first synthetic image data is synthetically generated. A second class of image data represented by a second portion of the first synthetic image data is detected and the second portion is sent to a second discriminator network. The second discriminator network generates a prediction of whether the second portion of the first synthetic image data is synthetically generated. The generator network is updated based on the predictions of the discriminators.;3
580;11574168;2021;2023;Moffett International Co., Limited;Moffett International Co., Limited;G06N3/045,G06N3/096,G06N3/042,G06N3/0455,G06N3/0475,G06N3/08,G06N3/092;System and method for pivot-sample-based generator training;Methods, systems, and apparatus, including computer programs encoded on computer storage media, for few-shot learning-based generator training based on raw data collected from a specific domain or class. In cases where the raw data is collected from multiple domains but is not easily divisible into classes, the invention describes training multiple generators based on a pivot-sample-based training process. Pivot samples are randomly selected from the raw data for clustering, and each cluster of raw data may be used to train a generator using the few-shot learning-based training process.;3
581;11574624;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L13/10,G10L13/02,G10L13/047,G10L13/08;Synthetic speech processing;A speech-processing system receives input data representing text. An input encoder processes the input data to determine first embedding data representing the text. A local attention encoder processes a subset of the first embedding data in accordance with a predicted size to determine second embedding data. An attention encoder processes the second embedding data to determine third embedding data. A decoder processes the third embedding data to determine audio data corresponding to the text.;4
582;11579841;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F3/167,G10L15/22,G06F9/4881,G10L13/00,G10L15/00,G10L2015/223,G10L2015/228;Task resumption in a natural understanding system;A speech-processing system may provide access to one or more skills via spoken commands and/or responses in the form of synthesized speech. The system may be capable of keeping one or more skills active in the background while a user interacts (e.g., provides inputs to and/or receives outputs from) with a skill running in the foreground. A background skill may receive some trigger data, and determine to request the system to return the background skill to the foreground to, for example, request a user input regarding an action previously requested by the user. In some cases, the user may invoke a background skill to continue a previous interaction. The system may return the background skill to the foreground. The resumed skill may continue a previous interaction to, for example, to query the user for instructions, provide an update or alert, or continue a previous output.;0
583;11580673;2020;2023;Duke University;Duke University;G06T11/00,G06N3/045,G06N3/0455,G06N3/0464,G06N3/0475,G06N3/088,G06N3/094,G16H30/40,G16H50/20,G06T2210/41;Methods, systems, and computer readable media for mask embedding for realistic high-resolution image synthesis;"The subject matter described herein includes methods, systems, and computer readable media for mask embedding for realistic high-resolution image synthesis. According to one method for mask embedding for realistic high-resolution image synthesis includes receiving, as input, a mask embedding vector and a latent features vector, wherein the mask embedding vector acts as a semantic constraint; generating, using a trained image synthesis algorithm and the input, a realistic image, wherein the realistic image is constrained by the mask embedding vector; and outputting, by the trained image synthesis algorithm, the realistic image to a display or a storage device.";3
586;11580968;2019;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F40/35,G10L15/02,G10L15/16,G10L15/18,G10L15/1822,H04L67/02;Contextual natural language understanding for conversational agents;Techniques are described for a contextual natural language understanding (cNLU) framework that is able to incorporate contextual signals of variable history length to perform joint intent classification (IC) and slot labeling (SL) tasks. A user utterance provided by a user within a multi-turn chat dialog between the user and a conversational agent is received. The user utterance and contextual information associated with one or more previous turns of the multi-turn chat dialog is provided to a machine learning (ML) model. An intent classification and one or more slot labels for the user utterance are then obtained from the ML model. The cNLU framework described herein thus uses, in addition to a current utterance itself, various contextual signals as input to a model to generate IC and SL predictions for each utterance of a multi-turn chat dialog.;3
587;11581020;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G11B27/036,G06V40/168,G06V40/174;Facial synchronization utilizing deferred neural rendering;Techniques are disclosed for performing video synthesis of audiovisual content. In an example, a computing system may determine first facial parameters of a face of a particular person from a first frame in a video shot, whereby the video shot shows the particular person speaking a message. The system may determine second facial parameters based on an audio file that corresponds to the message being spoken in a different way from the video shot. The system may generate third facial parameters by merging the first and the second facial parameters. The system may identify a region of the face that is associated with a difference between the first and second facial parameters, render the region of the face based on a neural texture of the video shot, and then output a new frame showing the face of the particular person speaking the message in the different way.;6
588;11582485;2022;2023;Mitsubishi Electric Research Laboratories, Inc.;Mitsubishi Electric Research Laboratories, Inc.;G06V20/46,H04N19/597,G06F16/532,G06T7/11,G06V10/82,G06V10/86,G06V20/647,G06V30/194,H04N19/107,H04N19/179,H04N21/251;Scene-aware video encoder system and method;Embodiments of the present disclosure discloses a scene-aware video encoder system. The scene-aware encoder system transforms a sequence of video frames of a video of a scene into a spatio-temporal scene graph. The spatio-temporal scene graph includes nodes representing one or multiple static and dynamic objects in the scene. Each node of the spatio-temporal scene graph describes an appearance, a location, and/or a motion of each of the objects (static and dynamic objects) at different time instances. The nodes of the spatio-temporal scene graph are embedded into a latent space using a spatio-temporal transformer encoding different combinations of different nodes of the spatio-temporal scene graph corresponding to different spatio-temporal volumes of the scene. Each node of the different nodes encoded in each of the combinations is weighted with an attention score determined as a function of similarities of spatio-temporal locations of the different nodes in the combination.;0
589;11582519;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06V40/174,H04N21/4666,G06V10/82,G06V20/41,G06V20/46,G06V40/10,G06V40/161,G06V40/168;Person replacement utilizing deferred neural rendering;Techniques are disclosed for performing video synthesis of audiovisual content. In an example, a computing system may determine first parameters of a face and body of a source person from a first frame in a video shot. The system also determines second parameters of a face and body of a target person. The system determines that the target person is a replacement for the source person in the first frame. The system generates third parameters of the target person based on merging the first parameters with the second parameters. The system then performs deferred neural rendering of the target person based on a neural texture that corresponds to a texture space of the video shot. The system then outputs a second frame that shows the target person as the replacement for the source person.;3
590;11586724;2020;2023;Authidote LLC;Authidote LLC;G06F21/51,G06F21/64,G06F21/54,G06N20/00,G09C5/00,H04L9/0643,H04L9/3239,G06N3/045,G06N3/0464,G06N3/0475,G06N3/094;System and methods for authenticating content;The invention relates generally to the field of content authentication, and more particularly, to a system and methods for verifying the authenticity of content output to a user. In certain preferred embodiments, the content is verified by identifying the source data of the content, distributing the content, and authenticating the distributed content. Where the content has not been changed, the system may authenticate the content using a cryptographic hash. When minor changes to the content are made, the system may use a perceptual hash to authenticate the content. Further, the system may utilize machine learning algorithms to identify patterns between the same content in, for example, multiple formats and sizes. Advantageously, the content that is uploaded to the system may be used to train machine-learning models that the system may use to authenticate content that has been converted but unmanipulated.;0
591;11600260;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/063,G06Q20/12,G06Q30/015,G06Q30/0631,G06Q30/0641,G10L15/16,G10L15/183,G10L15/22,G10L2015/223,G10L2015/227;Utterance generation and evaluation;Devices and techniques are generally described for generating and evaluating utterances. In some examples, an utterance generation and evaluation system can receive intent data and target data. The utterance generation and evaluation system can determine related target names and related intent names and, based on the related target names and related intent names, can generate an utterance phrase. The utterance generation and evaluation system can determine a confidence score associated with the utterance phrase and, based on the confidence score, determine the utterance phrase as a recommended utterance phrase.;3
592;11600263;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;A63F9/24,G10L15/18,A63F11/0074,G06F18/00,G06F40/20,G06F40/40,G06T1/00,G06T7/00,G06T7/13,G06V20/52,G06V20/60,G06V30/10,G06V30/224,G06V40/172,G09B19/22,A63F1/00,A63F3/00643,A63F9/04,A63F2009/2476,A63F2011/0086,G06F40/30,G10L15/26;Natural language configuration and operation for tangible games;This disclosure describes a tabletop game assistant system configured to ingest and guide tangible games (such as board games, card games, etc.) using natural language interaction and image capture/visual display components. The system can include features enabling a game developer to âteachâ the system the rules of a game using natural language, such as written instructions, to reduce or eliminate the need for writing dedicated code. The system may process images of a game board and/or tokens such as game pieces and/or cards to further generate game data in the form of a logical game model. The system can use the game data to guide human players of the game and, in some cases, participate as a player itself. The system may further be configured to observe a game and detect invalid actions, answer questions regarding the rules, and suggest moves. The system may provide additional utilities such as generating a random output (e.g., rolling virtual dice) and learning to recognize new game pieces.;6
593;11604966;2020;2023;COGNIZANT TECHNOLOGY SOLUTIONS U.S. CORPORATION;COGNIZANT TECHNOLOGY SOLUTIONS U.S. CORPORATION;G06N3/045,G06F16/9027,G06F17/16,G06N3/0475,G06N3/08,G06N3/086,G06N3/126,G06N3/048;Generative adversarial network optimization;"A process for discovering optimal Generative Adversarial Networks (GAN) includes jointly optimizing the three functions of a GANs process including (i) a real component of a discriminator network's loss that is a function of D(x), wherein D(x) is the discriminator network's output for a real sample from an input dataset; (ii) a synthetic component of the discriminator network's loss that is a function of D(G(z)), wherein D(G(z)) is the discriminator network's output for a generator network's synthetic samples z from a latent distribution; and (iii) a generator network's loss which is a function of D(G(z)), with the discriminator network's total loss being the sum of components (i) and (ii). And separately optimizing each of the three functions in accordance with a process for evolving candidate loss functions having tree format by a genetic algorithm and optimizing selected candidate loss functions by optimizing set of coefficients of each of the one or more best candidate loss functions, wherein the set of coefficients is represented by a vector with dimensionality equal to the number of nodes in the best candidate loss function's tree.";3
595;11605387;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/32,G10L13/033,G10L15/1815,G10L15/22,G10L2015/223,G10L2015/228;Assistant determination in a skill;"A speech-processing system may provide access to multiple virtual assistants. Speech-processing systems may perform actions for or on behalf of users with the aid of skills; e.g., a shopping skill, navigation skill, communications skill, etc. Some skills may be associated with more than one assistant. The speech-processing system may determine which assistant to invoke upon receiving a command from a user device. The identity of the virtual assistant is propagated to the skill and the device, as well as other components of the speech-processing system. In some cases, however, a multi-assistant skill may determine that an assistant other than the one initially selected by the speech-processing system is to handle the command. The skill may send the identity of the new assistant back to the speech-processing system. The speech-processing system may restart the command dissemination process to provide each component of the system with the updated assistant identity.";1
596;11605388;2020;2023;Electronic Arts Inc.;Electronic Arts Inc.;G10L17/04,G10L21/007,A63F13/215,A63F13/355,A63F13/424,A63F13/54,A63F13/60,G10L15/005,G10L2021/0135;Speaker conversion for video games;This specification describes a computer-implemented method of generating speech audio for use in a video game, wherein the speech audio is generated using a voice convertor that has been trained to convert audio data for a source speaker into audio data for a target speaker. The method comprises receiving: (i) source speech audio, and (ii) a target speaker identifier. The source speech audio comprises speech content in the voice of a source speaker. Source acoustic features are determined for the source speech audio. A target speaker embedding associated with the target speaker identifier is generated as output of a speaker encoder of the voice convertor. The target speaker embedding and the source acoustic features are inputted into an acoustic feature encoder of the voice convertor. One or more acoustic feature encodings are generated as output of the acoustic feature encoder. The one or more acoustic feature encodings are derived from the target speaker embedding and the source acoustic features. Target speech audio is generated for the target speaker. The target speech audio comprises the speech content in the voice of the target speaker. The generating comprises decoding the one or more acoustic feature encodings using an acoustic feature decoder of the voice convertor.;3
597;11631208;2022;2023;RealizeMD Ltd.;RealizeMD Ltd.;G06T11/00,G06T11/60,G06N3/045,G06N3/047,G06N3/088,G06V10/48,G06V10/82,G06V40/168,G06V40/171,G06V40/174;Systems and methods for generating clinically relevant images that preserve physical attributes of humans while protecting personal identity;A computer implemented method of generating at least one anonymous image, comprises: extracting and preserving at least one real facial region from at least one real image of a real human face, and generating at least one anonymous image comprising a synthetic human face and the preserved at least one real facial region, wherein an identity of the real human face is non-determinable from the at least one anonymous image.;3
598;11631237;2022;2023;Northeast Electric Power University;Northeast Electric Power University;G06V10/764,G06V10/82,G06V20/52,G06V20/60,Y02E10/50;Infrared thermal image classification and hot spot positioning method of photovoltaic panel;"Disclosed is an infrared thermal image classification and hot spot positioning method of a photovoltaic panel, comprising following steps: constructing a photovoltaic panel infrared thermal image data set, preprocessing the image data set, and dividing into a training set and a testing set according to a preset proportion; training an auxiliary generating countermeasure network based on the training set to obtain a trained generator and a trained discriminator, training an encoder by combining the image data in the training set with the generator, and fixing the parameters of a trained encoder; inputting the image data in the test set into a trained discriminator to obtain a photovoltaic image classification result; inputting images in the test set into the trained encoder, and then inputting the images into the generator for a reconstruction; comparing pixels of the input images with the reconstructed infrared thermal images, and positioning hot spots.";3
599;11640493;2022;2023;ActionPower Corp.;ActionPower Corp.;G06F40/166,G06F40/35,G06F16/3329,G06F16/345,G06F16/9024,G06F40/284,G06F40/56;Method for dialogue summarization with word graphs;"Disclosed is a method for dialogued summarization with word graphs, which is performed by one or more processors of a computing device. The method may include: generating a word graph based on information on a dialogue which is a summary target; extracting at least one keyword based on the generated word graph; generating a plurality of candidate summary sentences based on the generated word graph; and calculating a score associated with at least one keyword for each of the plurality of candidate summary sentences, and selecting at least one of the plurality of candidate summary sentences based on the calculated score.";4
600;11641384;2022;2023;MindwareWorks Co., Ltd.;MindwareWorks Co., Ltd.;H04L51/046,H04L65/1096,H04L51/02,H04L65/1104,H04L51/214;System for linking chatbot services and contact center services;Disclosed is a system for linking chatbot services and contact center services, and more particularly to a system for linking chatbot services and contact center services, in which a client is allowed to receive a query processing service from at least one of other chatbot servers or other contact center servers without changing a preexisting connection state while being initially connected to a specific chatbot server or a specific contact center server, so that a user can easily use a plurality of query processing services by linking the chatbot servers and contact center servers operated by a plurality of service providers, thereby minimizing the time, efforts and costs of the user who wants to receive the plurality of query processing services through the plurality of service providers.;0
601;11644961;2022;2023;ADOBE INC.;ADOBE INC.;G06F3/04842,G06F3/04845,G06F3/0482,G06F40/106,G06F40/284,G06T7/60;Utilizing a transformer-based generative language model to generate digital design document variations;The present disclosure relates to systems, non-transitory computer-readable media, and methods for utilizing a design language model and a generative language model to generate digital design documents with design variations. In particular embodiments, the disclosed systems implement the design language model to tokenize the design of a document into a sequence of language tokens. For example, the disclosed systems tokenize visual elements and a layout of the documentâin addition to optional user-added content. The generative language model utilizes the sequence of language tokens to predict a next language token representing a suggested design variation. Based on the predicted language token, the disclosed systems generate a modified digital design document visually portraying the suggested design variation. Further, in one or more embodiments, the disclosed systems perform iterative refinements to the modified digital design document.;5
602;11645479;2020;2023;;;G06F40/35,G06F40/58;Method for AI language self-improvement agent using language modeling and tree search techniques;A novel method provides an AI language virtual agent having self-improvement features and which uses language modeling and tree search techniques. The AI language virtual agent exchanges textual discussion with users and other simulated agents. The method includes receiving a current situational description depicting natural language user input, temperament qualities and textual tendencies of the virtual agent, and indicia regarding subject matter context of a present conversation. The indicia regarding subject matter context include textual logs from recent conversational exchanges. The current situational description includes audio, visual, and tactile inputs collected proximate to the virtual agent. The method preferably utilizes an MCTS tree search in combination with self-moving modules, one or more language models, tree search techniques outputting textual responses to the current situation description, and the virtual agent responding with textual expression to verbal input in combination with the audio, visual, tactile, and other sensory inputs.;5
604;11646007;2022;2023;DAACI LIMITED;DAACI LIMITED;G10H1/0025,G10H1/0041,G10H1/38,G10H2210/111,G10H2210/121,G10H2210/555,G10H2210/576,G10H2240/075,G10H2240/085,G10H2240/145,G10H2250/015;Generative composition with texture groups;A computer-implemented method of generating a musical composition containing a plurality of musical texture groups is disclosed. The method includes assembling musical texture groups from musical instrument components and associating therewith a tag expressing emotional textural connotation. The instrument components have musical textural classifiers selected from a set of pre-defined textural classifiers such that different instrument components may have a different subset of pre-defined textural classifiers. The textural classifiers within a texture group possess either no musical feature attribute or a single musical feature attribute and any number of musical accompaniment attributes. The method then generates at least one chord scheme to a narrative brief, to provide an emotional connotation to a series of events, the chord scheme generated by selecting and assembling Form Atoms. The final step includes applying a texture to the chord scheme to generate the musical composition reflecting the narrative brief.;3
605;11647153;2022;2023;Dell Products L.P.;Dell Products L.P.;H04N7/0117,G06T3/4046,G06V10/774,G06V20/46,G06V20/49,G06V10/82;Computer-implemented method, device, and computer program product;Embodiments of the present disclosure relate to a computer-implemented method, a device, and a computer program product. A method includes: determining a first group of frames corresponding to a first scene and a second group of frames corresponding to a second scene different from the first scene in a reference video, respectively. The first group of frames and the second group of frames each have a first resolution. The method further includes determining a first model for the first scene and a second model for the second scene, respectively. The first model and the second model are respectively used to convert frames corresponding to the first scene and the second scene from the first resolution to a second resolution different from the first resolution. The method further includes training the first model and the second model using the first group of frames and the second group of frames, respectively.;3
606;11651390;2021;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06Q30/0244,G06N3/044,G06N20/00,G06N3/084;Cognitively improving advertisement effectiveness;A computer-implemented method includes one or more processors configured for receiving ad scoring data corresponding to a biometric response to an advertisement displayed on a computing device user interface at a first time, receiving ad feature data corresponding to the advertisement, and generating first model output data corresponding to a user engagement score based on ad scoring data and ad feature data. Further, the method includes receiving user interface activity data corresponding to graphical images displayed at the first time, generating second model output data corresponding to a watching ad classification based on the user interface activity data and the ad feature data, determining an ad effectiveness score based on the first model output data and the second model output data, and generating ad improvement data based on the ad effectiveness score and the ad feature data, the ad improvement data comprising recommendations to adjust features of the advertisement.;3
608;11657292;2020;2023;ARCHITECTURE TECHNOLOGY CORPORATION;ARCHITECTURE TECHNOLOGY CORPORATION;G06N3/088,G06N3/094,G06N3/045,G06N3/0455;Systems and methods for machine learning dataset generation;Disclosed herein are embodiments of systems, methods, and products comprising an analytic server that automates training dataset generation for different application areas. The server may perform an automated, iterative refinement process to build a collection of dataset generator models over time. The server may receive a set of seed examples in a domain and generate candidate examples based on the features of the seed examples using data synthesis techniques. The server may execute a pre-trained label discriminator (LD) and domain discriminator (D2) on the candidate examples. The LD may identify and reject mislabeled data. The D2 may identify and reject out of domain data. The analytic server may regenerate new labeled data based on the feedback of the LD and D2. The analytic server may train a dataset generator by iteratively performing these steps for refinement until the regenerated candidate examples reach a pass rate threshold.;3
609;11657598;2022;2023;SIMPLE INTELLIGENCE, INC.;SIMPLE INTELLIGENCE, INC.;G06V10/772,G06F16/951,G06N3/0455,G06N3/0475,G06N3/088,G06N3/094,G06V10/26,G06V10/762,G06V10/774,G06V10/82,G06V20/70,G06N3/0464,G06V2201/08;Composite car image generator;The present disclosure relates generally to artificial intelligence (AI), machine learning (ML), and deep learning technologies. More specifically, the disclosure relates to a vehicle image composite system that employs computer vision (CV) along with a Generative Adversarial Network (GAN) to generate realistic composite car images. For example, in one or more embodiments, the composite car image generator system trains a Convolutional Neural Network (CNN) to learn the Make Model Year parameters of all vehicle images provided. Once trained, the determined Make Model Year parameters of the vehicles allow the CNN to produce realistic composite images of a vehicle of any make, model, year, and trim level.;5
610;11659085;2021;2023;United Services Automobile Association (USAA);United Services Automobile Association (USAA);H04M3/493,H04M3/5166,G10L13/033,H04M3/4936,H04M3/5175,H04M3/5183,H04M3/5237,G10L2021/0135,H04M2203/305;Generative voice for automated bot handoff to customer service representative;A system and method for applying a generative voice associated with a particular customer service representative to an automated bot that initially interacts with a customer to provide a seamless handoff between the automated bot and the particular customer service representative is described. In one embodiment, when a call from a customer is received at the customer service call center, the customer is matched with a potential customer service representative that is likely to handle the customer's call. The customer will then initially interact with an automated bot that has applied a generative voice associated with the likely customer service representative. The customer can talk with the automated bot using the generative voice and, if needed, when the call is handed off from the automated bot to the customer service representative, the customer will not notice a change in voice or other discontinuity on the call.;3
612;11675817;2021;2023;Wells Fargo Bank, N.A.;Wells Fargo Bank, N.A.;G06F16/285,G06F21/6245,G06N3/0455,G06N3/047,G06N3/0475,G06N3/094,G06N5/046,G06N20/00;Synthetic data generation;Methods, apparatuses, and computer program products are disclosed for synthetic data generation and dynamic processing model selection. An example method includes receiving a request for synthetic data generation where the request for synthetic data generation includes one or more configuration data parameters. The method further includes selecting at least one processing model from amongst a plurality of processing models based upon the one or more configuration data parameters. The method also includes generating one or more synthetic datasets that include one or more synthetic data values via the selected at least one processing model. The method further includes providing, to a user via a user interface, the one or more generated synthetic datasets.;2
613;11676044;2022;2023;Ada Support Inc.;Ada Support Inc.;G06N5/022,G06N3/042,G06N3/09,G06N5/041,G06N5/043,G06N3/0455;Systems and methods for generating a chatbot;Systems and methods for generating a chatbot are disclosed. Source data is identified. A first chunk of the source data is also identified. A first machine learning model is executed for automatically generating a first candidate question associated with the first chunk. A determination is made as to whether the first candidate question satisfies a criterion. The first candidate question is output as training data for training the chatbot in response to the determination.;2
614;11676163;2022;2023;ROSETAL SYSTEM INFORMATION LTD.;ROSETAL SYSTEM INFORMATION LTD.;G06Q30/0204,G06Q30/0645,G06Q40/03,G06Q40/08,G06Q50/16,G06Q50/18;System and method for determining a likelihood of a prospective client to conduct a real estate transaction;"Disclosed are a computer implemented method and system for determining a likelihood of a prospective client to engage in a real estate transaction, by obtaining and/or retrieving one or more characteristics of the prospective client; extracting data regarding a digital interaction behavior of the prospective client; deriving from the retrieved/extracted data one or more digital interaction features of the prospective client directly or indirectly associated with real estate, applying a machine learning algorithm on the derived one or more digital interaction features and on the prospective client's characteristics to determine a probability, a range of probabilities or a category of likelihood of the prospective client to engage in the real-estate transaction.";0
616;11677991;2022;2023;AnyClip Ltd.;AnyClip Ltd.;H04N21/234,H04N21/8549,G06F16/739,G06F16/7844,G06F40/30,G06F40/40,G10L15/26,H04N21/23418,H04N21/8547,H04N21/8586;Creating automatically a short clip summarizing highlights of a video stream;Disclosed herein are methods, and program products for creating automatically a short video clip summarizing highlights of a long video stream, comprising identifying a plurality of topics in a video stream based on analysis of the video stream's content, extracting a plurality of sentences based on analysis of a textual representation of the content, computing a score for each of the sentences indicating a relation of the respective sentence to each of the topics, selecting a plurality of sentence subsets each comprising one or more sentences having a highest score with respect to a receptive one of the topics, selecting a plurality of video sections of the video stream each mapped to the one or more sentences of a respective sentence subset, and creating a video clip by merging the plurality of video sections each relating to one of the plurality of topics.;3
617;11687152;2022;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F3/011,A61B5/1118,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G16Y40/10;Directional recommendations based on movement tracking while performing an activity;A computer-implemented method is disclosed which includes determining a current activity of an individual, comparing movements of the individual while performing the current activity to corresponding movements in a reference activity, determining whether a performance level of respective movements of the individual while performing the current activity are within a predetermined performance range for corresponding movements in the reference activity, generating, via a generative adversarial network (GAN), a directional guidance recommendation based, at least in part, on: a comparative analysis of the movement of the individual while performing the current activity and the corresponding movement included in the reference activity, and a degree of deviation between the performance level of the at least one movement of the individual and an optimal performance level of the corresponding movement included in the reference activity, and displaying the generated directional guidance recommendation to the individual.;5
618;11688100;2021;2023;Apple Inc.;Apple Inc.;G06T7/80,G06N3/04,G06N3/0455,G06N3/047,G06N3/0895,G06T5/50,G06T5/60,G06T9/002,H04N23/45,H04N23/62,H04N23/951,G06N3/0464,G06N3/0475,G06T2207/10021,G06T2207/10024,G06T2207/20016,G06T2207/20021,G06T2207/20052,G06T2207/20081,G06T2207/20084,G06T2207/20221;Systems and methods for multi-sensor image enhancement;"Devices, methods, and non-transitory program storage devices are disclosed to provide enhanced images in multi-camera systems, e.g., by using information from images captured by cameras with different properties in terms of optics and/or sensors. In one embodiment, the techniques comprise: obtaining a first image from a first image capture device, wherein the first image has a first field of view (FOV) and a first set of quality characteristics; obtaining a second image from a second image capture device, wherein the second image has a second FOV and a second set of quality characteristics, and wherein the second FOV partially overlaps the first FOV; obtaining a neural network that produces a modified second image having a modified second set of quality characteristics determined by the neural network attempting to match the first set of quality characteristics; and generating an output image based, at least in part, on the modified second image.";3
619;11689486;2022;2023;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F40/30,H04L51/21,G06F18/22,G06F40/237,G06F40/279,G06F40/35,H04L51/02,H04L51/214,H04L51/18;Topic overlap detection in messaging systems;Embodiments are provided for detecting overlapping topics in a messaging system. In an example system, a plurality of trigger phrases is received, where each trigger phrase is configured to trigger a bot that receives the trigger phrase to select a corresponding topic for conversation. For each trigger phrase, a vector representation is generated. Measures of similarity are generated based at least on the vector representations, where each measure of similarity represents a degree of similarity between a respective pair of vector representations. A topic overlap is detected based on a pair of vector representations having a measure of similarity above a similarity threshold, where the topic overlap indicates two trigger phrases that are overlapping. The topic overlap is provided to an authoring tool that comprises one or more interactive elements to enable a user to change at least one of the two trigger phrases that are overlapping.;0
620;11689601;2022;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04L65/80,G06N3/0475,H04L65/61,H04L65/762,H04L65/765,H04N21/23418,H04N21/234354,H04N21/234363,H04N21/2402,H04N21/251,H04N21/25841,H04N21/647,G06N3/094;Stream quality enhancement;Aspects of the present disclosure relate to stream quality enhancement. A determination can be made that quality of an audio/video (A/V) stream being streamed to a first device falls below a quality threshold. A plurality of edge processing devices in an environment of the first device can be identified. Available computing resources of each of the edge processing devices can be determined. At least one edge processing device of the plurality of edge processing devices with sufficient computing resources for correcting quality of the A/V stream using a correction model can be selected. The selected at least one edge processing device can be instructed to correct the A/V stream using the correction model to generate a corrected A/V stream that satisfies the quality threshold. A command to transmit the corrected A/V stream to the first device can be issued.;3
621;11694039;2021;2023;WALGREEN CO.;WALGREEN CO.;G06F40/35,G06F40/30,G06Q10/08,G06Q10/107,G06Q30/016,G06Q30/0207,G06Q30/0281,G06Q30/0631,G06Q30/0633,G06Q30/0639,G10L15/1815,G10L15/22,G06F3/167,G10L2015/225,G10L2015/227,G10L2015/228;Intelligent automated order-based customer dialogue system;Based on a detection that a customer has arrived at an enterprise location to pick up a previously-placed order, an intelligent automated customer dialogue system generates an interface via which an intelligent customer dialogue application dialogues with the customer. The application generates and initially offers, at the interface using natural language, content which is contextual to one or more items of the order, e.g., by using a specially trained intelligent dialogue machine learning model. The application may intelligently respond to the customer's natural language responses and/or requests to refine, augment, or redirect subsequently-offered content and/or dialogue, e.g., by using the model. Offered content (e.g., product information, services, coupons, suggestions, recommendations, etc.) generally provides value-add to the customer as well as maintains customer engagement. The system may be implemented at least partially by using a chatbot upon curbside pick-up, for example, as well as through other electronic customer facing channels.;3
622;11699027;2022;2023;Salesforce Inc.;Salesforce Inc.;G06F40/106,G06F40/166,G06F16/345,G06F40/169,G06F40/30,G06F40/56,G09B7/02;Systems and methods for a collaborative reading assistance tool;Embodiments described herein provide methods and systems for presenting a document and generating a human-AI summary. A system provides a user with a selection of an amount of time to spend reading the document, or a list of questions from which the user may select which questions they would like answered by reading the document. The system highlights sections of the document according to the user selection. Implicit and explicit user data such as dwell times, user highlights, and user notes, are collected while displaying the document. A human-AI summary is generated based on the document and the user data.;2
624;11706492;2022;2023;GENESIS LAB, INC.;GENESIS LAB, INC.;H04N21/4666,H04N21/4826,G06F18/21355,G06F18/22,G06N3/08,G06V10/235,G06V10/454,G06V10/761,G06V10/82,G06V10/95,H04N21/251,H04N21/4668,G06N3/0442,G06N3/0455,G06N3/0464,G06V40/175;Method, server and computer-readable medium for recommending nodes of interactive content;Disclosed are a method, a server and a computer-readable medium for recommending nodes of an interactive content, in which, when receiving recommendation request information for requesting a recommendation node for a specific node included in an interactive content from a user generating the interactive content, a first embedding value for a first set including the specific node is calculated, and a second embedding value for each second set including each of a plurality of nodes of each of one or more other interactive contents included in the service server is calculated, so as to calculate a similarity between the first embedding value and the second embedding value and provide the user with a next node, as a recommendation node, of a node corresponding to the second embedding value determined based on the similarity.;3
627;11709989;2022;2023;Ada Support Inc.;Ada Support Inc.;G06F40/10,G06F16/313,G06F16/345,G06F16/38,G06F40/30,G06N3/044,G06N3/045,G06N3/08,G06N3/084,G06F40/216,G06F40/284,G06N20/00;Method and system for generating conversation summary;Methods and systems for generating and using a conversation summary model. The method comprises receiving at least one training dataset. The at least one training dataset comprises data samples, each data sample comprising a text comprising text segments. The text is labelled with a conversation summary comprising any of the text segments which summarize the text. The at least one training dataset includes a dataset from a specific source. Using the at least one training dataset and the pre-trained model, the method further comprises generating the conversation summary model by fine-tuning the pre-trained model. The generated conversation summary model may be used to generate conversation summaries for chat conversations.;2
628;11710420;2019;2023;X Development LLC;X Development LLC;G09B19/00,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06N3/088,G06N7/01,G16H20/70,G16H50/20,G16H50/30,G16H50/70;Derivative content creation using neural networks for therapeutic use;A technique for dynamic generation of a therapeutic derivative story includes obtaining attribute data that describes characteristics of a content consumer along with situational details describing an emotional situation involving the content consumer. A relatability score for the therapeutic derivative story is determined. A content data structure (CDS) is selected. The CDS specifies story elements of a preexisting story. The story elements are associated with metadata constraints that constrain modification or use of the story elements. The metadata constraints indicate whether associated ones of the story elements are mutable story elements. One or more of the mutable story elements are adapted based on the attribute data or the situational details as constrained by the metadata constraints and to an extent determined at least in part by the relatability score to generate the therapeutic derivative story.;3
629;11710479;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/183,G06F40/30,G06N3/0442,G06N3/045,G06N3/09,G10L15/063,G10L15/1815,H04L51/02,G10L15/1822;Contextual biasing of neural language models using metadata from a natural language understanding component and embedded recent history;"Techniques for implementing a chatbot that utilizes context embeddings are described. An exemplary method includes determining a next turn by: applying a language model to the utterance to determine a probability of a sequence of words, generating a context embedding for the utterance based at least on one or more of: a dialog act as defined by a chatbot definition of the chatbot, a topic vector identifying a domain of the chatbot, a previous chatbot response, and one or more slot options; performing neural language model rescoring using the determined probability of a sequence of words as a word embedding and the generated context embedding to predict an hypothesis; determining at least a name of a slot and type to be fulfilled based at least in part on the hypothesis and the chatbot definition; and determining a next turn based at least in part on the chatbot definition, any previous state, and the name of the slot and type to be fulfilled.";3
630;11714877;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06F18/214,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G06N3/088,G06V10/145,G06V10/772,G06V10/774,G06V10/82,G06V10/993,G06V40/107,G06V40/1318,G06V40/1347,G06V40/70,G06V10/776;System for training of recognition system using ad hoc training data;A machine learning system to determine an identity of a user is trained using triplets of ad hoc synthetic data and actual data. The data may comprise multimodal images of a hand. Each triplet comprises an anchor, a positive, and a negative image. Synthetic triplets for different synthesized identities are generated on an ad hoc basis and provided as input during training of the machine learning system. The machine learning system uses a pairwise label-based loss function, such as a triplet loss function during training. Synthetic triplets may be generated to provide more challenging training data, to provide training data for categories that are underrepresented in the actual data, and so forth. The system uses substantially less memory during training, and the synthetic triplets need not be retained further reducing memory use. Ongoing training is supported as new actual triplets become available, and may be supplemented by additional synthetic triplets.;2
631;11721023;2022;2023;HeHealth PTE Ltd.;HeHealth PTE Ltd.;G06T7/0014,A61B5/0077,A61B5/43,A61B5/7264,G06T5/20,G06T5/70,G06T7/0012,G06T7/194,G06V10/764,G06V10/774,G06V20/70,G16H30/40,G16H50/20,G06T2207/20021,G06T2207/20081,G06T2207/20084,G06T2207/20132,G06T2207/20212,G06T2207/30004,G06V2201/03;Distinguishing a disease state from a non-disease state in an image;Disclosed herein are system, method, and computer program product embodiments for distinguishing a disease state from a non-disease state in an image. An embodiment operates by receiving an image of a target area over a network. The embodiment then corrects for background noise in the image by applying a semantic segmentation filter to obtain a segmented image. The sematic segmentation filter may be trained to remove the background noise from the image. The embodiment then determines, using a trained artificial intelligence (AI) model and the segmented image, at least one classification for the target area. The embodiment finally causes the display of the at least one classification and disease information on a user device associated with a user. The trained AI model may be trained using at least augmented images obtained from a set of images to correct for at least an imbalance in the set of images.;0
632;11721325;2022;2023;ActionPower Corp.;ActionPower Corp.;G10L15/063,G06F40/284,G10L15/01;Method for generating learning data for speech recognition error detection;"Disclosed is a method for generating data, the method is performed by one or more processors of a computing device. The method may include: segmenting text data generated based on speech information into a token unit; generating a first feature vector based on the text data segmented into the token unit, and generating a first label vector corresponding to the generated first feature vector, and generating a second feature vector and a second label vector by performing mix-up for each of the generated first feature vector and the generated first label vector.";2
633;11726902;2021;2023;NTT DATA Services, LLC;NTT DATA Services, LLC;G06F11/3688,G06F11/0772,G06F11/3636,G06F11/3664,G06F11/3684,G06F11/3692,G06N20/00;System and method for automated bot testing scenario simulations;In an embodiment, a method includes receiving information identifying an input bot for testing. The method also includes detecting a functionality performed by the input bot. The method also includes creating a plurality of inputs for simulation of the functionality of the input bot. The method also includes executing the input bot a plurality of times using a same sample of actions. The method also includes checking for consistency of at least one of behavior and output for the same sample of actions. The method also includes executing the input bot a plurality of times using different samples of actions. The method also includes generating an execution plan for the input bot. The method also includes automatically validating the input bot, where the validation results in an automated determination of whether the input bot is defective.;0
634;11727596;2021;2023;Meta Platforms Technologies, LLC;Meta Platforms Technologies, LLC;G06T7/73,G06T7/75,G06N3/045,G06N3/08,G06N20/20,G06T7/194,G06T7/20,G06N3/048,G06T2207/10016,G06T2207/20081,G06T2207/20084,G06T2207/20221,G06T2207/30221;Controllable video characters with natural motions extracted from real-world videos;A video generation system is described that extracts one or more characters or other objects from a video, re-animates the character, and generates a new video in which the extracted characters. The system enables the extracted character(s) to be positioned and controlled within a new background scene different from the original background scene of the source video. In one example, the video generation system comprises a pose prediction neural network having a pose model trained with (i) a set of character pose training images extracted from an input video of the character and (ii) a simulated motion control signal generated from the input video. In operation, the pose prediction neural network generates, in response to a motion control input from a user, a sequence of images representing poses of a character. A frame generation neural network generates output video frames that render the character within a scene.;3
635;11727618;2022;2023;xNeurals Inc.;xNeurals Inc.;G06T13/40,G06T13/80,G06F40/40,G10L15/26,G10L21/10,G06F40/284,G06F40/289,G06F40/30,G10L25/30;Artificial intelligence-based system and method for generating animated videos from an audio segment;An AI-based system and method for generating animated videos from an audio segment is disclosed. The method includes receiving a first audio segment including a description of one or more characters and a scenery for the one or more characters, and a second audio segment including a character speech to be spoken by each of the one or more characters by using one or more expressions. The method includes generating a character image for each of the one or more characters, extracting one or more character sounds and one or more character phrases from the second audio segment, and obtaining one or more prestored video clips from an external database. Furthermore, the method includes generating one or more character video clips and a final character video, such that the final character video may be outputted on user interface screen of one or more electronic devices.;5
637;11735158;2021;2023;Electronic Arts Inc.;Electronic Arts Inc.;G10L13/027,G10L13/033,G10L13/047,G10L25/30;Voice aging using machine learning;"This specification describes systems and methods for aging voice audio, in particular voice audio in computer games. According to one aspect of this specification, there is described a method for aging speech audio data. The method comprises: inputting an initial audio signal and an age embedding into a machine-learned age convertor model, wherein: the initial audio signal comprises speech audio; and the age embedding is based on an age classification of a plurality of speech audio samples of subjects in a target age category; processing, by the machine-learned age convertor model, the initial audio signal and the age embedding to generate an age-altered audio signal, wherein the age-altered audio signal corresponds to a version of the initial audio signal in the target age category; and outputting, from the machine-learned age convertor model, the age-altered audio signal.";3
638;11735178;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L15/08,G10L15/30,G10L2015/088,G10L2015/223;Speech-processing system;A user device may include a first and second wakeword-detection components. If the first wakeword-detection component determines that first audio data includes a representation of a first wakeword, the user device communicates with a first speech-processing system corresponding to the first wakeword. While the communication is ongoing, if a second wakeword-detection component determines that second audio data includes a representation of a second wakeword, the second audio data is not send to a corresponding second speech-processing system.;0
639;11736556;2022;2023;UiPath, Inc.;UiPath, Inc.;H04L67/025,B25J9/1671,G06F3/0482,G06F3/0484,G06F9/54,H04L67/02,H04L67/12,H04L67/141;Systems and methods for using a browser to carry out robotic process automation (RPA);In some embodiments, a robotic process automation (RPA) agent executing within a browser window/tab interacts with an RPA driver executing outside of the browser. A bridge module establishes a communication channel between the RPA agent and the RPA driver. In one exemplary use case, the RPA agent receives a robot specification from a remote server, the specification indicating at least one RPA activity, and communicates details of the respective activity to the RPA driver via the communication channel. The RPA driver identifies a runtime target for the RPA activity within the target web page and executes the respective activity.;0
640;11740372;2022;2023;Institute of Geology and Geophysics, Chinese Academy of Sciences;Institute of Geology and Geophysics, Chinese Academy of Sciences;G01V1/282,G01V1/30,G01V1/302,G01V1/306,G01V2210/614,G01V2210/62,G01V2210/624,G01V2210/63,Y02P90/70;Method and system for intelligently identifying carbon storage box based on GAN network;"The present disclosure belongs to the field of capture, utilization, and storage of carbon dioxide, particularly relates to a method and system for intelligently identifying a carbon storage box based on a GAN network, and aims at solving the problem that the analysis accuracy of a fault zone area in the prior art is insufficient. The method comprises the steps: delineating seismic waveform data of a stable sedimentary area through a GAN network, and removing seismic waveform data points in the fault zone area; obtaining a stable sedimentary background seismic waveform data invertomer; obtaining a three-dimensional wave impedance prediction data volume; making a difference to obtain an abnormal wave impedance data volume; retaining abnormal wave impedance data of fault-karst in the three-dimensional variance attribute volume to obtain a fault-karst wave impedance data volume; and then obtaining a carbon storage box interpretation model.";2
641;11740862;2022;2023;ALGORIDDIM GMBH;ALGORIDDIM GMBH;G06F3/165,G06F16/632,G06F16/683,G10H1/0008,G10H1/0041,G10H2210/056,G10H2210/125,G10H2240/131,G10H2240/141,G10H2240/175,G10H2250/235,G10H2250/311;Method and system for accelerated decomposing of audio data using intermediate data;A method for processing audio data, comprising providing song identification data identifying a particular song from among a plurality of songs or identifying a particular position within a particular song, loading intermediate data associated with the song identification data from a storage medium or from a remote device. The method also comprises obtaining input audio data representing audio signals of the song as identified by the song identification data. The audio signals comprise a mixture of different musical timbres, including at least a first musical timbre and a second musical timbre different from said first musical timbre. The method comprises combining the input audio data and the intermediate data with one another to obtain output audio data. The audio data represent audio signals of the first musical timbre separated from the second musical timbre.;3
642;11741302;2022;2023;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F40/253,G06F40/166,G06F40/205,G06F40/279,G06N3/045,G06N3/0475,G06N3/08;Automated artificial intelligence driven readability scoring techniques;A data processing system implements obtaining a first textual content, segmenting the first textual content into a plurality of first segments, and providing each segment of the plurality of first segments to a first natural language processing (NLP) model to obtain a set of first readability scores for the plurality of first segments. The first NLP model is configured to analyze a textual input and to output a readability score representing a measurement of readability of the textual input. The system further implements aggregating the set of first segment readability scores to determine a first readability score for the first textual content, and perform at least one of causing the first readability score to be presented to a user or performing one or more actions on the first textual content based on the readability score.;2
643;11741965;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/26,G06F40/56,G10L13/00,G10L13/086,G10L15/005,G10L15/18,G10L2015/088;Configurable natural language output;A system is provided for determining a natural language output, responsive to a user input, using different speech personality profiles. The system may determine to user a particular language generation profile based at least in part on data relating to the user input and data corresponding to the response to the user input. The language generation profile may include different attributes that are used to determine the natural language output, such as, prosody, replacement words, injected words, sentence structure, etc.;5
644;11741996;2022;2023;Roku, Inc.;Roku, Inc.;G11B27/031,G06Q30/0251,G06Q30/0255,G06Q30/0269,G06Q30/0271,G06Q30/0276,G06Q30/0277,G10L13/02,G10L13/047;Method and system for generating synthetic video advertisements;"In one aspect, an example method includes (i) obtaining a set of user attributes for a user of a content-presentation device; (ii) based on the set of user attributes, obtaining structured data and determining a textual description of the structured data; (iii) transforming, using a text-to-speech engine, the textual description of the structured data into synthesized speech; and (iv) generating, using the synthesized speech and for display by the content-presentation device, a synthetic video of a targeted advertisement comprising the synthesized speech.";5
645;11743378;2020;2023;Interactions LLC;Interactions LLC;G06Q30/015,H04M3/4938,G10L13/00,G10L21/10,H04M3/4933,H04M3/4936,H04M3/5133,H04M3/5232,H04M3/5237,H04M3/527,H04M3/5315,H04M3/5322,G10L15/26,H04M3/5175,H04M2203/355,H04M2203/403;Intelligent agent assistant for natural language understanding in a customer service system;A virtual assistant system for communicating with customers uses human intelligence to correct any errors in the system AI, while collecting data for machine learning and future improvements for more automation. The system may use a modular design, with separate components for carrying out different system functions and sub-functions, and with frameworks for selecting the component best able to respond to a given customer conversation. The system may have agent assistance functionality that uses natural language processing to identity concepts in a user conversation and to illustrate that concepts within a graphical user interface of a human agent so that the human agent can more accurately and more rapidly assist the user in accomplishing the user's conversational objectives.;3
646;11743552;2022;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;H04N21/41407,H04N21/64738,G06N3/045,G06N3/0475,H04N21/2662,H04N21/43615,H04N21/43637,H04N21/4402,H04N21/44227,G06N3/088;Computer technology for enhancing images with a generative adversarial network;Computer technology for use when one or more devices is receiving low resolution video typically due to low internet bandwidth. Multiple devices are chosen for a cluster, with the devices of the cluster being in device to device communication (see definition of âdevice to device communicationâ herein). One, or more, of the devices in the cluster is set up to do GAN image enhancement locally for the cluster, and the low resolution video stream is sent to the GAN enhancement device(s) and then enhanced to be sent to other device(s) in the cluster as a higher resolution, clearer video image by device to device communication.;3
647;11748577;2023;2023;ROHIRRIM, INC.;ROHIRRIM, INC.;G06F40/40,G06F40/169,G06F40/30,G06N3/045,G06N3/09,G06N20/00,G06F40/289,G06F40/56;Computer-generated content based on text classification, semantic relevance, and activation of deep learning large language models;The disclosure relates to systems and methods of automatically generating unique content including natural language text based on a corpus of previously generated response documents and discrete requirements defined in a requirements specification. The system may use generative stitching that includes multi-layer processes that execute to influence the generation of unique content including natural language text through an artificial intelligence (AI) language transformer model trained to output the content based on previously written material that is semantically relevant to the discrete requirements and is weighted against labeled attributes. The labeled attributes may determine the influence asserted against the language transformer, thereby generating unique on-target content that may be combined to create a computer-generated response document.;3
648;11749282;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L15/26,G06F40/30,G10L15/08,G10L15/1822,G10L2015/223,G10L2015/228;Goal-oriented dialog system;A dialog system receives a user request corresponding to a dialog with a user. The dialog system processes the user request to determine multiple service providers capable of responding to the user request. The dialog system selects one service provider based on a request-to-handle score, and selects another service provider based on a satisfaction rating. The dialog system updates the dialog state based on further input provided by the user to determine an output responsive to the user request.;0
650;11755888;2023;2023;FUDAN UNIVERSITY;FUDAN UNIVERSITY;G06N3/0475,G06N3/084,G06N3/094;Method and system for accelerating score-based generative models with preconditioned diffusion sampling;"A method for accelerating score-based generative models (SGM) is provided, including setting a frequency mask (R) and a space mask (A) and a target sampling iteration number (T); sampling an initial sample (x0); conducting iteration comprising steps as follows: sampling a noise term; applying a preconditioned diffusion sampling (PDS) operator (M) to the noise term and thus generate a preconditioned noise term; calculating a drift term; applying the transpose of the PDS operator (MT) and then applying the PDS operator (M) to the drift term, and thus generate a preconditioned drift term; diffusing the sample of each iteration (xt); and outputting the result.";3
652;11756525;2022;2023;Zoom Video Communications, Inc.;Zoom Video Communications, Inc.;G10K11/17827,G10K11/17823,G10K11/17853,H04M3/568;Joint audio interference reduction and frequency band compensation for videoconferencing;One disclosed example method includes a device receiving an audio signal recorded in a physical environment and applying a machine learning model onto the audio signal to generate an enhanced audio signal. The machine learning model is configured to simultaneously remove interference and distortion from the audio signal and is trained via a training process. The training process includes generating a training dataset by generating a clean audio signal and generating a noisy distorted audio signal based on the clean audio signal that includes both an interference and a distortion. The training further includes constructing the machine learning model as a generative adversarial network (GAN) model that includes a generator model and multiple discriminator models, and training the machine learning model using the training dataset to minimize a loss function defined based on the clean audio signal and the noisy distorted audio signal.;3
653;11757907;2020;2023;Cytellix Corporation;Cytellix Corporation;H04L63/1425,G06N7/01,G06N20/00,H04L63/1433,H04L63/1441;Cybersecurity threat intelligence and remediation system;A cybersecurity system is provided for automated cybersecurity insights, remediation recommendations, and service provisioning. The cybersecurity system can generate threat insights and/or generate remediation recommendations using machine learning models and cybersecurity data obtained from target networks, partners, and the like. To provision cybersecurity services, cybersecurity system may collect metadata regarding the network connections and use cases desired for one or more services. Once the metadata has been collected, the cybersecurity assessment system automatically provisions the selected services based on the provided data, such as duration of time elected, service metrics, and the like.;0
654;11757923;2022;2023;Second Sight Data Discovery, Inc.;Second Sight Data Discovery, Inc.;H04L63/1433;Apparatus and method for intelligent processing of cyber security risk data;An apparatus and method for intelligent processing of cyber security risk assessment data are provided. The apparatus includes a processor and a memory communicatively coupled to the at least a processor. The memory contains instructions configuring the at least a processor to receive a cyber profile associated with a digital environment. The processor is also configured to generate a cyber profile summary of the cyber profile data and generate a user interface data structure including the cyber profile summary and the cyber profile. A graphical user interface (GUI) is communicatively connected to the processor and the GUI is configured to receive the user interface data structure including the cyber profile summary and the cyber profile and display the cyber profile summary on a first portion of the GUI.;0
657;11763453;2023;2023;East China Normal University;East China Normal University;G06T7/0012,G06V10/58,G06T5/50,G06T7/11,G06V10/46,G06T2207/10024,G06T2207/10036,G06T2207/10056,G06T2207/20221,G06T2207/30024,G06T2207/30096,G06V2201/031;Automatic generation method of fine-labeled digital pathological data set based on hyperspectral imaging;"Disclosed is an automatic generation method of a fine-labeled digital pathological data set based on hyperspectral imaging, comprising following steps: obtaining reference histological stained slides and double-stained slides based on pathological samples; obtaining two-dimensional color reference whole slide images based on the reference histological stained slides, and obtaining double-stained hyperspectral images based on the double-stained slides; establishing virtual staining models based on the two-dimensional color reference whole slide images and the double-stained hyperspectral images; establishing a segmentation model for automatically generating labeling information based on the double-stained hyperspectral images; and obtaining the fine-labeled digital pathological data set based on the double-stained hyperspectral images and the virtual staining models, the double-stained hyperspectral images and the segmentation model.";4
658;11763809;2020;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G06F3/017,G06F3/167,G10L13/033,G10L15/1815,G10L15/24,G10L15/32,G10L2015/088,G10L2015/223;Access to multiple virtual assistants;A speech-processing system may provide access to multiple virtual assistants via one or more voice-controlled devices. Each assistant may leverage language processing and language generation features of the speech-processing system, while handling different commands and/or providing access to different back applications. Each assistant may be associated with its own voice and/or speech style, and thus be perceived as having a particular âpersonality.â In some situations, a user may invoke a first assistant, e.g., with a wakeword or button press, and provide a command that the speech-processing system may determine will be better handled by a second assistant. The speech-processing system may thus call on a component to generate plan data describing one or more operations for the speech-processing system to execute to handoff the command to the second assistant and provide the user with indications of which assistant will handle the command.;3
659;11763849;2022;2023;LEMON INC.;LEMON INC.;G11B27/031,G06V20/46,G10H1/0025,G10H1/368,G11B27/10,G11B27/28,H04N21/4394,H04N21/8113,H04N21/845,G10H2210/021,G10H2210/125,G10H2240/085,G10H2240/131;Automatic and fast generation of music audio content for videos;The present disclosure describes techniques for automatically and fast generating music for videos. The techniques comprise receiving a video from a user. The video may comprise a plurality of segments of frames. Information may be extracted from the video, wherein the extracted information comprises information indicating motion speed in the video, information indicating motion saliency in the video, information indicating scene transition in the video, and timing information associated with the video. A plurality of sets of music notes matching the plurality of segments of frames may be generated based at least in part on the extracted information. A plurality of vectors corresponding to the plurality of sets of music notes may be generated. The plurality of pieces of music audio corresponding to the plurality of segments of frames may be generated based at least in part on the plurality of vectors.;5
661;11769017;2023;2023;GOOGLE LLC;GOOGLE LLC;G06F40/40,G06F40/56,G06F16/3328,G06F16/3344,G06F16/345,G06N3/0455,G06N3/0475,G06N20/00;Generative summaries for search results;At least selectively utilizing a large language model (LLM) in generating a natural language (NL) based summary to be rendered in response to a query. In some implementations, in generating the NL based summary additional content is processed using the LLM. The additional content is in addition to query content of the query itself and, in generating the NL based summary, can be processed using the LLM and along with the query contentâor even independent of the query content. Processing the additional content can, for example, mitigate occurrences of the NL based summary including inaccuracies and/or can mitigate occurrences of the NL based summary being over-specified and/or under-specified.;3
662;11769239;2023;2023;INTUIT INC.;INTUIT INC.;G06T7/0002,G06T5/70,G06T9/00,G06V10/70,G06V10/82,G06V30/10,G06V30/164,G06V30/18086,G06T2207/20081,G06T2207/30168;Model based document image enhancement;Systems and methods are disclosed for model based document image enhancement. Instead of requiring paired dirty and clean images for training a model to clean document images (which may cause privacy concerns), two models are trained on the unpaired images such that only the dirty images are accessed or only the clean images are accessed at one time. One model is a first implicit model to translate the dirty images from a source space to a latent space, and the other model is a second implicit model to translate the images from the latent space to clean images in a target space. The second implicit model is trained based on translating electronic document images in the target space to the latent space. In some implementations, the implicit models are diffusion models, such as denoising diffusion implicit models based on solving ordinary differential equations.;3
663;11769312;2023;2023;Roku, Inc.;Roku, Inc.;G06T19/20,G06T15/10,G06T15/503,G06V10/70,G06V10/82,G06V20/41,G06T2219/2016,G06V2201/10;Video system with scene-based object insertion feature;"In one aspect, an example method includes (i) obtaining video that depicts an area across multiple frames of the video, wherein the area is part of a scene of the video, and wherein the area is suitable for having an object inserted therein; (ii) detecting the area within the obtained video and determining area characteristic data associated with the detected area; (iii) determining scene attribute data associated with the scene; (iv) using at least the determined area characteristic data and the determined scene attribute data as a basis to select an object from among a set of multiple candidate objects; (v) inserting into the detected area the selected object to generate video that is a modified version of the obtained video; and (vi) outputting for presentation the generated video.";3
664;11769531;2023;2023;Roku, Inc.;Roku, Inc.;G11B27/031,H04N21/251,H04N21/4532,H04N21/4667,H04N21/4755,H04N21/8456,H04N21/854,G06N20/00;Content system with user-input based video content generation feature;"In one aspect, an example method includes (i) obtaining a first segment of video content; (ii) outputting for presentation, the obtained first segment; (iii) after outputting for presentation the obtained first segment, causing a user to be prompted for user-input data; (iv) receiving user-input data provided in response to the prompting; (v) using at least the received user-input data to synthetically generate a second segment of the video content, wherein the generated second segment is static, non-interactive content; and (vi) outputting for presentation, the generated second segment.";4
665;11775581;2019;2023;Meta Platforms, Inc.;Meta Platforms, Inc.;G06F16/685,G06F40/279,G06F16/635,G06F16/639,G06F16/64,G06F40/30,G06N3/04,H04L51/046,G06N3/045,G06N3/0464,G10H2210/076,G10H2210/081,H04L51/52;Systems and methods for feature-based music selection;Systems and methods for feature-based music selection may include (1) receiving user input selecting a music composition, (2) identifying features of the music composition including (i) a musical feature, relating to a musical quality of the music composition and (ii) a lyrical feature, relating to one or more of the music composition's lyrics, (3) determining that an additional music composition is similar to the music composition based on a comparison of the features of the music composition with features of the additional music composition, and (4) selecting the additional music composition to be added to a queue associated with the music composition based on the determination. Various other methods, systems, and computer-readable media are also disclosed.;0
666;11776007;2023;2023;INSTREAMATIC, INC.;INSTREAMATIC, INC.;G06Q30/0252,G06Q30/0261,G06Q30/0276,G10L13/02,G10L13/08,G10L13/033;Environmental and context-based customization of advertisement messages;Systems and methods for providing environmental and context-based customized advertisement messages are provided. An example method includes receiving, from the application server, a request for advertisement. The request may include context parameters obtained by the application server via the application program running on the user device. The method further includes selecting, based on the context parameters, an advertisement media file from a database. The selected advertisement media file may be generated by an Artificial Intelligence model based on an advertisement message and the context parameters. The method further includes transmitting the selected advertisement media file to the application server. The application server may be configured to transmit the selected advertisement media file to the user device and the application program running on the user device may be configured to play back the selected advertisement media file to a user.;5
667;11777874;2019;2023;Carvana, LLC;Carvana, LLC;H04L51/02,G06F40/295,G06F40/30,G06F40/35,G06N5/01,G06N5/04,G06N20/00,G06N20/20,H04L51/216,G06N3/08,H04W4/14;Artificial intelligence conversation engine;Systems, methods, and devices of the various embodiments may provide an artificial intelligence (AI) conversation system, such as an AI driven virtual assistant, that can participate in automated conversations with users. The AI conversation system may be configured to respond to user inquiries or requests and implement conversations to achieve tasks.;2
668;11783474;2022;2023;CHANGZHOU MICROINTELLIGENCE CO., LTD.;CHANGZHOU MICROINTELLIGENCE CO., LTD.;G06T7/001,G06T2207/20081,G06T2207/20084,G06T2207/30164,Y02P90/30;Defective picture generation method and apparatus applied to industrial quality inspection;"A defective picture generation method applied to industrial quality inspection and a defective picture generation apparatus applied to industrial quality inspection are provided. The method includes: acquiring a first workpiece picture set with defects and a second workpiece picture set without defects; determining a defect annotation picture corresponding to each first workpiece picture in the first workpiece picture set; determining a feature value of each second workpiece picture in the second workpiece picture set; training a pix2pixHD network based on the first workpiece pictures, the second workpiece pictures, the defect annotation pictures and the feature values; acquiring a target defect annotation picture from the defect feature database according to the desired defect type; acquiring a target feature value from the picture feature database according to the desired picture type; and inputting the target defect annotation picture and the target feature value into the trained generator.";3
669;11783813;2022;2023;;;G10L15/16,H04R25/70,G10L15/02,G10L21/007,G10L21/0364,G10L25/90,H04R25/507,G10L2015/025;Methods and systems for improving word discrimination with phonologically-trained machine learning models;A hearing aid system presents a hearing impaired user with customized enhanced intelligibility speech sound in a preferred language while maintaining the voice identity of speaker. The system includes a neural network model trained with a set of source speech data representing sampling from a speech population relevant to the user. The model is also custom trained with a set of parallel or non-parallel alternative articulations, collected during an interactive session with user or algorithmically generated based on the hearing profile of the user or category of users with common linguistic and hearing profiles.;3
670;11783824;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G10L15/22,G10L15/26,G06F3/167,G06F40/35,G06F40/58,G10L13/00,G10L15/18,G10L15/30,G10L2015/088,G10L2015/223;Cross-assistant command processing;A speech-processing system may provide access to one or more virtual assistants via an audio-controlled device. A user may leverage a first virtual assistant to translate a natural language command from a first language into a second language, which the device can send to a second virtual assistant for processing. The device may receive a command from a user and send input data representing the command to a first speech-processing system representing the first virtual assistant. The device may receive a response in the form of a first natural language output from the first speech-processing system along with an indication that the first natural language output should be directed to a second speech-processing system representing the second virtual assistant. For example, the command may be in the first language, and the first natural language output may be in the second language, which is understandable by the second speech-processing system.;1
671;11790492;2020;2023;THALES SA;THALES SA;G06N3/0464,G06T5/70,G06N3/045,G06N3/0455,G06N3/0475,G06N3/048,G06N3/0495,G06N3/08,G06N3/094,G06T5/50,G06T5/60,G06T7/0002,G06T7/80,G06T2207/10028,G06T2207/10044,G06T2207/10081,G06T2207/10116,G06T2207/20081,G06T2207/20084,G06T2207/20092,G06T2207/30168;Method of and system for customized image denoising with model interpretations;There is provided a method and a system for customized image denoising with interpretability. A deep neural network (NN) is trained to denoise an image on a training dataset including pairs of noisy and corresponding clean images acquired from an imaging apparatus, where during the training a structured covariance score (SCS) indicative of a performance of the deep NN in recovering content of corresponding clean images relative to the denoised image is determined based on sparse conditional correlations. A test noisy image is received and denoised by the deep NN. A user feedback score indicative of user satisfaction of the denoising is obtained. A quality parameter is obtained based on the SCS and a quality metric indicative of denoised image quality is obtained from a pretrained NN, and compared with the user feedback score. If the SCS is above the user feedback score, the deep NN is provided for denoising.;3
672;11790558;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06T7/97,G06V10/82,G06F18/214,G06N3/0475,G06N3/094,G06N20/00,G06T11/00,G06V10/76,G06V40/16,G06V40/168,G06N3/0455;Generation of synthetic image data with varied attributes;Techniques are generally described for generation of synthetic image data. In some examples, a selection of a first image may be received. The first image may depict at least a first object having a plurality of image attributes representing visual characteristics of the at least the first object. In some examples, a selection of a first image attribute of the plurality of image attributes to be maintained in subsequently-generated images may be received. In various examples, a first machine learning model may generate a second image having the plurality of image attributes. The change in an appearance of the first image attribute may be minimized in the second image while a change in the appearance of other attributes of the plurality of image attributes may be maximized in the second image.;3
673;11790876;2023;2023;Library X Music Inc.;Library X Music Inc.;G10H1/0025,G10H1/38,G10H2210/036,G10H2210/061,G10H2210/066,G10H2210/111,G10H2210/125,G10H2210/145,G10H2210/185,G10H2210/335,G10H2210/576,G10H2240/085,G10H2250/311;Music technique responsible for versioning;"Systems and methods for versioning audio elements used in generation of music are provided. An example method includes receiving musical format data associated with a plurality of audio elements of a melody; determining, based on the musical format data, harmonic and melodic characteristics of each of the plurality of audio elements; matching the harmonic and melodic characteristics to a plurality of chord progressions using predetermined music theory rules, counterpoint rules, and rhythm matching rules; deriving, based on the matching and predetermined melodic movement rules, from the plurality of chord progressions, melodic movement characteristics applicable to using in versioning; and creating, based on the predetermined music theory rules and the melodic movement characteristics, versions of the audio elements that match the chord progressions.";3
674;11790884;2020;2023;Electronic Arts Inc.;Electronic Arts Inc.;G10L13/047,G10L21/003,A63F13/215,A63F13/54,G10L13/033,G10L21/007,G10L25/30;Generating speech in the voice of a player of a video game;A computer-implemented method of generating speech audio in a video game is provided. The method includes inputting, into a synthesizer module, input data that represents speech content. Source acoustic features for the speech content in the voice of a source speaker are generated and are input, along with a speaker embedding associated with a player of the video game into an acoustic feature encoder of a voice convertor. One or more acoustic feature encodings are generated as output of the acoustic feature encoder, which are inputted into an acoustic feature decoder of the voice convertor to generate target acoustic features. The target acoustic features are processed with one or more modules, to generate speech audio in the voice of the player.;3
675;11791018;2019;2023;HRL Laboratories, LLC;HRL Laboratories, LLC;G16C20/10,G06N3/042,G06N3/0464,G06N3/08,G06N3/084,G16C20/20,G16C20/30,G16C20/50,G16C20/70,G16C20/80;System and method for discovering chemically active compounds of a molecule;Described is a system for automatically identifying chemical properties of a molecule. A chemical representation of a molecular structure is converted into atomic features and an adjacency matrix. The atomic features and the adjacency matrix are processed with a neural network, resulting in neural activations corresponding to each atom in the molecular structure. The system determines a probability for each atom quantifying its relevance for a given chemical characteristic. The probabilities are displayed as a graphical representation on the molecular structure, and groups of atoms are identified for the given chemical characteristic from the graphical representation. The identified groups of atoms for the given chemical characteristic are stored in a database, and a new molecule having the given chemical characteristic is designed based on the stored identified groups of atoms.;3
676;11793725;2022;2023;KURE, LLC;KURE, LLC;A61J7/0076,A61J7/02,A61J7/0418,A61J7/0427,A61J7/0481,G06F21/32,G16H10/60,G16H20/13,G16H40/67,A61J2200/30,A61J2200/70,A61J2205/10,A61J2205/60,A61J2205/70;Smart dispensing system;"Embodiments relate to a system comprising a drug dispenser; a user interface to receive a user information; a bio-feedback monitoring device to monitor at least one vital of a user as an input to the bio-feedback monitoring device; a communication module; a processor; a database; a memory; wherein the processor is communicatively coupled with the memory; and wherein the processor is configured to: receive a prescription; dispense a drug through the drug dispenser on authenticating the user information with at least one vital of the user and the prescription; update an inventory of the drug through the communication module; log record of dispensing of the drug; maintain a ledger of the record of dispensing of the drug, use blockchain technology. The output of the bio-feedback monitoring device controls a quantity of drug dispensed from the drug dispenser.";0
677;11797780;2022;2023;INTUIT INC.;INTUIT INC.;G06F40/40,G06F40/216,G06F40/30,G06F40/56,G06T11/60;Context-biased artificial intelligence video generation;A method includes receiving a set of text documents. The method also includes generating a summary of the set of text documents by a set of large language machine learning models. The method further includes generating a set of keywords from the summary by the set of large language machine learning models. The method additionally includes generating an image prompt from the set of keywords by the set of large language machine learning models. The method also includes generating a set of images from the image prompt by a text-to-image machine learning model. The method further includes generating a video clip from the set of images. The method additionally includes presenting the video clip.;5
679;11804022;2023;2023;Illuscio, Inc.;Illuscio, Inc.;G06T19/20,G06F3/011,G06F3/013,G06F3/017,G06F3/0304,G06T2200/24,G06T2219/2004,G06T2219/2016,G06T2219/2021,G06T2219/2024;Systems and methods for generative drawing and customization of three-dimensional (â3Dâ) objects in 3D space using gestures;Disclosed is a system and associated methods for the generative drawing and customization of three-dimensional (â3Dâ) objects in 3D space using hand gestures. The system adapts the hand gestures as intuitive controls for rapidly creating and customizing the 3D objects to have a desired artistic effect or a desired look. The system selects a 3D model of a particular object in response to a first user input, sets a position in a virtual space at which to generate the particular object in response to a mapped position of a first hand gesture tracked in a physical space, and generates a first state representation of the particular object at the position in the virtual space in response to a second hand gesture. The first state representation presents the particular object at one of different modeled stages of the particular object lifecycle.;5
680;11809688;2023;2023;Typeface Inc.;Typeface Inc.;G06F3/0482,G06F3/04812,G06F3/0484,G06F3/04845,G06F40/103,G06F40/186,G06F40/216,G06F40/30,G06F40/44,G06F40/56,G06N3/0475,G06T5/50,G06T5/77,G06T7/11,G06T7/194,G06T11/00,G06V20/62,G06T2200/24,G06T2207/20084;Interactive prompting system for multimodal personalized content generation;"Methods, systems, and computer programs are presented for providing a prompt tool with interactive entry. One method includes operations for providing a multimodal prompt tool for entering textual description of an item to be generated, and detecting an input that is one of a special character entered in the textual description or a mouse action requesting assistance. Furthermore, a menu is presented with options for the item to be generated, and a list of products, previously added to a data store, is obtained. Further, the method includes providing the list of products for selection; in response to a selection of a product from the list of products, including text associated with the selected product in the textual description; and detecting submittal of the textual description. Further, the textual description is entered as input to a generative artificial intelligence (GAI) tool, and causing presentation of items generated by the GAI tool.";5
682;11817079;2023;2023;NANJING SILICON INTELLIGENCE TECHNOLOGY CO., LTD.;NANJING SILICON INTELLIGENCE TECHNOLOGY CO., LTD.;G10L13/047,G10L13/10,G10L25/30,G10L2013/083;GAN-based speech synthesis model and training method;The present disclosure provides a GAN-based speech synthesis model, a training method, and a speech synthesis method. According to the speech synthesis method, to-be-converted text is obtained and is converted into a text phoneme, the text phoneme is further digitized to obtain text data, and the text data is converted into a text vector to be input into a speech synthesis model. In this way, target audio corresponding to the to-be-converted text is obtained. When a target Mel-frequency spectrum is generated by using a trained generator, accuracy of the generated target Mel-frequency spectrum can reach that of a standard Mel-frequency spectrum. Through constant adversary between the generator and a discriminator and the trainings thereof, acoustic losses of the target Mel-frequency spectrum are reduced, and acoustic losses of the target audio generated based on the target Mel-frequency spectrum are also reduced, thereby improving accuracy of audio synthesized from speech.;5
683;11817088;2023;2023;INTUIT INC.;INTUIT INC.;G06F40/274,G10L15/16,G06F40/35,G06N3/044,G06N3/045,G06N3/047,G06N3/08,G06N3/084,G06N7/01,G10L15/197,G06N20/20;Ensemble of machine learning models for real-time predictions in expert electronic chats;An ensemble of machine learning models used for real-time prediction of text for an electronic chat with an expert user. A global machine learning model, e.g., a transformer model, trained with domain specific knowledge makes a domain specific generalized prediction. Another machine learning model, e.g., an n-gram model, learns the specific style of the expert user as the expert user types to generate more natural, more expert user specific text. If specific words cannot be predicted with a desired probability level, another word level machine learning model, e.g., a word completion model, completes the words as the characters are being typed. The ensemble therefore produces real-time, natural, and accurate text that is provided to the expert user. Continuous feedback of the acceptance/rejection of predictions by the expert is used to fine tune one or more machine learning models of the ensemble in real time.;3
684;11823235;2022;2023;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06Q30/0269,G06N3/045,G06N3/08,G06Q30/0276,G06N3/006,G06N3/0475,G06N3/094,G06N20/20;Virtual personalization based on personas;In an approach for identifying a user's interest in a media object and one or more personalities and presenting the user with a visual of how the media object looks on a particular personality, a processor receives a request to determine suitability of the media object for the user. A processor presents the media object to a multilabel classifier to be matched to a set of profile data of the user and a set of data of one or more personalities. A processor receives an output instruction from the multilabel classifier to combine the media object with a particular personality of the one or more personalities. A processor generates a combined media object of the media object with the particular personality using a generative adversarial neural network. A processor inserts the combined media object into an advertising template to generate an advertisement. A processor presents the advertisement to the user.;3
685;11829474;2023;2023;JIANGNAN UNIVERSITY;JIANGNAN UNIVERSITY;G06F21/566,G06F18/2415,G06F2221/034,Y02D10/00;Text classification backdoor attack prediction method, system, and device;"The present invention provides a text classification backdoor attack method, system, device and a computer storage medium. The method includes: training a pretraining model by using a clean training set to obtain a clean model; generating a pseudo label data set by using a positioning label generator; performing multi-task training on a Sequence-to-Sequence model by using the pseudo label data set to obtain a locator model; generating a backdoor data set by using the locator model; and training the clean model by using the backdoor data set to obtain a dirty model. A pseudo label data set is generated by using a pretrained clean model without manual annotation. A backdoor attack location in a text sequence may be dynamically predicted by using a locator model based on a Sequence-to-Sequence and multi-task learning architecture without manual intervention, and a performance indicator obtained by dynamically selecting an attack location is better.";0
686;11830159;2022;2023;Flawless Holding Limited;Flawless Holding Limited;G06T19/20,G06T3/4046,G06T11/60,G06T13/40,G10L15/02,G10L25/30,G10L25/57,H04N5/2224,H04N5/272,G06T2200/24,G06T2219/2021,G10L21/10,G10L2015/025;Generative films;Obtaining input video data depicting footage of an object, obtaining current values of a set of adjustable parameters of an object representation model comprising a neural network and arranged to generate animated representations of the object using the neural network in which a geometry of the object is controllable by the set of adjustable parameters. For a plurality of iterations, using the object representation model to generate a video layer comprising an animated representation of the object in which the geometry of the object corresponds to the current values of the set of adjustable parameters, presenting composite video data comprising the video layer overlaid on the object in the input video data, and updating the current values of the set of adjustable parameters in response to user input via the user interface.;3
687;11830463;2023;2023;Library X Music Inc.;Library X Music Inc.;G10H1/0025,G10H2210/111,G10H2210/121,G10H2240/085,G10H2250/311;Automated original track generation engine;"Systems and methods for automated music generation are provided. An example method includes receiving, from a user, a user input including at least one of configuration settings and a musical audio input in the form of audio files or an audio recording; selecting, based on the user input and from a plurality of predetermined musical development scenarios, a musical development scenario including a chronologically ordered sequence of set settings; selecting, based on the musical development scenario, from a plurality of event probability scenarios, an event probability scenario defining a probability of a music element creation event; selecting a plurality of sets of audio elements from a plurality of pre-composed audio elements based on the musical development scenario, the user input, the event probability scenario, and predetermined music theory rules; and synthesizing the plurality of sets of audio elements to generate an audio output for providing to the user.";3
688;11830476;2021;2023;Amazon Technologies, Inc.;Amazon Technologies, Inc.;G06N3/0455,G10L13/10,G06N3/047,G06N3/08,G10L13/033,G10L13/047,G10L13/07,G10L13/086,G10L25/30;Learned condition text-to-speech synthesis;Devices and techniques are generally described for learned condition text-to-speech synthesis. In some examples, first data representing a selection of a type of prosodic expressivity may be received. In some further examples, a selection of content comprising text data may be received. First audio data may be determined that includes an audio representation of the text data. The first audio data may be generated based at least in part on sampling from a first latent distribution generated using a conditional primary variational autoencoder (VAE). The sampling from the first latent distribution may be conditioned on a first learned distribution associated with the type of prosodic expressivity. In various examples, the first audio data may be sent to a first computing device.;3
689;11836680;2022;2023;;;G06Q10/101;System and method for interactive learning;A system for interactive learning has a database of content, the content including spatial, temporal, material, psychological, moral, artistic, philosophical, scientific, and probability elements from which to create a story track. At least one software program is disposed on at least one computer system designed to calculate story vectors from which to craft stories and select learning objectives. A learning management system with at least one user interface is designed to allow at least one or more people to, via at least one or more of video, audio, and text, interact with at least one or more of the software program and people, people inclusive of audience members, characters, actors, clients, and moderators. The software program is designed to direct the story within degrees of freedom calculated from the story vectors to create new story vectors, the story vectors used to create one or more new story tracks.;6
691;11837021;2023;2023;HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY;HUAZHONG UNIVERSITY OF SCIENCE AND TECHNOLOGY;G06V10/82,G06V40/176,G06V10/764,G06V10/774,G06V10/806,G06V40/165,G06V40/172,G06V40/174;Compound expression recognition method with few samples of multi-domain adversarial learning;Disclosed is a compound expression recognition method with few samples of multi-domain adversarial learning. To extract compound expression features with diversity and complexity with few samples, multiple small sample datasets are fused, and divided into expression sub-domains, and multi-domain adversarial learning is performed to improve the performance of compound expression recognition. Based on the generative adversarial network framework, the face domain and the contour-independent compound expression domain are fused in the generative network to enhance diversity and complexity, and two discriminators are designed to guide the generator. The face discriminator uses the face domain to guide the generator and identify the generator to generate expression-independent face identity attributes, so that the generator has identity diversity. The compound expression fusing discriminator fuses the basic expression domain and the contour-related compound expression domain together to guide the generator and identify the complexity of the expressions generated by the generator.;3
692;11842144;2023;2023;Rammer Technologies, Inc.;Rammer Technologies, Inc.;G06F40/166,G06F40/35,G06F16/345,G10L15/02,G10L15/04,G10L15/1815,G10L15/22,G10L15/30,G10L15/26;Summarizing conversational speech;Embodiments are directed to summarizing conversational speech. Conversation segments may be provided based on a conversation stream and segmentation models. Summarization models may be determined based on characteristics of the conversation segments. Summarization information may be generated for each of the conversation segments based on the summarization models such that the summarization information includes a text-based summarization of the conversation segment. Summarization profiles may be generated for the conversation segments based on the summarization information such that each summarization profile is associated with quality scores. Summarization models may be modified based on the summarization profiles and the associated quality scores such that the summarization profiles are updated based on the modified summarization models. Modified summarization models and the updated summarization profiles may be employed to provide reports to a user.;4
695;11896902;2023;2024;Electronic Arts Inc.;Electronic Arts Inc.;A63F13/54,A63F13/40,A63F13/67,A63F13/79;Emotion based music style change using deep learning;Various aspects of the subject technology relate to systems, methods, and machine-readable media for changing music of a video game based on a player's emotion. The method includes receiving indicators of emotion comprising in-game attributes of a player in a video game. The method also includes predicting an emotion of the player based on the indicators of emotion from the video game. The method also includes receiving original music from the video game. The method also includes determining an original tone of the original music. The method also includes determining a transformed tone based at least in part on the emotion of the player that was predicted. The method also includes transforming the original tone of the original music to the transformed tone. The method also includes generating transformed music from the original music based on the transformed tone.;5
696;11899148;2023;2024;Institute of Geomechanics, Chinese Academy of Geological Sciences;Institute of Geomechanics, Chinese Academy of Geological Sciences;G01V1/306,G01V1/282,G01V1/301;Seismic imaging free gas structure identification method and system;"The present disclosure relates to a seismic imaging free gas structure identification method and system. The method includes: acquiring a seismic imaging dataset, and annotating free gas structures in the seismic imaging sample data set to obtain a training dataset; using a generative adversarial network (GAN) to expand the training dataset for network training, wherein the GAN includes a generative network and a discrimination network; conducting domain conversion on original label data of the free gas training samples to obtain annotating labels; training a Bayesian neural network according to the free gas training datasets and labels, to obtain a free gas structure identification model; acquiring actual seismic imaging data of a target work area; and conducting, according to the actual seismic imaging data, identification by using the free gas structure identification model.";5
698;11907885;2022;2024;Slate Technologies Inc.;Slate Technologies Inc.;G06F40/30,G06Q10/067,G06N20/20,G06Q10/06315,G06N3/045,G06N3/0475;System and method for computational simulation and augmented/virtual reality in a construction environment;A method for simulating a scenario in a computing environment is described. The method includes determining a user intent of a user from an input received from the user for executing at least one intended task by the user, generating a feature set based on analyzing the user intent, and processing, based on the user intent, at least one data feed received from a database to select at least one plan of action for executing the at least one intended task. At least one plan of action is simulated in a graphical user interface as virtual or augmented reality based on the feature set to enable at least one of responding additionally received input from the user and the at least one intended task is performed according to the user intent.;5
699;11915137;2023;2024;"BEIHANG UNIVERSITY;XICHENG DISTRICT BUREAU OF SCIENCE AND TECHNOLOGY AND INFORMATION TECHNOLOGY OF BEIJING MUNICIPALITY";BEIHANG UNIVERSITY;G06N3/08,G06N3/044,G06N3/045,G06N3/0455,G06N3/047,G06N7/01,Y04S10/50;Urban data prediction method based on a generative causal interpretation model;An urban data prediction method based on a generative causal interpretation model is provided. The generative causal interpretation model includes exogenous variables, spatio-temporal conditional parent variables, controlled causal transition functions, and spatio-temporal mixing functions. By inferring the model's exogenous variables, causal descriptors, spatio-temporal conditional parent variables, and other causal latent variables from the observation data and fitting the corresponding functions such as the controlled causal transfer function and the spatio-temporal mixing function, the invention can predict the spatio-temporal data in city level based on the model. The observation data of the urban complex system can be decomposed into causal descriptors with physical meanings. Under the influence of stable causal structure, the robustness and applicability of the model can be improved, so that the prediction results are more in line with the operation of urban complex systems.;3
700;11926332;2022;2024;Waymo LLC;Waymo LLC;B60W50/0098,B60W60/0017,B60W30/18,G05B13/048,G05D1/0088,G05D1/0212,B60W2050/0014,B60W2400/00;Assessing surprise for autonomous vehicles;Aspects of the disclosure provide for controlling an autonomous vehicle. For instance, a first probability distribution may be generated for the vehicle at a first future point in time using a generative model for predicting expected behaviors of objects and a set of characteristics for the vehicle at an initial time expected to be perceived by an observer. Planning system software of the vehicle may be used to generate a trajectory for the vehicle to follow. A second probability distribution may be generated for a second future point in time using the generative model based on the trajectory and a set of characteristics for the vehicle at the first future point expected to be perceived by the observer. A surprise assessment may be generated by comparing the first probability distribution to the second probability distribution. The vehicle may be controlled based on the surprise assessment.;3
701;2016164519;2015;;Finger Wave, LLC;Finger Wave, LLC;H03K17/94,A61M37/0076,H03K17/945;TOUCHLESS ON/OFF TATTOO COMPONENT FOR HANDHELD TATTOO MACHINES AND TATTOO POWER SUPPLIES;"The invention consists of a touchless on/off tattoo component that manipulates the operation of hand held tattoo machines and tattoo power supplies through the use of motion with any body part; such as but not limited to a finger, fingers, hand, hands, arm and arms. The touchless on/off tattoo component includes a housing for retaining the interior electronic components needed to operate a hand held tattoo machine and a tattoo power supply. A sensor enables touchless activation through the use of motion with any body part across the face of the sensor; as well as deactivation with the use of the same motion.";0
702;2016189705;2014;;National Institute of Information and Communications Technology;National Institute of Information and Communications Technology;G10L21/0364,G10L13/086,G10L13/027,G10L25/18,G10L13/10;QUANTITATIVE F0 CONTOUR GENERATING DEVICE AND METHOD, AND MODEL LEARNING DEVICE AND METHOD FOR F0 CONTOUR GENERATION;"[Object] An object is to provide an F0 contour synthesizing device based on statistic model, to clarify correspondence between linguistic information and F0 contour while maintaining accuracy. [Solution] An HMM learning device includes: a parameter estimating unit representing an F0 contour 133 fitting a continuous F0 contour 132 as a sum of phrase components and accent components and estimating target points of these; and an HMM learning means conducting learning of HMM 139 using the fitted F0 contour as training data. The continuous F0 contour may be decomposed to accent components 134, phrase components 136 and micro-prosody components 138, and separate HMMs 140, 142 and 144 may be trained. Using results of text analysis, accent components, phrase components and micro-prosody components are separately synthesized from HMMs 140, 142 and 144 and the results are synthesized to obtain an F0 contour.";0
703;2016210663;2015;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G06Q30/0251;Method and System for Determining Advertisement to Be Displayed Based on Price Bid for Peripheral Sensing Information;"An advertisement display determining method configured as a computer may include: collecting situation information from at least one sensor of a digital signage device; registering advertisement content based on setting information input from an advertiser or campaign setting information input to a campaign template selected by the advertiser; extracting the advertisement content corresponding to the situation information in response to matching between the setting information input from the advertiser or the campaign setting information input from the advertiser and the situation information; and providing the extracted advertisement content to the digital signage device, and logging the advertisement content for differentially determining a charge based on the situation information.";0
704;2016232256;2016;;AUTODESK, INC.;AUTODESK, INC.;G06F30/13,G06F30/00;MODEL BUILDER;A system and method for automatically creating a three-dimensional (3D) model for use in a design application, such as a transportation planning and design application that supports roadway, bridge or drainage design. A user at a client computer selects an area of interest (AOI) on a two-dimensional (2D) map, as well as one or more data sets to be used, and then submits a model creation request to a server computer. The server computer automatically aggregates one or more data sources to generate a 3D model of the area of interest. At least one of the data sources comprises a curated database. The server computer publishes or stores the 3D model into a database, for subsequent downloading to the client computer. The 3D model is a proprietary model owned by the user, and is fully editable by the user.;3
705;2016239865;2016;;TENCENT TECHNOLOGY (SHENZHEN) COMPANY LIMITED;TENCENT TECHNOLOGY (SHENZHEN) COMPANY LIMITED;G06Q30/0251;METHOD AND DEVICE FOR ADVERTISEMENT CLASSIFICATION;"The present disclosure provides a method and a device for advertisement classification, a server and a storage medium in the field of information technologies. The method includes: obtaining, according to text information of an advertisement to be classified, a plurality of feature words of the text information; acquiring a Term Frequency-Inverse Document Frequency value of each feature word from the plurality of feature words as a weight value of the feature word, according to statistical information of the feature word in the text information and statistical information of the feature word in known product titles; and acquiring a category of the advertisement according to the weight values of the plurality of feature words, classification information of the advertisement and a preset classification model. Accordingly, selecting the data from the advertisement in a manner of manual labeling is avoided, so that the time taken for advertisement classification is reduced.";0
707;2016283469;2016;;Babelman LLC;Babelman LLC;H04R1/02,H04R2201/023,H04S2420/01,G10L13/00,G10L21/0208,H04R29/001,G10L15/26,G10L2021/02087,H04S7/00,G06F40/58;WEARABLE TRANSLATION DEVICE;A wearable translation device that provides real-time language translation without a network connection is provided. The wearable translation device picks up speech from a user in a first language using a microphone facing the user, translates it into a second language, and outputs synthesized speech in the second language through a speaker facing the listener. The use of large speakers allows for greater comprehensibility than with existing systems. In some embodiments, noise cancellation signals are output through a speaker facing the user to reduce the amount of the user's voice and ambient background noise that is audible to the listener. In some embodiments, the wearable translation device provides two-way translation.;1
708;2016307239;2016;;Yahoo Japan Corporation;Yahoo Japan Corporation;G06Q30/0276;GENERATION APPARATUS, GENERATION METHOD, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM;A generation apparatus includes an acquiring unit that acquires a first content to he displayed in a display area included in a predetermined content, and a second content that is provided by a provider different from a provider of the first content. The generation apparatus includes a generating unit that generates a third content, in which the first content and the second content arc superimposed such that at least a part of the second content is visible.;4
709;2016328014;2015;;King's Metal Fiber Technologies Co., Ltd.;King's Metal Fiber Technologies Co., Ltd.;G06F3/015,G10L13/033,G06F3/011,G06F3/0304,G06F3/017,G06F3/0321,G06F3/167,G10L21/003;SITUATIONAL SIMULATION SYSTEM;A situational simulation system is provided, using an intelligent system connected to an input unit and an interactive display interface. The input unit receives dynamic images and audio and transforms the dynamic images and audio into action signals and speech signals and transmits the action signals and the speech signals to the intelligent system so that the intelligent system performs computation and analysis based on the input dynamic action signals and speech signals to simulate the dynamic images and audio. Then, the interactive display interface displays the simulated dynamic images and audio from the intelligent system and interacts with the users. As such, the situational intelligent system is used to generate simulated images and audio so that the users can interact through the interactive display interface to achieve a realistic effect of situational simulation for practical application.;6
710;2016371887;2014;;SNECMA;SNECMA;G06T15/20,G06T7/70,G06T7/0004,G06T2207/30108,G06T19/006,G06T2200/04;AUGMENTED REALITY METHOD AND SYSTEM FOR MONITORING;"A method for displaying image for the supervision of a device (20) by means of a system comprising a display (14) and cameras (24). The method comprises the following steps: a0) a state parameter of the device is acquired; a1) an active camera is selected; a2) acquired image parameters and synthesis image parameters are determined; b2) a camera image is acquired according to the acquired image parameters; c) a synthesis image of the device is calculated according to the synthesis image parameters; d) the acquired image and the synthesis image are combined realistically to form a supervision image which is displayed. At step a2), the image parameters are determined, and/or at step c), the synthesis image is calculated, and/or at step d), the supervision image is formed, as a function of the state parameter of the device. A supervision system for carrying out the method.";2
711;2016379252;2015;;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06F16/951,G06Q30/0255,G06Q50/01,H04L67/53,H04L67/535;DYNAMIC GENERATION OF ADVERTISEMENT BASED UPON USER NEED;"Provided are techniques for the generation of advertising content based upon users' needs and use cases. The techniques include monitoring social media to generate a history of user interest; identifying a current interest of a user based upon information derived from a group of sources, the first group of sources comprising: social media data corresponding to the user; communications to and from the user; interactions between the user and other users; calendar entries of the user; and a location corresponding to the user; predicting a user case corresponding to a product such that the use case conforms to the current interest; generating an advertisement based upon the use case and a second group of sources, the second group of sources comprising literature corresponding to the product; user feedback with respect to the product; user ratings of the product; and blogs referencing the product; and displaying the advertisement to the user.";1
712;2016379611;2015;;MEDIALAB SOLUTIONS CORP.;MEDIALAB SOLUTIONS CORP.;G10H1/0066,G10H2240/181,G10H2220/096,G10H2210/145,G10H2210/576,H04M19/02,G10H2210/125,G10H2240/251,G10H1/0025,G10H2230/021,H04W4/14,G10H2250/211;Systems and Method for Music Remixing;Systems and methods for creating, modifying, interacting with and playing music are provided, preferably employing a top-down process, where the user is provided with a musical composition that may be modified and interacted with and played and/or stored (for later play). The system preferably is provided in a handheld form factor, and a graphical display is provided to display status information, graphical representations of musical lanes or components which preferably vary in shape as musical parameters and the like are changed for particular instruments or musical components such as a microphone input or audio samples. An interactive auto-composition process preferably employs musical rules and a pseudo random number generator, which may also incorporate randomness introduced by timing of user input or the like. The user may quickly begin creating desirable music in accordance with one or a variety of musical styles, with the user modifying the auto-composed (or previously created) musical composition, either for a real time performance and/or for storing and subsequent playback. An analysis process flow also is disclosed for using pre-existing music as input(s) to an algorithm to derive music rules that may be used as part of a music style in a subsequent auto-composition process. In addition, the present invention makes use of node-based music generation as part of a system and method to broadcast and receive music data files, which are then used to generate and play music. By incorporating the music generation process into a node/subscriber unit, bandwidth requirements are lowered, and consequently the bandwidth can preferably be used for additional features such as node-to-node and node-to-base music data transmission. The present invention is characterized by the broadcast of relatively small data files that contain various parameters sufficient to describe the music to the node/subscriber music generator. In addition, improved audio synthesis in a portable environment is provided with the present invention by performing audio synthesis in a manner that simplifies design requirements and/or minimizes cost, while still providing quality audio synthesis features targeted for a portable system (e.g., portable telephone). In addition, problems associated with the tradeoff between overall sound quality and memory requirements in a MIDI sound bank are addressed in the present invention by providing systems and methods for a reduced memory size footprint MIDI sound bank. In addition, music ringtone alert tone remixing, navigation, and purchasing capabilities are disclosed that are particularly advantageous in the context of a portable communications device, such as a cellular telephone, in connection with a communications network.;3
713;2017018270;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L15/065,G10L15/063,G10L13/00,G10L25/30;SPEECH RECOGNITION APPARATUS AND METHOD;A speech recognition apparatus includes a converter configured to convert a captured user speech signal into a standardized speech signal format, one or more processing devices configured to apply the standardized speech signal to an acoustic model, and recognize the user speech signal based on a result of application to the acoustic model.;0
714;2017032245;2016;;The Board of Trustees of the Leland Stanford Junior University;The Board of Trustees of the Leland Stanford Junior University;G06N3/044,G06N3/08,G06N3/045;Systems and Methods for Providing Reinforcement Learning in a Deep Learning System;Systems and methods for providing reinforcement learning for a deep learning network are disclosed. A reinforcement learning process that provides deep exploration is provided by a bootstrap that applied to a sample of observed and artificial data to facilitate deep exploration via a Thompson sampling approach.;0
715;2017060523;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G11B27/34,G11B27/105,G06F3/167,G11B27/031,G06F3/165;ELECTRONIC DEVICE AND CONTROLLING METHOD THEREOF;"An electronic device and a controlling method thereof are provided. The electronic device may include a microphone configured to receive sound and generate an audio signal based on the sound; a communicator configured to perform communication with an external device; and a controller configured to control the communicator to transmit the audio signal to external device and receive information regarding music content corresponding to the audio signal from the external device.";0
716;2017068514;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G06F2200/1637,G06F2203/04802,G06F1/1694,G06F3/0485,G06F3/167,G06F3/0346,G06F3/165,G06F3/0482,G06F3/04842,G06F3/0486,G06F3/0488;ELECTRONIC DEVICE AND METHOD FOR CONTROLLING THE SAME;An electronic device and a method of controlling the same, in which the electronic device includes a display configured to display a polyhedral user interaction (UI), a sensor configured to sense a user interaction with respect to the polyhedral UI, an audio outputter configured to reproduce music content, and while an image of a first music content is being displayed on a first face of a plurality of faces of the polyhedral UI, a controller is configured to control the audio outputter to reproduce a second music content which is in the same category as the first music content in response to the user interaction sensed by the sensor being a first interaction, and control the audio outputter to reproduce a third music content which is in a different category from the first music content, in response to the user interaction sensed by the sensor being a second interaction.;0
717;2017069086;2016;;XUANWU HOSPITAL OF CAPITAL MEDICAL UNIVERSITY;XUANWU HOSPITAL OF CAPITAL MEDICAL UNIVERSITY;A61B5/0042,G16H30/40,A61B2576/026,G06T2207/30016,G01R33/5608,G06T7/35,G06V10/772,G06F18/28,A61B5/055,G06T2207/10088;METHOD AND DEVICE FOR CONSTRUCTING BRAIN TEMPLATES;Embodiments of the present disclosure provide a method for constructing brain templates. First, thousands of brain MRI images are collected. Then, the collected brain MRI images are preprocessed and normalized to construct brain templates. Further, the constructed brain templates include brain templates corresponding to different age and gender groups and brain tissue probability maps corresponding to different age and gender groups. Therefore, embodiments of the present disclosure can construct brain templates by using the large sample size of brain MRI data, and support clinical applications and brain researches.;0
718;2017072640;2015;;Caterpillar Inc.;Caterpillar Inc.;G05B2219/35134,G05B19/4099,B33Y50/00;3D Printing Portal for Crowd-Sourced Designs;A method includes receiving a design file in a first format at a computer-based system, wherein the design file is indicative of a structural design. The method includes categorizing the structural design into a first category based on at least a structure type associated with the structural design. The method includes analyzing the structural design, by the computer-based system, based on at least a structural criterion and assigning a structural value to the structural design based on at least the analyzing of the structural design. The method includes generating an additive manufacturing design based on at least the structural design, wherein the additive manufacturing design is generated in a second format and is configured to be processed by an additive manufacturing machine to construct at least a portion of a structure based on at least the structural design in a layer-by-layer manner. The method includes causing a display to be rendered, the display comprising a three-dimensional representation of the structural design, the first category, and the first structural value, wherein the structural type is at least one of a building type or a component type.;3
720;2017076327;2015;;YAHOO HOLDINGS, INC.;YAHOO HOLDINGS, INC.;G06F16/285,G06F16/9535,G06Q30/0261,G06Q30/0275,G06Q30/0256,G06Q30/0277,G06F16/954;METHOD AND SYSTEM FOR DYNAMICALLY PROVIDING ADVERTISEMENTS FOR COMPARISON;The present teaching relates to providing dynamic advertisements for comparison. In one example, a request is received for selecting advertisement. The request incorporates a query. The query is analyzed to determine a category of product and determine one or more attributes of the category of product based on the query. At least one advertisement associated with the category of product is selected based on the one or more attributes and the request. An instruction on presentation of the at least one advertisement is generated in accordance with the one or more attributes. Information related to the instruction and the at least one advertisement is sent as a response to the request.;0
721;2017083933;2016;;Yahoo Japan Corporation;Yahoo Japan Corporation;G10L13/00,G01C21/3697,H04W8/24,G01C21/3629,G10L15/00,G06Q30/0242;INFORMATION PROCESSING DEVICE, INFORMATION PROCESSING METHOD, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM;An information processing device that performs navigation processing for searching a route to a destination and presenting a guide route in accordance with a search result includes an output control module that causes a voice output module to output, by voice, a voice advertisement or a questionnaire related to a voice advertisement based on a conversation with a user during the navigation processing.;0
722;2017084266;2014;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L13/047,G10L13/02,G10L25/93,G10L25/78,G10L15/24,G10L13/00,G10L13/08;VOICE SYNTHESIS APPARATUS AND METHOD FOR SYNTHESIZING VOICE;"A voice synthesis apparatus is provided. The voice synthesis apparatus includes: an electrode array configured to, in response to voiceless speeches of a user, detect an electromyogram (EMG) signal from skin of the user; a speech activity detection module configured to detect a voiceless speech period of the user; a feature extractor configured to extract a signal descriptor indicating a feature of the EMG signal for the voiceless speech period; and a voice synthesizer configured to synthesize speeches by using the extracted signal descriptor.";4
723;2017084268;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L15/14,G10L15/18,G10L15/07,G10L2015/0635,G10L15/08,G10L15/183,G10L2015/226,G10L15/063,G10L2015/228,G10L2015/227;APPARATUS AND METHOD FOR SPEECH RECOGNITION, AND APPARATUS AND METHOD FOR TRAINING TRANSFORMATION PARAMETER;Provided are a method and an apparatus for speech recognition, and a method and an apparatus for training transformation parameter. A speech recognition apparatus includes an acoustic score calculator configured to use an acoustic model to calculate an acoustic score of a speech input, an acoustic score transformer configured to transform the calculated acoustic score into an acoustic score corresponding to standard pronunciation by using a transformation parameter, and a decoder configured to decode the transformed acoustic score to output a recognition result of the speech input.;1
724;2017084274;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L13/02,G10L15/1822,G10L2015/225,G10L15/1815,G10L2015/221,G10L15/183,G10L15/02,G10L15/22;DIALOG MANAGEMENT APPARATUS AND METHOD;An intelligent dialog processing apparatus and method. The intelligent dialog processing apparatus includes a speech understanding processor, of one or more processors, configured to perform an understanding of an uttered primary speech of a user using an idiolect of the user based on a personalized database (DB) for the user, and an additional-query processor, of the one or more processors, configured to extract, from the primary speech, a select unit of expression that is not understood by the speech understanding processor, and to provide a clarifying query for the user that is associated with the extracted unit of expression to clarify the extracted unit of expression.;1
726;2017124187;2015;;Renault S.A.S.;Renault S.A.S.;G06F16/686,G11B27/34,G11B27/105,G11B27/28,G06F16/683,G06F16/639,G06F16/634;METHOD OF ADAPTATION OF A REPRESENTATION OF MUSICAL CONTENT;A method adapts a representation of musical contents. The method includes generating a musical content representation centered on a first musical content and playing the musical content on which the representation is centered. The musical content on which the representation is centered is played so long as no action is undertaken by the user. The method also includes playing a new track when an action ending the play of the musical content is undertaken by the user, continuing the play of the musical content when an action not ending the play of the musical content is undertaken and the action is taken into account, playing new musical content when the play of the musical content on which the representation is centered is finished, and modifying the representation with each new play of a musical content so as to be centered on the new musical content played.;1
727;2017143207;2015;;SUNGSHIN WOMEN'S UNIVERSITY INDUSTRY-ACADEMIC COOPERATION FOUNDATION;SUNGSHIN WOMEN'S UNIVERSITY INDUSTRY-ACADEMIC COOPERATION FOUNDATION;A61B5/0042,A61B2576/026,G16H30/40,G06T2207/30016,G06T5/92,A61B5/055,A61B5/726,G06T2207/10088,G06T7/0012,A61B5/4064,G06T7/11,G06T2207/30096;Apparatus and method for detecting lesion in brain magnetic resonance image, and computer-readable recording medium for implementing the method;"Disclosed is an apparatus and method for detecting a brain lesion in a magnetic resonance (MR) image, and computer-readable recording medium for implementing the method. The apparatus includes an image area selector for receiving an MR image and creating an image of a brain portion (brain portion image) in which the brain portion is selectively presented; and an image processor for receiving the brain portion image and performing contrast adjustment on the brain portion image to obtain an image for diagnosis in which an area with a suspected lesion is emphasized.";0
728;2017169822;2016;;HITACHI, LTD.;HITACHI, LTD.;G10L15/19,H04M11/10,H04M2203/40,G10L17/06,G10L15/22,G06F16/345,G06F16/3329,G10L15/16;DIALOG TEXT SUMMARIZATION DEVICE AND METHOD;"Provided is a summarization technology for correcting a dialog text on a word-by-word basis for readability using a dialog structure. A dialog text summarization device includes: a recognition result acquisition unit that acquires, from a database, a word recognized from a dialog form text, time-series information of the word, and identification information identifying a speaker of the word; and a text summarization unit that corrects the word based on the word, the time-series information of the word, the identification information, and a summarization model, and that outputs a correction result to the database.";0
729;2017178204;2015;;digi.me Limited;digi.me Limited;H04L63/102,G06F21/6218,H04W12/084,G06Q30/0283,G06F16/437,G06Q30/0631,G06Q30/0282;Recommendation Generation;"A method of recommending a supply by a recommender to a user having personal data available electronically consists in the steps of: â¢ granting permission by a user for a facilitator's software to retrieve an excerpt of the user's personal data for use in generating a recommendation; â¢ organising the user's data into a standard, interrogatabie format and storing the formatted data in a storage location; â¢ pre-allocating by a facilitator' software to the recommender a recommender's code for use in conjunction with the recommender's recommendation generating software, the latter including a specification for certain fields of user's data, requesting by the user of a recommendation of goods or services via the recommender's software; â¢ running of the recommender's software to pass the recommender's pre-allocated code to the user's data processor; â¢ passing of the user's pre-allocated code and the recommender's pre-allocated code to the facilitator's interface software; â¢ running of the facilitator's software to retrieve an excerpt of the user's data for the specified fields from the storage location for use by the recommender's software in generating the recommendation; â¢ running of the recommender's software to generate the recommendation for the user utilising the excerpt of the user's data.";1
730;2017199958;2017;;Fujitsu Limited;Fujitsu Limited;G16C20/30,G16B5/00,G16B15/00,G16B15/30,G16B5/30,G16C10/00,G06F7/535,G06F17/18,G16C20/50;METHOD FOR CALCULATING INTERACTION ENERGY, CALCULATION DEVICE, PROGRAM, AND NON-TRANSITORY RECORDING MEDIUM;A method for calculating interaction energy between a target molecule and a drug candidate molecule, the method including: dividing a trajectory over a total time duration of a molecular dynamic simulation of the target molecule and the drug candidate molecule into groups based on molecular mechanic interaction energy between the target molecule and the drug candidate molecule calculated by molecular mechanics, where the method is a method for calculating the interaction energy between the target molecule and the drug candidate molecule using a calculator.;0
731;2017206328;2015;;FiNC Co. Ltd.;FiNC Co. Ltd.;G16H20/60,G06F40/30,G16H40/67,G06F40/232;HEALTHCARE SERVER, HEALTHCARE SERVER CONTROL METHOD, AND NON-TRANSITORY COMPUTER READABLE MEDIUM;A healthcare server connected to a terminal over a network includes a storage unit that stores healthcare target information of a healthcare target having the terminal, question information, and answer information, a reception unit that receives image information or message information transmitted from the terminal, an analysis unit that analyzes language information based on the received message information and acquires the question information from the analyzed language information, a generation unit that extracts the answer information corresponding to the acquired question information and generates a sentence example based on the extracted answer information, an evaluation unit that evaluates the extracted answer information with a degree of confidence indicating the certainty of the answer information, a correction unit that corrects the sentence example and the evaluation based on the healthcare target information, and a transmission unit that transmits the corrected sentence example and evaluation to the terminal.;2
732;2017225077;2016;;MINKONET CORPORATION;MINKONET CORPORATION;A63F13/352,A63F13/86,A63F13/63,A63F13/525;SPECIAL VIDEO GENERATION SYSTEM FOR GAME PLAY SITUATION;Provided is a special video generation system for a game play situation, generating and providing a special video of a game play situation being played on a game terminal, for example, as a 360-degree video or a virtual reality (VR) video. According to the special video generation system for a game play situation, the special video of the game play situation may be easily generated without greatly increasing a calculation load of the game terminal. In addition, according to the special video generation system for a game play situation, interest of game users in playing games may be aroused by providing the special video of the game play situation.;3
733;2017235888;2017;;Tellit Health, Inc.;Tellit Health, Inc.;G06F16/345,G06F40/30,G16H10/60,G06F40/284,G06F40/211,G06F40/295;Systems and Methods for Creating Contextualized Summaries of Patient Notes from Electronic Medical Record Systems;"A computer-implemented method includes: (1) receiving at least one patient note from an electronic medical record (EMR) system as a source text narrative; (2) deriving lexical chains corresponding to themes in the source text narrative; (3) scoring the lexical chains with respect to a medical taxonomy to identify higher scoring lexical chains among the lexical chains; (4) scoring sentences in the source text narrative with respect to the higher scoring lexical chains to identify higher scoring sentences among the sentences; and (5) creating a textual summary of the source text narrative from the higher scoring sentences.";2
734;2017242847;2016;;Kabushiki Kaisha Toshiba;Kabushiki Kaisha Toshiba;G06F40/51,G06F40/284,G06F40/58,G10L15/26,G06F40/42,G06F40/242,G10L13/08;APPARATUS AND METHOD FOR TRANSLATING A MEETING SPEECH;According to one embodiment, a speech translation apparatus includes a speech recognition unit, a machine translation unit, an extracting unit, and a receiving unit. The extracting unit extracts words used for a meeting from a word set, based on information related to the meeting, and sends the extracted words to the speech recognition unit and the machine translation unit. The receiving unit receives the speech in a first language in the meeting. The speech recognition unit recognizes the speech in the first language as a text in the first language. The machine translation unit translates the text in the first language into a text in a second language.;1
735;2017243520;2017;;Fujitsu Limited;Fujitsu Limited;H04N21/47,H04R2430/20,H04N21/4884,G06F3/167,G06T11/60,G10L2021/02166,G06F3/013,G09B21/009,H04R3/005,H04N5/44504,H04R3/00,G06F1/163,G10L25/51,G10L15/26,H04R1/40,G02B2027/014,G06F3/011,G02B27/0172,G09B21/00;WEARABLE DEVICE, DISPLAY CONTROL METHOD, AND COMPUTER-READABLE RECORDING MEDIUM;A wearable device is provided that includes a microphone, a display, and a controller. The controller analyzes sound picked up by the microphone, identifies a direction from which sound was emitted, and performs control so as to cause information indicating the identified emitted direction to be displayed on the display.;0
737;2017243600;2017;;Fujitsu Limited;Fujitsu Limited;G10L21/10,G09B21/00,H04R3/00,H04R1/40,G10L25/78,H04R3/005,G10L25/51,G10L2021/065,G06F3/167,G06F3/04817,H04R1/406,G10L15/26;WEARABLE DEVICE, DISPLAY CONTROL METHOD, AND COMPUTER-READABLE RECORDING MEDIUM;A wearable device is provided that includes a microphone, a display, and controller. The controller analyzes audio information picked up by the microphone, and, when audio corresponding to a predetermined verbal address phrase has been detected in the acquired audio information, causes the display to display an indication of an utterance of a verbal address on the display.;0
738;2017256259;2016;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G10L15/04,G10L2015/225,G10L25/87,G10L2015/0635,G10L15/02,G10L15/183,G10L15/063,G10L15/22,G10L15/19,G10L2015/088,G10L15/222,G10L2015/221;Speech Recognition;"A computer system comprises an input configured to receive voice input from a user, the voice input having speech intervals separated by non-speech intervals; an ASR system configured to identify individual words in the voice input during speech intervals thereof, and store the identified words in memory; a response generation module configured to generate based on the words stored in the memory an audio response for outputting to the user; and a response delivery module configured to begin outputting the audio response to the user during a non-speech interval of the voice input, wherein the outputting of the audio response is terminated before it has completed in response to a subsequent speech interval of the voice input commencing whilst the audio response is still being outputted.";1
739;2017270948;2014;;ZTE CORPORATION;ZTE CORPORATION;G06V40/171,G10L25/63,G06T13/40,G10L15/25,G10L15/22,G10L21/10,G06V40/176,G11B27/031;METHOD AND DEVICE FOR REALIZING VOICE MESSAGE VISUALIZATION SERVICE;"A method and device for realizing a voice message visualization service are described. The device includes an information receiving module for receiving an original message and a portrait picture sent by a message sender or locally stored, wherein the original message is a text message or a voice message; and a dynamic video generation module for extracting facial features from the portrait picture, generating a facial expression, and synthesizing the facial expression and the original message into dynamic video information, the generated facial expression corresponding to the content of the original message. The dynamic video information is sent to a message receiver and is displayed on a terminal of the message receiver.";6
742;2017316453;2017;;Yahoo Japan Corporation;Yahoo Japan Corporation;G06Q30/0244;INFORMATION PROCESSING APPARATUS AND INFORMATION PROCESSING METHOD;An information processing apparatus includes a determination unit that determines the degree of familiarity between a user and an artifact that performs interactive interaction with the user, on the basis of at least one of the type and the amount of the user's action detected when the artifact is accompanied by the user, and a change unit that changes the content of the interaction in accordance with the degree of familiarity determined by the determination unit.;0
743;2017323313;2017;;Alibaba Group Holding Limited;Alibaba Group Holding Limited;G06Q50/01,G06N20/00,G06N5/01,G06Q30/0201,G06N7/01,H04L51/52;INFORMATION PROPAGATION METHOD AND APPARATUS;"An information propagation method includes: determining a first user corresponding to to-be-propagated information, the first user being a user whose influence is greater than a preset value in an interest type network to which the first user belongs; and acquiring a user relation network that takes the first user as a starting point, and propagating the information in the user relation network by taking the first user as a starting point. The method improves efficiency and credibility of information propagation.";0
744;2017330388;2017;;Ramot at Tel-Aviv University Ltd.;Ramot at Tel-Aviv University Ltd.;G06T17/10,G06T2200/08,G06T19/20,G06T2219/2021;THREE-DIMENSIONAL MODELING FROM SINGLE PHOTOGRAPHS;"A method of obtaining a three-dimensional digital model of an artificial object, made up of a plurality of geometric primitives, the artificial object being in a single two-dimensional photograph, the method comprising: using edge detection to define a two-dimensional outline of the artificial object within the photograph; interactively allowing a user to define two-dimensional profiles of successive ones of the geometric primitives; interactively allowing a user to sweep respective profiles over an extent of a corresponding one of the geometric primitives within the image; generating successive three-dimensional model parts from existing detected edges of the corresponding geometric primitives and the sweeping of the respective profile; and aligning the plurality of three-dimensional model parts to form the three-dimensional model.";3
745;2017345412;2015;;NEC CORPORATION;NEC CORPORATION;G10L13/027,G10L13/07,G10L13/10;SPEECH PROCESSING DEVICE, SPEECH PROCESSING METHOD, AND RECORDING MEDIUM;A speech processing device according to an aspect of the present invention examines precision and quality of each piece of data stored in a database so that it is able to generate highly stable synthesized speech close to human voice A speech processing device according to an aspect of the present invention includes a first storing means for storing an original-speech F0 pattern being an F0 pattern extracted from recorded speech and first determination information associated with the original-speech F0 pattern, and a first determining means for determining whether or not to reproduce an original-speech F0 pattern, in accordance with first determination information.;3
747;2017358003;2015;;SK PLANET CO., LTD.;SK PLANET CO., LTD.;G06Q30/0267,G06Q30/0256,G06Q30/0241;OBJECT RECOGNITION BASED RETARGETING ADVERTISEMENT PRODUCT RECOMMENDING SERVER, CONTROL METHOD THEREOF, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM HAVING COMPUTER PROGRAM RECORDED THEREON;Disclosed are an object recognition based retargeting advertisement product recommending terminal, a server, an advertisement product recommending system including the same, a control method thereof, and a non-transitory computer readable storage medium having computer program recorded thereon. That is, when a terminal requests an advertisement, a tracking log including information on an advertisement, and the like, which has been searched before in the terminal and an advertisement related to an object corresponding to a unique ID of the object which is registered in advance in an advertising terminal matching a unique ID of the object corresponding to unique identification information of the terminal are provided to the terminal. Therefore, an advertisement suitable for a user may be transmitted through intuitive analysis of a favorable impression of a product of an advertisement receiver and thus an advertisement effect is increased.;0
750;2017366479;2016;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;H04L51/02,H04L51/23,H04L51/046;Communication System;"A computer system comprises computer storage holding at least one code module configured to implement a bot, and at least one processor configured to execute the code module. The computer system also comprises a communication system for effecting communication events between users of the communication system; a bot interface for exchanging messages between the communication system and the bot; and a dialogue manager. The communication system transmits, to the dialogue manager directly, content of a first message received at a processor of the communication system from a user of the communication system. The dialogue applies an intent recognition process to the content to generate at least one intent identifier, and transmits a second message comprising the intent identifier to the bot using the bot interface. The bot automatically generates a response using the intent identifier received in the second message, and transmits the generated response to at least the user.";2
751;2017371506;2017;;BEIJING XIAOMI MOBILE SOFTWARE CO., LTD.;BEIJING XIAOMI MOBILE SOFTWARE CO., LTD.;G06F16/583,H04L67/01,G06Q50/01,G06V40/161,G06V40/103,H04L67/10,G06Q50/00,G06F3/0482,G06V40/172;METHOD, DEVICE, AND COMPUTER-READABLE MEDIUM FOR MESSAGE GENERATION;"Methods, devices and computer-readable medium for generating a message are provided in this disclosure. The method for generating the message includes receiving a target image transmitted from a terminal, the target image including an image of person; acquiring a similar image of the target image from a plurality of images stored in a library, where the plurality of images are images having corresponding contact information; transmitting the similar image and the contact information corresponding to the similar image to the terminal, such that the terminal is enabled to a recommendation message based on the similar image and the contact information.";1
753;2018007404;2015;;CREA-JAPAN INC.;CREA-JAPAN INC.;H04N21/854,H04N9/8227,H04N21/83,G06F40/103,H04N19/61,H04N21/234,G06F40/10,H04N9/8233,H04N9/8211,G06F16/9535,H04N21/23418;VIDEO CREATION SERVER, VIDEO CREATION PROGRAM, VIDEO CREATION METHOD, AND VIDEO CREATION SYSTEM;A technique for presenting text information to service users with enhanced visual effects so that the text information makes a strong impression on the users. A video creation server includes an acquisition section configured to acquire material data including one or both of text data and a still image, and a control section configured to acquire a script code editable by a user and to create moving-image data having the material data embedded into each frame of previously defined moving-image data such that the material data moves within the previously defined moving-image data in accordance with the script code.;6
754;2018018392;2015;;Hewlett-Packard Development Company, L.P.;Hewlett-Packard Development Company, L.P.;G06F16/353,G06F16/345;TOPIC IDENTIFICATION BASED ON FUNCTIONAL SUMMARIZATION;Topic identification based on functional summarization is disclosed. One example is a system including a plurality of summarization engines, each summarization engine to receive, via a processing system, a document to provide a summary of the document. At least one meta-algorithmic pattern is applied to at least two summaries to provide a meta-summary of the document using the at least two summaries. A content processor identifies, from the meta-summaries, topics associated with the document, maps the identified topics to a collection of topic dimensions, and identifies a representative point based on the identified topics. An evaluator determines distance measures of the representative point from topic dimensions in the collection of topic dimensions, the distance measures indicative of proximity of respective topic dimensions to the representative point. A selector selects a topic dimension to be associated with the document, the selection based on optimizing the distance measures.;1
755;2018025108;2017;;ACADEMIA SINICA;ACADEMIA SINICA;G16B40/00,G16B50/00,G16B50/10,G16B5/20,G16B5/00;COMPUTATIONAL METHOD FOR PREDICTING FUNCTIONAL SITES OF BIOLOGICAL MOLECULES;In a general aspect, a method for inferring one or more biomolecule-to-biomolecule interaction sites includes receiving data representative of a plurality of prediction models. Each prediction model is associated with a different atom type of a plurality of atom types and characterizes biomolecule-to-biomolecule interaction site specific patterns common to a plurality of three dimensional probability density maps. Each three dimensional probability density map is associated with a corresponding biomolecule of a plurality of biomolecules included in a training data set and represents a probability of a non-covalent interacting atom on a surface of the corresponding biomolecule interacting with the atom type associated with the prediction model. Data representative of a query biomolecule is received, the data including one or more unknown biomolecule-to-biomolecule interaction sites. The one or more unknown biomolecule-to-biomolecule interaction sites of the query biomolecule are inferred based on the data representative of the plurality of prediction models.;0
756;2018025112;2017;;TOPCON CORPORATION;TOPCON CORPORATION;G16H30/20,G06T2207/10024,G06N20/00,G06T2207/10056,G16H50/70,G06T7/0014,G06F18/24,G06T2207/30041,G06V2201/03,G06T2207/20084,G06N3/004,G06T2207/10068,G06V40/197,G06V10/764,G16H50/20,G06T2207/20081;MEDICAL INFORMATION PROCESSING SYSTEM AND MEDICAL INFORMATION PROCESSING METHOD;An aspect of a medical information processing system according to an exemplary embodiment includes an artificial intelligence engine that processes medical information based on a database. The medical information processing system includes a reception unit, first classification processor, selection processor, and second classification processor. The reception unit receives medical information comprising a medical image. The first classification processor classifies, based on the database, the medical image into a category among two or more categories set in advance. The selection processor selects a category among the two or more categories based on the medical information. The second classification processor classifies the medical image into a singular category when the category determined by the first classification processor and the category determined by the selection processor do not agree with one another.;1
758;2018039618;2016;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F40/35,H04L51/02,G06F3/04842,G06F3/0482,H04L51/18,G06Q10/101,G06F40/30;COMPUTERIZED GROUP TASK DIGITAL ASSISTANCE;A digital conversational bot can be joined in a natural language group conversation between profiles over a computer conversation platform. A recommendation option set of multiple options can be generated from an initial option set. The identifying can include analyzing the initial option set using individual data of the profiles pertaining to the task. Also, a natural language script can be generated and transmitted to the profiles via the digital conversational bot as part of the group conversation, with the natural language script describing the options of the recommendation option set. A group consensus of the profiles in selecting a group selected option from the recommendation option set can be facilitated via the digital conversational bot. Additionally, assistance in completion of the task using the group selected option for task completion can be provided via the digital conversational bot.;2
759;2018039767;2016;;ZTE CORPORATION;ZTE CORPORATION;G10L17/06,G10L17/02,G10L17/22,G06F3/0487,G06F3/167,G06F21/84,G10L17/04,G06F21/32;VOICEPRINT-RECOGNITION-BASED SECURITY PROTECTION METHOD AND DEVICE;"Provided is a voiceprint-recognition-based security protection method. The method includes: acquiring voice data of a current user of a terminal and extracting voiceprint characteristic information from the voice data; matching the extracted voiceprint characteristic information of the current user of the terminal with a pre-saved voiceprint model of an owner of the terminal, and judging whether the current user of the terminal is the owner of the terminal; and when judging that the current user of the terminal is not the owner of the terminal, performing security protection processing on the terminal.";0
760;2018061393;2016;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G10L25/63,G10L15/1815,G10L13/04,G06N20/00,G10L13/0335,G10L13/027;SYSTEMS AND METHODS FOR ARTIFICAL INTELLIGENCE VOICE EVOLUTION;Systems and methods for evolving an AI voice are provided herein. More specifically, the systems and methods modify the pitch, duration, volume and/or timbre of an AI voice based on one or more user spoken language inputs and/or the evaluation of other known user data. Accordingly, the systems and methods as disclosed herein provide an AI voice that changes or evolves over time based on the user to increase engagement, trust, and/or emotional connection with the user without requiring any AI voice setting changes by the user.;1
763;2018068329;2016;;INTERNATIONAL BUSINESS MACHINES CORPORATION;INTERNATIONAL BUSINESS MACHINES CORPORATION;G06N3/08,G06T11/60,G06N3/045,G06F16/29,G06V20/13,G06F18/24,G06Q30/0202,G06V10/82,G06F16/58,G06V10/764,G06Q50/16;PREDICTING REAL PROPERTY PRICES USING A CONVOLUTIONAL NEURAL NETWORK;A subset of a set of image data is input into a trained convolutional neural network (CNN), the subset of image data including several of digital images, each image including a depiction of a real estate property at a different zoom level. By executing the CNN, a set of features is extracted from the subset of image data, a feature in the set of features being unrepresented in the subset of image data, and where the feature is derived from a depiction in the subset of image data. Using a set of node values configured at a set of nodes in a layer of the CNN, and using the set of features, a combined value of the set of features is computed, relative to the real estate property. A predicted price of the real estate property is predicted, by executing the CNN, using the combined value.;1
764;2018070872;2016;;The Regents of the University of California;The Regents of the University of California;A61B5/4836,A61B5/162,A63F13/44,A63F13/67,A63F2300/64,A61B5/7475,A61B5/16,A61B5/168;NEUROTHERAPEUTIC VIDEO GAME FOR IMPROVING SPATIOTEMPORAL COGNITION;The disclosed embodiments relate to a system that uses a video game to improve spatial and/or temporal information-processing capabilities of a user. During operation, the system enables the user to play the video game. During execution of the video game, the system first measures spatial and/or temporal information-processing capabilities of the user during the course of playing the video game. Next, the system uses the measured spatial and/or temporal information-processing capabilities to control a spatial placement and/or a temporal presentation rate of target items that the user is required to respond to during subsequent game play to stimulate enhancement of the user's spatial and/or temporal information-processing capabilities.;0
765;2018077460;2016;;The Aleph Group Pte, Ltd;The Aleph Group Pte, Ltd;H04N21/251,H04N21/4668,H04N21/854,H04N21/8106,H04N21/439,H04N21/233,H04N21/84;Method, System, and Apparatus for Providing Video Content Recommendations;"The present inventive subject matter is drawn to method, system, and apparatus for generating video content related to a first audio media asset. In one aspect of this invention, a method for generating recommendation images related to the first audio media asset stored in a computer memory is presented, where a plurality of other audio media assets are compared to the first audio media asset to determine whether the first audio media asset is similar to the other audio media assets; constructing a common metadata document from the metadata documents of the audio assets; and generating a set of recommended video content items.";1
766;2018082184;2016;;TCL RESEARCH AMERICA INC.;TCL RESEARCH AMERICA INC.;G06F40/30,G06F40/56,G10L2015/226,G06F40/289,G06F40/35,G06N3/044,G10L15/22,G06N5/04;CONTEXT-AWARE CHATBOT SYSTEM AND METHOD;"A context-aware chatbot method and system are provided. The context-aware chatbot method comprises receiving a user's voice; converting the user's voice to a question to be answered; determining a question type of the question to be answered; generating at least one answer to the question based on a context-aware neural conversation model; validating the answer generated by the context-aware neural conversation model; and delivering the answer validated to the user. The context-aware neural conversation model takes contextual information of the question into consideration, and decomposes the contextual information of the question into a plurality of high dimension vectors.";3
767;2018082388;2016;;SONY GROUP CORPORATION;SONY GROUP CORPORATION;G06Q30/0278,G06Q30/0283,G06Q50/167,G06Q50/16,G06Q30/0201;SYSTEM, METHOD, AND PROGRAM;"A system that generates a first parameter corresponding to a type of an object; generates a second parameter corresponding to transaction information corresponding to the object; calculates a feature value corresponding to the object by applying a predetermined function to the first and second parameters; generates display data based on the calculated feature value; and outputs the display data to a device remotely connected to the system via a network.";0
768;2018090132;2017;;Toyota Jidosha Kabushiki Kaisha;Toyota Jidosha Kabushiki Kaisha;G10L13/00,G10L15/1815,G10L15/30,G10L15/22,G06F40/35,G10L15/1822;VOICE DIALOGUE SYSTEM AND VOICE DIALOGUE METHOD;A voice dialogue system includes a dialogue scenario storage storing a plurality of dialogue scenarios and a dialogue text generator generating a dialogue text for responding to a user utterance based on a result of voice recognition. The dialogue scenario is a single set of three contents: a content of a first system utterance, a content of an expected user utterance, and a content of a second system utterance for responding to the expected user utterance. The dialogue text generator determines whether or not the user utterance is an expected response and, when the user utterance is an expected response, generates a second system utterance defined in a dialogue scenario as a response to the user utterance as a dialogue text for responding to the user utterance.;1
769;2018101770;2017;;RICOH COMPANY, LTD.;RICOH COMPANY, LTD.;G06N3/047,G06N3/088,G06N3/045;METHOD AND SYSTEM OF GENERATIVE MODEL LEARNING, AND PROGRAM PRODUCT;A generative model learning method includes learning a first generative model by unsupervised learning based on train data prepared beforehand, generating generated data by the first generative model, and learning a second generative model by supervised learning based on the train data and the generated data determined as undesirable by a user.;3
771;2018121784;2017;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;G16H40/63,G16H50/30,A61B5/0295,G09B7/06,G06N20/00,A61B2503/12,G16H10/20,A61B5/024,A61B5/165,A61B5/0205,G06N5/04,G06N3/006,G16H50/20;CONVERSATION CONTROL SYSTEM;A conversation control system includes a conversation device, an acquisition unit that acquires personality information of a user that is registered in advance, a detection unit that detects biological information of the user, an estimation unit that estimates a mental state of the user from the acquired personality information and the detected biological information, and a changing unit that changes a personality of the conversation device in accordance with the estimated mental state of the user.;0
772;2018131642;2017;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G10L15/30,G10L2015/223,G10L15/26,H04L51/02,G10L15/22,G10L13/08,G10L13/00;CONVERSATION RUNTIME;Examples are disclosed that relate to a conversation runtime for automating transitions of conversational user interfaces. One example provides a computing system comprising a logic subsystem and a data-holding subsystem. The data-holding subsystem comprises instructions executable by the logic subsystem to execute a conversation runtime configured to receive one or more agent definitions for a conversation robot program, each agent definition defining a state machine including a plurality of states, detect a conversation trigger condition, select an agent definition for a conversation based on the conversation trigger condition, and execute a conversation dialog with a client computing system using the agent definition selected for the conversation and automatically transition the state machine between different states of the plurality of states during execution of the conversation dialog.;1
774;2018150768;2017;;Gluru Limited;Gluru Limited;G06N3/044,G06N20/00,G06N5/022,G06F40/35,G06F40/30,G06F40/268,G06N5/041,G06F40/211,G06N3/08,G06F40/166;AUTOMATED GENERATION OF NATURAL LANGUAGE TASK/EXPECTATION DESCRIPTIONS;"Embodiments of the present invention are directed to computer-implemented methods and systems for automatically generating a description of a task or expectation. The method comprises receiving, in the form of an electronic communication, a natural language sentence that expresses a call or commitment to action; generating, using a machine learning model, a description of a task or expectation for a user based on the natural language sentence; and storing the description of the task or expectation in a non-transitory computer-readable memory.";2
777;2018189797;2016;;Wipro Limited;Wipro Limited;G06Q50/22,H04L63/20,G06Q30/018,G06F21/552,G06Q10/06395,G06Q10/0635;VALIDATING COMPLIANCE OF AN INFORMATION TECHNOLOGY ASSET OF AN ORGANIZATION TO A REGULATORY GUIDELINE;This disclosure relates to system and method for validating compliance of an information technology (IT) asset of an organization to a regulatory guideline. In one embodiment, a method is provided for validating the compliance of the IT asset to a regulatory guideline. The method comprises accessing raw data from a plurality of data sources, wherein the raw data comprises at least one of an operation data, an IT asset data, a regulatory intelligence data, and a regulatory reference data, processing the raw data to extract one or more regulatory parameters, analyzing the one or more regulatory parameters using one or more artificial intelligence computing processes to assess at least one of a regulatory risk and a corresponding regulatory impact, and validating the compliance of the IT asset to the regulatory guideline based on the at least one of the regulatory risk and the corresponding regulatory impact.;0
778;2018189980;2018;;Black Sails Technology Inc.;Black Sails Technology Inc.;H04L43/0888,H04N19/40,G06T15/205,G06F3/012,H04N19/44,G06T3/40,H04N21/231,H04L65/75,H04N21/234381,H04N21/2335,H04N13/275,H04N19/70,H04L67/131,G06F3/011,H04L65/752,H04N21/816,H04N5/76,H04N13/332,H04N13/398,H04N13/378,G06T9/001,H04N13/122,H04N13/383,H04N9/8715,H04N13/189,H04N21/234345,G06T2215/16,H04N13/161,H04N13/344,H04N21/234363,H04N13/139,H04N13/117,G06T15/04;Method and System for Providing Virtual Reality (VR) Video Transcoding and Broadcasting;"Disclosed a method and a system for providing virtual reality (VR) video transcoding and broadcasting. The method comprises: obtaining a use's viewport; processing a VR video data into a basic video set and an enhancement video set in accordance with the user's viewport, wherein the basic video set comprises a plurality of basic video segments, the enhancement video set comprises a plurality of enhancement video segments, and the playback effect of the sum of the basic video segment and the enhancement video segment is better than that of the basic video segment; downloading the basic video segments and the enhancement video segments; and displaying a sum of two video data obtained by adding the basic video segments and the enhancement video segments in accordance with the user's viewport. According to the embodiments of the present disclosure, the VR video data is processed into a basic video set and an enhancement video set and an video data obtained by adding the basic video segments and the enhancement video segments in accordance with the user's viewport is displayed. Thus, viewing experience is ensured while downloaded data is reduced and transmission effect is improved.";0
779;2018190294;2017;;BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.;BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.;G10L15/22,G10L15/26,G10L15/08,G10L2015/225,G06F3/167;INPUT METHOD AND APPARATUS;"The present disclosure provides an input method and apparatus, wherein the method comprises: displaying a speech collecting control on a current interface after a word input function is triggered; collecting speech data after the speech collecting control is triggered; upon completion of the speech data collection, converting the collected speech data into words, and displaying the words obtained from the conversion on the screen. In the present disclosure, it is only necessary to display the speech collecting control on the current interface during the whole word input procedure, without popping up the word input method panel that occupies a lot of screen space. The user may input speech after triggering the speech collecting control, thereby achieving automatic conversion and input of words without requiring the user to put in a lot of energy to manually input words. The present disclosure is particularly adapted for application scenarios such as gaming type applications, video type applications and navigation type applications, improves the user's input efficiency and also reduces the input method's impact on the experience of using the original interface.";1
780;2018203851;2017;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06N3/045,G06F16/50,H04L51/02,G06N3/044,G06F40/35,G06N3/006,G06F40/56,G06T9/002,G06F16/334,G06F40/30,H04L51/04;SYSTEMS AND METHODS FOR AUTOMATED HAIKU CHATTING;Systems and methods for automated (or artificial intelligence) haiku chatting are provided. The systems and methods provide automated haiku chatting by generating, selecting, and/or scoring haikus. The systems and methods provide automated haiku chatting that may generate and/or select a haiku based on previously collected user inputs, that provides an image with a selected and/or generated haiku, that may generate or select a haiku based on an collected image from the user, and/or that may utilize bi-directional recurrent neural network learning model. Further, systems and methods as described herein are able update or train learning models utilized by the systems and/or methods based on user feedback and/or world feedback.;3
781;2018211652;2017;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L15/08,G10L15/14,G10L15/187,G10L2015/025,G10L15/22,G10L15/02;SPEECH RECOGNITION METHOD AND APPARATUS;A speech recognition method includes generating pieces of candidate text data from a speech signal of a user, determining a decoding condition corresponding to an utterance type of the user, and determining target text data among the pieces of candidate text data by performing decoding based on the determined decoding condition.;1
782;2018247183;2018;;RICOH COMPANY, LTD.;RICOH COMPANY, LTD.;G06N5/046,G06N3/045,G06N3/088;METHOD AND SYSTEM FOR GENERATIVE MODEL LEARNING, AND RECORDING MEDIUM;"A system and a method for learning generative model includes: first learning a generative model for generating data based on first learning data; and second learning the generative model being learned in the step of first learning based on second learning data, and the step of first learning and the step of second learning are repeated.";3
783;2018268816;2018;;Yahoo Japan Corporation;Yahoo Japan Corporation;G10L13/00,G10L15/02,G10L15/22,G10L15/16,G10L15/04;GENERATING DEVICE, GENERATING METHOD, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM;A generating device includes an accepting unit that accepts a speech of a user. The generating device includes a generating unit that, by inputting the speech of the user to a single model in which a group of parameters are learned simultaneously to output a response directly from a speech, generates a response to the speech.;3
786;2018294059;2018;;Savant Care Inc.;Savant Care Inc.;G16H50/20,G06N3/08,G06N5/04,G16H40/20,G06N3/02,G16H10/60,G16H20/00;Mental Health Modeling Language;The invention is a novel, internet-enabled doctor-patient workflow system comprising, inter alia, an âintelligentâ electronic health record and healthcare management process, offering an interactive âmachine-learningâ electronic health record and medical management system. The invention features inputs and commands from doctors through the use of a conversation pane (conversation window). The invention uses artificial intelligence and machine learning algorithms to accomplish routine activities via short code commands and auto-fill menu-populating technology which adapts itself to a particular physician's style as the System is used.;1
787;2018301144;2016;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G10L15/22,G10L15/10,G10L15/06,G10L15/063,G10L15/07,G10L2015/0635,G10L2015/025;ELECTRONIC DEVICE, METHOD FOR ADAPTING ACOUSTIC MODEL THEREOF, AND VOICE RECOGNITION SYSTEM;"An electronic device, a method for adapting an acoustic model thereof, and a voice recognition system are provided. According to one embodiment of the present invention, the electronic device comprises: a voice input unit for receiving a voice signal of a user; a storage unit for storing, therein, a transformer having a plurality of transformation parameters and an acoustic model having a parameter transformed by the transformer; and a control unit for generating a hypothesis from the received voice signal by using the acoustic model, estimating, by using the hypothesis, an optimal transformer having an optimal transformation parameter on which a voice feature of the user is reflected, and updating the plurality of transformation parameters of the transformer stored in the storage unit by combining the estimated optimal transformer with the transformer.";1
788;2018330742;2018;;OLYMPUS CORPORATION;OLYMPUS CORPORATION;H04R1/326,G10L21/0232,H04R2227/001,H04R2460/01,H04R2225/49,G10L25/84,H04R3/04,H04R2410/01,H04R2410/03,G10L21/0364,H04R1/1083,H04R1/222,H04R2410/07,G10L15/26,H04R2410/05;SPEECH ACQUISITION DEVICE AND SPEECH ACQUISITION METHOD;A speech acquisition device, comprising a microphone for converting speech to voice data, and a sound quality adjustment circuit that adjusts sound quality of the voice data, wherein the sound quality adjustment circuit performs different sound quality adjustment in a case where a transcript is created using speech recognition and in a case where a transcript is created by a person listening to speech.;1
789;2018330818;2018;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G16H40/40,G16H40/67,G16H30/20,G16H40/60,G16H30/40;METHOD OF PROVIDING SCAN PROTOCOL INFORMATION TO MEDICAL DEVICE AND ELECTRONIC DEVICE THEREFOR;"A method, performed by an electronic device, of providing configuration information related to image scanning to a medical device includes obtaining identification information of the medical device and a list of a plurality of configuration information corresponding to information of an examinee; identifying first configuration information from the list of the plurality of configuration information as recommendation information based on a negative index; and displaying the first configuration information identified as the recommendation information on a display.";0
790;2018331839;2016;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;H04L51/02,H04L51/04,G06Q10/107,H04L12/1813,G06F9/453;EMOTIONALLY INTELLIGENT CHAT ENGINE;A chat engine is disclosed herein that can conduct emotionally intelligent chat conversations with client device users. User chat responses and surrounding environmental data are analyzed to respectively detect the user's emotional state and surrounding environments. A series of response selector components identify or generate possible chat responses to a user's chat statements based on the detected emotional states environment of the user. Emotionally intelligent chat responses are selected for presentation to a user based on calculated likelihoods that the responses will likely change or maintain the user's emotional state. Using the techniques disclosed herein, the chat engine tailors conversational responses to a user depending the user's detected emotional state.;3
791;2018332167;2017;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F18/24,G06F16/3322,G06F16/951,G06N3/004,G06F16/9538,G06F16/3329,H04M3/4936,G06N3/006,G06V40/1365,G06F16/9535,G10L15/1815,G06F40/40,H04M3/493;CHAT BOT SEARCH RESULTS INTEGRATION;Systems, methods, and computer-executable instructions for verifying a chat bot. Registration information for a chat bot is received and stored. A search query is received from a browser that includes a search keyword. A determination if a chat bot should be included in search engine results page based upon the search query is made. The search query is matched to keywords associated with the chat bot. Using the registration information, if the chat bot is launchable from the search engine results page is determined. The chat bot is integrated in the search engine results page. The search engine results page is provided to the browser.;0
792;2018336439;2017;;Intel Corporation;Intel Corporation;G06N3/088,G06N3/047,G06F18/2433,G06N3/045,G06F18/24,H04L43/04;NOVELTY DETECTION USING DISCRIMINATOR OF GENERATIVE ADVERSARIAL NETWORK;An example apparatus for detecting novel data includes a discriminator trained using a generator to receive data to be classified. The discriminator may also be trained to classify the received data as novel data in response to detecting that the received data does not correspond to known categories of data.;1
794;2018351899;2016;;SONY GROUP CORPORATION;SONY GROUP CORPORATION;G06F16/00,G06F13/00,G06F40/00,G06F16/3344,G06F16/345,H04L51/216,G06F16/353;INFORMATION PROCESSING DEVICE, INFORMATION PROCESSING METHOD, AND PROGRAM;"[Object] To provide an information processing device, an information processing method, and a program that can tally intentions of conversation participating members on the basis of an input conversation content. [Solution] An information processing device including: a tally unit configured to tally intentions of respective conversation participating members regarding acceptance or denial on a basis of a predetermined word included in an input conversation content; and an output control unit configured to perform control to output a tally result of the tally unit.";0
795;2018357543;2018;;Bonsai AI, Inc.;Bonsai AI, Inc.;G06F16/9024,G06N3/045,G06N3/042,G06F30/20,G06N3/044,G06N3/08,G06F18/214,G06F2111/10;ARTIFICIAL INTELLIGENCE SYSTEM CONFIGURED TO MEASURE PERFORMANCE OF ARTIFICIAL INTELLIGENCE OVER TIME;An AI engine configured for measuring training accuracy of one or more AI models over time is disclosed. The AI engine includes, in some embodiments, one or more AI-engine modules including an instructor module, a learner module, and an assessor module. The instructor module is configured to coordinate training for each AI model of the one or more AI models with a corresponding simulator. The learner module is configured to train each AI model with the corresponding simulator on one or more concepts of a mental model defined in a pedagogical programming language. The assessor module is configured to determine when each AI model is sufficiently trained on at least a concept of the mental model by measuring the training accuracy of the AI model over time. The assessor module is also configured to terminate the training of each AI model by ending any simulations of the corresponding simulator.;0
797;2019005013;2018;;BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.;BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.;G06F3/0484,G06F40/30,H04L51/04,H04L51/02,G06F40/35,G06N3/006,H04L51/216,G06N5/02,G06F40/169;CONVERSATION SYSTEM-BUILDING METHOD AND APPARATUS BASED ON ARTIFICIAL INTELLIGENCE, DEVICE AND COMPUTER-READABLE STORAGE MEDIUM;The present disclosure provides a conversation system building method and apparatus based on artificial intelligence, a device and a computer-readable storage medium. In embodiments of the present disclosure, the user only needs to intervene the annotation operation of the conversation samples in the case that the conversation system is not satisfied with the recognition parameters of the input information provided by the user, without manually participating in the annotation operations of all conversation samples. The operations are simple, the correctness rate is high, and thereby the efficiency and reliability of building the conversation system is improved.;1
799;2019013012;2018;;MINDS LAB INC.;MINDS LAB INC.;G06F40/247,G10L2015/0635,G10L15/063,G06F40/216,G10L15/16,G10L25/51,G06F40/289,G06F40/56,G10L15/18;SYSTEM AND METHOD FOR LEARNING SENTENCES;"The present disclosure relates to a system and method of sentence learning based on an unsupervised learning method. For the same, a sentence learning method may include: enhancing a basis sentence corpus by using a word similar to a word included in a basis sentence; performing learning for the basis sentence included in the basis sentence corpus based on an unsupervised learning method; and removing an abnormal sentence among at least one similar sentence obtained by performing of the sentence learning.";2
800;2019013017;2018;;SAMSUNG SDS CO., LTD.;SAMSUNG SDS CO., LTD.;G06N7/01,G10L25/51,G06F40/35,G10L25/63,G10L15/1815,G10L15/22,G06N20/00,G10L2015/223,G06N5/027,G06N5/04,G10L15/14;METHOD, APPARATUS AND SYSTEM FOR PROCESSING TASK USING CHATBOT;Provided is a task processing method performed by a task processing apparatus. The task processing method includes detecting a second user intent different from a first user intent based on an utterance of a user while a first dialogue task including a first dialogue processing process corresponding to the first user intent is being executed, determining whether to initiate execution of a second dialogue task including a second dialogue processing process corresponding to the second user intent based on the detection of the second user intent and generating a response sentence responding to the utterance based on the determination of the initiation of the execution of the second dialogue task.;1
801;2019019304;2018;;FUJIFILM Corporation;FUJIFILM Corporation;G06T2207/10081,G06T2207/10104,G06T7/60,G06T2207/30016,G06T2207/20128,G06T7/30,G06T2207/10088,G06T7/0014,G06T7/0012,G06T7/11;MEDICAL IMAGE PROCESSING APPARATUS, METHOD, AND PROGRAM;An image acquisition unit acquires a brain image of a subject. A non-bleeding region specifying unit specifies a non-bleeding region in the brain image, and a selection unit selects a standard brain image corresponding to at least one of the shape or the size of the non-bleeding region from a plurality of standard brain images. Then, a division unit divides the brain included in the brain image into regions based on the selected standard brain image.;0
803;2019019500;2018;;"KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;Yonsei University Industry Foundation (Yonsei UIF)";KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G10L25/03,G10L15/063,G10L15/32,G10L25/51,G10L13/04,G10L15/02,G10L13/00;APPARATUS FOR DEEP LEARNING BASED TEXT-TO-SPEECH SYNTHESIZING BY USING MULTI-SPEAKER DATA AND METHOD FOR THE SAME;"Disclosed is a method and apparatus for training a speech signal. A speech signal training apparatus of the present disclosure may include a target speaker speech database storing a target speaker speech signal; a multi-speaker speech database storing a multi-speaker speech signal; a target speaker acoustic parameter extracting unit extracting an acoustic parameter of a training subject speech signal from the target speaker speech signal; a similar speaker acoustic parameter determining unit extracting at least one similar speaker speech signal from the multi-speaker speech signals, and determining an auxiliary speech feature of the similar speaker speech signal; and an acoustic parameter model training unit determining an acoustic parameter model by performing model training for a relation between the acoustic parameter and text by using the acoustic parameter and the auxiliary speech feature, and setting mapping information of the relation between the acoustic parameter model and the text.";1
804;2019037261;2017;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;H04N21/252,H04N21/251,H04N21/231,H04N21/258,H04N21/25883,H04N21/25891,H04N21/2668;ELECTRONIC DEVICE, CONTROL METHOD THEREFOR, AND COMPUTER-READABLE RECORDING MEDIUM;"An electronic device comprises: a communicator for receiving population distribution data for at least one item among gender, age, and income from an external server and receiving user information composed of a plurality of items from each of a plurality of other electronic devices; and a processor for predicting unknown information by using a predictive model in which the unknown information is predicted by using a pre-stored variable parameter, for user information having the unknown information on a preset item in the received user information, generating population distribution data for the preset item by using the received user information and the predicted unknown information, comparing the generated population distribution data with the received population distribution data so as to calculate an error of the generated population distribution data, and changing the parameter of the predictive model on the basis of the calculated error so as to modify the predictive model.";1
805;2019042574;2018;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G06N3/086,G06F3/04845,G06F18/217,G06N3/08,G06F16/58,G06F16/532,G06F3/0482,G06F16/48,G06V10/17,G06F16/434,G06V10/7788,G06F3/04883,G06Q30/0601,G06F3/04842,G06F3/0488,G06V20/10,G06N3/045,G06V10/776,G06F18/41,G06N3/044;ELECTRONIC DEVICE AND METHOD FOR CONTROLLING THE ELECTRONIC DEVICE;An artificial intelligence (AI) system utilizing a machine learning algorithm to receive an area in an image provide a first search result by using first text information describing an object in the area by using a trained model, and provide a second search result by using second text information describing an object in the second area using the trained model.;4
806;2019043076;2018;;HUAWEI TECHNOLOGIES CO., LTD.;HUAWEI TECHNOLOGIES CO., LTD.;G06N20/00,G06Q30/02,G06Q30/0243;METHOD AND APPARATUS FOR ESTIMATING ADVERTISEMENT VALUE, AND DISPLAYING ADVERTISEMENTS ON USER TERMINAL ACCORDING TO THEIR VALUES;An advertisement management server in a communication system receives information of advertisements, determines an advertising value calculation policy, estimates a value of an advertising value element according to the information of each advertisement, calculates a value of each advertisement using the value of the advertising value element and the advertising value calculation policy as reference factors. The server instructs the communication system to broadcast the advertisements for displaying on the user terminals according to the calculated value of each advertisement.;0
807;2019043482;2018;;Baidu Online Network Technology (Beijing) Co. Ltd.;Baidu Online Network Technology (Beijing) Co. Ltd.;G10L15/02,G10L15/063,G10L21/0208,G10L15/16,G10L15/20,G06N3/08;FAR FIELD SPEECH ACOUSTIC MODEL TRAINING METHOD AND SYSTEM;"The present disclosure provides a far field speech acoustic model training method and system. The method comprises: blending near field speech training data with far field speech training data to generate blended speech training data, wherein the far field speech training data is obtained by performing data augmentation processing for the near field speech training data; using the blended speech training data to train a deep neural network to generate a far field recognition acoustic model. The present disclosure can avoid the problem of spending a lot of time costs and economic costs in recording the far field speech data in the prior art; and reduce time and economic costs of obtaining the far field speech data, and improve the far field speech recognition effect.";1
808;2019057081;2017;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G06N3/04,G06N3/044,G06N3/045,G06N3/084,G06F40/56,G06F40/30,G06N3/08,G06Q30/0251,G10L25/30;METHOD AND APPARATUS FOR GENERATING NATURAL LANGUAGE;A natural language generation method and apparatus are provided. The natural language generation apparatus converts an input sentence to a first vector using a first neural network model-based encoder, determines whether a control word is to be provided based on a criterion, and converts the first vector to an output sentence using a neural network model-based decoder, based on whether the control word is to be provided.;3
809;2019065449;2017;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G06F3/0484,G06F40/279,G06F40/111,G06F40/177,G06F40/56,G06F40/166,G10L13/08,G06F3/167,G06F40/143,G10L13/02;APPARATUS AND METHOD OF GENERATING ALTERNATIVE TEXT;Provided is an alternative text generating method. The alternative text generating method includes recognizing input visual content, generating input information corresponding to a recognition result of the recognition of the visual content, generating an editing window including an input item to which the input information is automatically input, automatically generating an alternative text, based on an alternative text generation rule and the input information, and displaying the generated alternative text on a text box of the editing window.;4
810;2019065498;2018;;CHIRRP, INC.;CHIRRP, INC.;G06N5/027,G06F16/248,G06N5/04,G06F16/90332,G06F16/24522;SYSTEM AND METHOD FOR RICH CONVERSATION IN ARTIFICIAL INTELLIGENCE;A method and system can include for âRich Converstationâ can include receiving a search query, identifying an intent of the search query, parsing the search query to identify one or more of an entity identifier and a scope identifier where an entity identifier is a subject of the search query and the scope identifier is a scope definition associated with the search query, identifying an answer to the search query based upon a user profile and the scope definition, generating a conversation-based interaction using the scope definition, and modifying the scope definition using the conversation-based interaction and user profile. The method and system can further modify a scope definition for a future conversation-based interaction based upon a prior conversation-based interaction and the user profile and present the answer to the search query and a second answer based on the future conversation-based interaction.;1
811;2019066658;2018;;HITACHI, LTD.;HITACHI, LTD.;G10L25/51,G10L15/16,G10L15/063,G06F16/632,G10L21/003,G10L25/24,G10L15/01;METHOD FOR LEARNING CONVERSION MODEL AND APPARATUS FOR LEARNING CONVERSION MODEL;"In information conversion, a subjective similarity with target information is increased. A method for learning a conversion model is disclosed and includes performing a conversion process of converting conversion source information to post conversion information using the conversion model; performing a first comparison process of comparing the post conversion information with target information to calculate a first distance; performing a similarity score estimation process of using an evaluation model to calculate a similarity score with the target information from the post conversion information; performing a second comparison process of calculating a second distance from the similarity score; and performing a conversion model learning process of learning the conversion model using the first distance and the second distance as evaluation indices.";1
812;2019066659;2018;;Fujitsu Limited;Fujitsu Limited;G10L25/63,G10L15/063,G10L15/22,G10L2015/225,G06F40/35,G06Q30/0257,G06F16/3329,G10L2015/0638,G10L2015/223;DIALOGUE CONTROL SYSTEM AND DIALOGUE CONTROL METHOD;A dialogue control system includes a memory, and a processor coupled to the memory and the processor configured to select, from a supplementary information candidate, supplementary information related to at least one of a past context of an interaction including an input from a user and a predetermined output of the dialogue control system with respect to the input from the user, an input of the user expected in future in association with the output of the dialogue control system, and an output of the dialogue control system scheduled in the future, and insert the selected supplementary information into the interaction and present the inserted supplementary information to the user.;1
813;2019066676;2017;;SONY GROUP CORPORATION;SONY GROUP CORPORATION;G10L15/26,G06F3/01,G10L15/1815,G06F3/167,G06F3/16,G10L15/10,G10L15/22,G10L15/187,G10L2015/223,G10L15/30,G06F40/295,G10L13/08,G10L17/00,G10L13/00;INFORMATION PROCESSING APPARATUS;"There is provided an information processing apparatus to perform a more natural response corresponding to the speech of the user, the information processing apparatus including: an acquiring unit configured to collect speech information; a transmitting unit configured to transmit recognition information based on the speech information; a receiving unit configured to receive processing content normalized by an analyzing unit on a basis of the recognition information; and an output unit configured to output response information corresponding to a processing result based on the processing content and a speech characteristic of a user. Also provided is an information processing apparatus, including: a receiving unit configured to receive recognition information based on collected speech information; an analyzing unit configured to normalize processing content on a basis of the recognition information; and a transmitting unit configured to transmit the processing content.";1
814;2019080425;2018;;TrendGrab, Inc;TrendGrab, Inc;G06Q50/16,G06Q50/01,G06Q10/0631,G06F16/9032,G06N5/022,G06Q30/0617;Method and Systems for Providing On-demand Real Estate Related Products and Services;"A computing device includes one or more memory devices, one or more processors and computer-readable instructions executable by the one or more processors, the computer-readable instructions to cause the computing device to 1) receive real estate product selection from real estate customer; 2) receive real estate meeting request time value and real estate meeting location from the real estate customer; 3) generate query including the real estate product selection, the real estate meeting request time value and the real estate meeting location; 4) communicate the query including the real estate product selection, the real estate meeting request time value and the real estate meeting location to a database server; 5) receive, in real time from the database server, one or more recommended real estate agents based at least in part on the real estate product selection, the real estate meeting request time and the real estate meeting location.";0
815;2019082065;2018;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;H04L51/046,G06F3/1275,H04L51/222,G06F3/1254,H04L51/02,G06F3/1204,H04N1/00411,H04N1/00493,G06F3/048,G06F3/1292,G06F3/1268,H04N1/00506;INFORMATION PROCESSING DEVICE AND NON-TRANSITORY COMPUTER READABLE MEDIUM;An information processing device includes a controller that controls, depending on a position of a user, an output of an instruction related to at least one device to a conversation partner who is responding, in an interface for the user to make a conversation with the conversation partner. In a further modification of the invention, depending on a positional relationship between the user and the at least one device, the controller may control the output of the instruction related to the at least one device to the conversation partner who is responding.;0
816;2019082255;2018;;OLYMPUS CORPORATION;OLYMPUS CORPORATION;H04R1/406,H04R1/083,G10L25/18,H04R3/005,H04R2420/09,H03G3/3005,G10L25/90,G10L25/48,H04R2499/11,H04R5/027,H04R2430/20,H04R29/005,H04S2400/15;INFORMATION ACQUIRING APPARATUS, INFORMATION ACQUIRING METHOD, AND COMPUTER READABLE RECORDING MEDIUM;"A disclosed information acquiring apparatus includes a display that displays an image thereon; a plurality of microphones provided at different positions to collect a sound produced by each of audio sources and generate audio data; an audio-source position estimating circuit that estimates a position of each of the audio sources based on the audio data generated by each of the microphones; and a display control circuit that causes the display to display audio-source positional information about a position of each of the audio sources in accordance with an estimation result estimated by the audio-source position estimating circuit.";0
817;2019087677;2017;;Ramot at Tel-Aviv University Ltd.;Ramot at Tel-Aviv University Ltd.;G06V30/10,G06V30/226,G06F18/24133,G06V2201/01,G06V30/19173,G06V30/18057,G06F16/5846,G06V30/153,G06F18/214,G06N3/045,G06V10/82,G06V30/18171;METHOD AND SYSTEM FOR CONVERTING AN IMAGE TO TEXT;In a method of converting an input image patch to a text output, a convolutional neural network (CNN) is applied to the input image patch to estimate an n-gram frequency profile of the input image patch. A computer-readable database containing a lexicon of textual entries and associated n-gram frequency profiles is accessed and searched for an entry matching the estimated frequency profile. A text output is generated responsively to the matched entries.;1
818;2019095430;2017;;Google LLC;Google LLC;G10L13/00,G10L15/005,G06F40/51,H04R1/406,G10L15/22,G10L21/003,G10L2021/02166,G06F40/58,H04R3/005;SPEECH TRANSLATION DEVICE AND ASSOCIATED METHOD;A computer-implemented method and associated computing device for translating speech can include receiving, at a microphone of a computing device, an audio signal representing speech of a user in a first language or in a second language at a first time. A positional relationship between the user and the computing device at the first time can be determined and utilized to determine whether the speech is in the first language or the second language. The method can further include obtaining, at the computing device, a machine translation of the speech represented by the audio signal based on the determined language, wherein the machine translation is: (i) in the second language when the determined language is the first language, or (ii) in the first language when the determined language is the second language. An audio representation of the machine translation can be output from a speaker of the computing device.;1
820;2019114593;2018;;ExpertHiring, LLC;ExpertHiring, LLC;G06F16/3344,G06F16/3334,G06F16/3329,G06Q10/1053,G06F16/3326,G06F16/24578,G06F16/3325;METHOD AND SYSTEM FOR MANAGING, MATCHING, AND SOURCING EMPLOYMENT  CANDIDATES IN A RECRUITMENT CAMPAIGN;A method and system for automating some aspects of a recruiting process, which may implement rules permitting the processes of sourcing candidates, setting up job interviews, and responding to candidate questions to all be automated with a computer. Such a system may match rÃ©sumÃ©s and job descriptions with a scoring system, and may initiate communications between one or more candidates and a recruiter once an appropriate number of matches have been found. The system may then be configured to field responses to commonly asked questions from a question database, and notify the recruiter if a question is asked that it cannot answer, and may further be configured to proactively ask questions to the candidate if desired. This may allow recruiters to focus on the highest level of vetting, and on aspects of the recruitment process such as promoting the hiring company and salary negotiations.;0
821;2019117088;2018;;TERUMO KABUSHIKI KAISHA;TERUMO KABUSHIKI KAISHA;A61B5/0082,G06T7/0016,G06T2207/30101,A61B6/5217,G06V10/454,G06T2207/30096,G06T2207/20076,A61B5/05,G06T2207/10081,G16H50/20,A61M2025/0681,A61B8/0891,G06T7/62,A61B6/037,G06T2207/30021,A61B8/5261,A61B8/5223,A61B5/7267,A61M25/104,A61B5/02007,G06V10/776,G06T7/0012,A61B5/7221,G06F18/217,G16H20/40,G06T2207/20072,A61B5/4836,A61B2576/02,G06V10/82,G06T2207/20084,A61B6/5247,G06T2207/20081,A61B6/481,A61B8/085,A61B6/504;Diagnostic Method, Method for Validation of Diagnostic Method, and Treatment Method;A method is disclosed for diagnosing, validating and treating a patient having lesions in both arteries of left and right lower limbs. By determining that a shorter lesion to be treated first, catheters and an operation time can be reduced is to be treated first on a priority basis based on diagnostic data, deciding that a longer lesion is to be treated next, then treating the lesions substantially continuously.;0
823;2019130896;2017;;salesforce.com, inc.;salesforce.com, inc.;G06N3/045,G10L2015/0631,G10L15/20,G10L13/00,G10L13/0335,G10L15/063,G06N3/084,G10L25/30,G06N3/082,G10L15/24,G10L15/16,G06N3/044,G06N3/02;Regularization Techniques for End-To-End Speech Recognition;The disclosed technology teaches regularizing a deep end-to-end speech recognition model to reduce overfitting and improve generalization: synthesizing sample speech variations on original speech samples labelled with text transcriptions, and modifying a particular original speech sample to independently vary tempo and pitch of the original speech sample while retaining the labelled text transcription of the original speech sample, thereby producing multiple sample speech variations having multiple degrees of variation from the original speech sample. The disclosed technology includes training a deep end-to-end speech recognition model, on thousands to millions of original speech samples and the sample speech variations on the original speech samples, that outputs recognized text transcriptions corresponding to speech detected in the original speech samples and the sample speech variations. Additional sample speech variations include augmented volume, temporal alignment offsets and the addition of pseudo-random noise to the particular original speech sample.;1
824;2019130900;2017;;INSTITUTE FOR INFORMATION INDUSTRY;INSTITUTE FOR INFORMATION INDUSTRY;G06F40/30,G10L2015/088,G10L2015/223,G10L15/22,G10L15/1815,G06N5/04,G10L25/63,G06N20/00,G06F3/167,G10L13/00;VOICE INTERACTIVE DEVICE AND VOICE INTERACTIVE METHOD USING THE SAME;A voice interactive device includes a semantic analyzing module, a tone analyzing module, a speaker classification determining module and a dialogue sentence database. The semantic analyzing module is configured to analyze a semantic meaning of speaking sentence from a speaker. The tone analyzing module is configured to analyze a tone of the speaking sentence. The speaker classification determining module is configured to determine that the speaker belongs to one of a plurality of speaker classification types according to the semantic meaning and the tone. The dialogue sentence database stores a plurality of relationships between speaker classifications and response sentences. The dialogue sentence generating module is configured to generate a response sentence corresponding to the speaker according to the relationships between speaker classifications and response sentences. The voice generator is configured to output a response voice of the response sentence.;1
827;2019149490;2018;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;G10L15/22,G10L13/00,G10L2015/223,H04L51/04,H04L51/02,H04L51/224;INFORMATION PROCESSING APPARATUS AND NON-TRANSITORY COMPUTER READABLE MEDIUM;An information processing apparatus includes a changing unit that changes a determination criterion for a response from a response assistant that responds to an inquiry of a user. In a further modification of the invention, the determination criterion may include a plurality of setting items, and the changing unit may change the contents of the setting items each individually.;1
829;2019164632;2018;;SYNTEKABIO, INC.;SYNTEKABIO, INC.;G06N3/08,G16B40/00,G16H20/10,G16C20/30,G16B20/00,G06N3/045,G16B15/30,G16C20/70,G06N3/04,G16B40/30,G16C60/00,G16C20/20;DRUG INDICATION AND RESPONSE PREDICTION SYSTEMS AND METHOD USING AI DEEP LEARNING BASED ON CONVERGENCE OF DIFFERENT CATEGORY DATA;"A system of predicting drug indications and drug response using an artificial intelligence (AI) deep learning model based on convergence of different types of information, the system including: a learning module configured to learn the response correlation between structure information on a drug and genetic information on a genome from collected learning information by deep machine learning; a prediction module configured to receive analysis information and output the result of prediction of the response of the genome to the drug from the analysis information; and a storage module configured to store a response prediction algorithm learned by the learning module. The learning information is drug response information obtained from clinical drug response information on target proteins, cell lines or living bodies.";1
831;2019205388;2018;;Fujitsu Limited;Fujitsu Limited;G06F40/279,G06F40/30;GENERATION METHOD, INFORMATION PROCESSING APPARATUS, AND STORAGE MEDIUM;"A non-transitory computer-readable storage medium storing a program that causes a computer to execute a process, the process includes obtaining document data; when a plurality of documents are included in the obtained document data, based on an occurrence frequency of each word included in one of the plurality of documents among the plurality of documents and an occurrence frequency of the each word in another document included in the plurality of documents, identifying a word from the each word; and generating a question sentence regarding the identified word.";2
832;2019214013;2018;;CA, Inc.;CA, Inc.;G06F3/167,G10L2015/228,G10L15/30,G10L15/26,G06F40/174;SPEECH-TO-TEXT CONVERSION BASED ON USER INTERFACE STATE AWARENESS;A web server system identifies a webpage being accessed by a user through a client terminal. The webpage is among a set of possible webpages that are accessible through the client terminal. Different identifiers are assigned to different ones of the webpages. Responsive to the identifier of the webpage, a set of user interface (UI) input field constraints is selected that define what the webpage allows to be input by a user to a set of UI fields provided by the webpage. An output text string is obtained that is converted from a sampled audio steam by a speech-to-text conversion server and that is constrained to satisfy one of the UI field input constraints of the selected set. The output text string is provided to an application programming interface of the webpage that corresponds to one of the UI fields having user input constrained by the UI field input constraint of the selected set.;1
835;2019272887;2019;;The Board of Trustees of the Leland Stanford Junior University;The Board of Trustees of the Leland Stanford Junior University;G06N20/20,G06F30/20,G16B40/30,G16B5/00,G16B15/30,G16C20/64,G06N5/01,G06F2111/08,G16C20/70,G16B40/20;Machine Learning and Molecular Simulation Based Methods for Enhancing Binding and Activity Prediction;Systems and methods for molecular simulation in accordance with embodiments of the invention are illustrated. One embodiment includes a method for predicting a relationship between a ligand and a receptor. The method includes steps for identifying a plurality of conformations of a receptor, computing docking scores for each of the plurality of conformations and a set of one or more ligands, and predicting a relationship between the set of one or more ligands and the plurality of conformations of the receptor.;1
836;2019279644;2017;;NEC CORPORATION;NEC CORPORATION;G10L17/02,G10L17/12,G10L15/02,G10L15/10,G10L17/00;SPEECH PROCESSING DEVICE, SPEECH PROCESSING METHOD, AND RECORDING MEDIUM;"A speech processing device includes at least one memory configured to store instructions and at least one processor configured to execute the instructions to: store one or more acoustic models; calculate an acoustic feature from a received speech signal, and by using the acoustic feature calculated and the acoustic model stored, calculate an acoustic diversity that is a vector representing a degree of variations of types of sounds; by using the calculated acoustic diversity and a selection coefficient, calculate a weighted acoustic diversity, and by using the weighted acoustic diversity calculated and the acoustic feature, calculate a recognition feature for recognizing identity of a speaker that concerns the speech signal; and calculate a feature vector by using the recognition feature calculated.";1
839;2019294669;2018;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F40/30,G06F40/295,G06F40/216,G06F40/56,G06F16/90335,G06N20/00,G06N7/01;Smart Narrative Consumption Platform;Described herein is a system and method for providing a summary of a portion of a narrative source document. Vectors for each of a plurality of attributes (e.g., characters, events, time, a sequence of events in chronology, characters and events in chronology, or events and characters in chronology, etc.) can be built based at least on a received portion of a narrative source document. A query can be received and an intent of the received query can be determined (e.g., using natural language processing). Summaries can be generated and probabilities calculated for each of the generated summaries using an algorithm applying a model, the vectors and the determined intent of the query. One or more of the generated summaries can be provided in response to the received query in accordance with the calculated probabilities.;3
842;2019311716;2017;;SHARP KABUSHIKI KAISHA;SHARP KABUSHIKI KAISHA;G06F40/268,G10L15/22,G10L2015/223,G06F40/274,G10L15/30,G10L13/00;DIALOG DEVICE, CONTROL METHOD OF DIALOG DEVICE, AND A NON-TRANSITORY STORAGE MEDIUM;A completion processing section (23) is configured to, if a user's speech inputted to an interactive device (1) omits some phrase, complete the speech of the user. A speech storing section (25) stores a user's speech having no omitted or incorrect phrases in a speech database (50) for use in generation of a speech of the interactive device (1). User's previous speech data thus stored are made effective use of for generation of a speech of the interactive device.;2
843;2019318433;2019;;NOBUL CORPORATION;NOBUL CORPORATION;G06Q30/0641,H04L9/50,G06Q50/16,H04L9/3239,G06Q30/0639,H04L9/0643,H04L9/0637;REAL ESTATE MARKETPLACE METHOD AND SYSTEM;The present invention is a real estate marketplace incorporating computerized systems and methods. The marketplace may incorporate a platform and/or application (app) operable to provide a user with one or more functions for a real estate process. The marketplace may be integrated with blockchain whereby secured blockchain-based ledger structures are generated to facilitate a secured and traceable real estate process. Users of the marketplace may include buyers, sellers, agents and/or other service providers. Each type of user will experience a different flow of options and steps in accordance with the activities of said user in a real estate process. Agents and other service providers compete to provide services to buyers and seller users in a real estate process. The choice of agent and other service providers, the services to be provided, and the fees for such services relating to the real estate process are all driven by the buyer and seller users.;0
844;2019340648;2018;;VIOOH LIMITED;VIOOH LIMITED;G06Q30/0242,G06Q30/0261,G06Q30/0264;Method And System For Displaying Contents;A method for displaying contents from advertisement campaigns on displays belonging to an OOH inventory, by an artificial intelligence module which is run by allocation server and which is trained to optimally allocate displays and timing to advertisement campaigns.;0
847;2019358906;2019;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06T17/10,B29C64/40,B29C64/386,B33Y10/00,G06T11/203,B33Y30/00,B33Y50/02;FABRICATING THREE-DIMENSIONAL OBJECTS WITH OVERHANG;The claimed subject matter includes a system and method to design 3D objects for fabrication. In embodiments, the method includes sampling coordinates of a two-dimensional object. The method also includes generating fabrication coordinates based on the coordinates and a plane comprising a top layer of a three-dimensional (3D) object. Additionally, the method includes generating a 2D triangular mesh for the top layer of an overhang based on the sampled coordinates, an angle between the top layer and two points in a previous top layer border less than or equal to an overhang threshold angle.;0
848;2019362709;2018;;Motorola Mobility LLC;Motorola Mobility LLC;G10L15/22,G10L2015/025,G10L2015/223,G10L15/063,G10L15/02;Offline Voice Enrollment;A device receives voice inputs from a user and can perform various different tasks based on those inputs. The device is trained based on the user's voice by having the user speak a desired command. The device receives the voice input from the user and applies various different voice training parameters to generate a voice model for the user. The training parameters used by the device can change over time, so the voice input used to train the device based on the user's voice is stored by the device in a protected (e.g., encrypted) manner. When the training parameters change, the device receives the revised training parameters and applies these revised training parameters to the protected stored copy of the voice input to generate a revised voice model for the user.;1
849;2019369924;2019;;Kyocera Document Solutions Inc.;Kyocera Document Solutions Inc.;G06F3/1253,G06F3/1205,G06F3/1268,G06F3/1288,G06F3/122,G06F3/1206,G06F3/1228,H04L51/02,G06F3/1204,H04L51/046;Printing Assistant System and Chatbot Device;A printing assistant system includes a server device and a chatbot device. The chatbot device: receives the answer message from the server device, creates a different question message that inquires of the user of the terminal device about a parameter to be set to a different settable item corresponding to contents of the answer message, and posts the created question message to the server device, repeats posting of a plurality of different question messages until receiving a plurality of answer messages identifying parameters to be set to all the plurality of settable items from the server device, and sets parameters to all the plurality of settable items based on the plurality of answer messages received from the server device. The chatbot device creates a print job that prints the file in accordance with the parameters set to all the plurality of settable items.;0
850;2019370616;2019;;Quantum-Si Incorporated;Quantum-Si Incorporated;G06F18/22,G06F18/256,G06F18/21355,G06F18/214;METHODS AND APPARATUS FOR MULTI-MODAL PREDICTION USING A TRAINED STATISTICAL MODEL;Methods and apparatus for predicting an association between input data in a first modality and data in a second modality using a statistical model trained to represent interactions between data having a plurality of modalities including the first modality and the second modality, the statistical model comprising a plurality of encoders and decoders, each of which is trained to process data for one of the plurality of modalities, and a joint-modality representation coupling the plurality of encoders and decoders. The method comprises selecting, based on the first modality and the second modality, an encoder/decoder pair or a pair of encoders, from among the plurality of encoders and decoders, and processing the input data with the joint-modality representation and the selected encoder/decoder pair or pair of encoders to predict the association between the input data and the data in the second modality.;1
851;2019371295;2019;;BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT CO., LTD.;BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT CO., LTD.;G10L15/22,G10L2015/228,G10L15/26,G10L15/063,G10L17/00,G10L15/02;SYSTEMS AND METHODS FOR SPEECH INFORMATION PROCESSING;System and methods for generating user behaviors using a speech recognition method are provided. The method may include obtaining an audio file including speech data associated with one or more speakers and separating the audio file into one or more audio sub-files that each includes a plurality of speech segments. Each of the one or more audio sub-files may correspond to one of the one or more speakers. The method may further include obtaining time information and speaker identification information corresponding to each of the plurality of speech segments and converting the plurality of speech segments to a plurality of text segments. Each of the plurality of speech segments may correspond to one of the plurality of text segments. The method may further include generating first feature information based on the plurality of text segments, the time information, and the speaker identification information.;1
852;2019371476;2019;;Quantum-Si Incorporated;Quantum-Si Incorporated;Y02A90/10,G16H50/20,G16H50/50,G16H70/40;METHODS AND APPARATUS FOR MAKING BIOLOGICAL PREDICTIONS USING A TRAINED MULTI-MODAL STATISTICAL MODEL;Methods and apparatus for predicting an association between input data in a first modality and data in a second modality using a statistical model trained to represent interactions between data having a plurality of modalities including the first modality and the second modality, the statistical model comprising a plurality of encoders and decoders, each of which is trained to process data for one of the plurality of modalities, and a joint-modality representation coupling the plurality of encoders and decoders. The method comprises selecting, based on the first modality and the second modality, an encoder/decoder pair or a pair of encoders, from among the plurality of encoders and decoders, and processing the input data with the joint-modality representation and the selected encoder/decoder pair or pair of encoders to predict the association between the input data and the data in the second modality.;1
854;2019385590;2019;;Yahoo Japan Corporation;Yahoo Japan Corporation;G10L2015/025,H04R1/406,G10L15/02,G10L15/28,G10L15/063,G10L15/20,H04R3/00,G10L15/22;GENERATING DEVICE, GENERATING METHOD, AND NON-TRANSITORY COMPUTER READABLE STORAGE MEDIUM;A generating device according to the present application includes an obtaining unit and a first generating unit. The obtaining unit obtains training data including an acoustic feature value of a first observation signal, a late reverberation component corresponding to the first observation signal, and a phoneme label associated with the first observation signal. The first generating unit generates an acoustic model to identify a phoneme label corresponding to a second observation signal based on the training data obtained by the obtaining unit.;1
855;2019385592;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G06N3/045,G10L15/32,G10L13/00,G06N3/08,G10L15/16,G10L15/063;SPEECH RECOGNITION DEVICE AND SPEECH RECOGNITION METHOD;A speech recognition method includes learning a first learning model to obtain first speech data corresponding to first training data, learning a second learning model to obtain a first speech recognition result corresponding to second training data, and controlling to change a parameter of the first learning model based on an error of the obtained first speech recognition result. The second training data may be first speech data.;1
856;2019385628;2018;;The University of Electro-Communications;The University of Electro-Communications;G10L21/013,G06N3/044,G06N3/047,G06N20/00,G06N3/088,G10L25/03,G10L17/04,G06F3/167,G10L21/007,G10L2021/0135;VOICE CONVERSION / VOICE IDENTITY CONVERSION DEVICE, VOICE CONVERSION / VOICE IDENTITY CONVERSION METHOD AND PROGRAM;This voice conversion/voice identity conversion device is provided with a parameter learning unit, a parameter storage unit and a voice conversion/voice identity conversion processing unit. The parameter learning unit prepares a probability model by means of a restricted Boltzmann machine assuming that there is a connection weight between a visible element representing input data and a hidden element representing potential information. The parameter learning unit defines, as a probability model, a plurality of speaker clusters having specific adaptive matrices, and determines parameters for each speaker by estimating weights for the plurality of speaker clusters. The parameter storage unit stores the parameters. A voice conversion/voice identity conversion processing unit performs voice conversion/voice identity conversion processing of acoustic information based on the voice of a source speaker based on the parameters stored in the parameter storage unit and speaker information of a target speaker.;1
857;2019392291;2019;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G06N3/047,G06N3/049,G06N3/08,G06N3/045,G06N3/065,G06N3/048;ELECTRONIC CIRCUIT FOR IMPLEMENTING GENERATIVE ADVERSARIAL NETWORK USING SPIKE NEURAL NETWORK;Provided is an electronic circuit for implementing a generative adversarial neural network. The electronic circuit includes a spike converter, a spike image generator, a spike image converter, and an image discriminator. The spike converter generates a first signal including spike signals. The number of the spike signals is determined based on first data associated with second data within a reference time interval. The spike image generator generates a second signal including spike signals being selected based on a weight among the spike signals of the first signal. The image converter converts the spike signals of the second signal to generate third data being represented in an analog domain. The image discriminator provides the spike image generator with result data being associated with a difference between a value of the third data and a value of the second data. The image generator determines the weight based on the result data.;1
858;2019392858;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L13/033,G10L25/78,G10L17/26,G06V10/82,G06N3/08,G10L13/08,G06F3/167,G06F3/165,G06N3/044,G06V20/10,G06V10/764,G06N3/047,G06N3/045;INTELLIGENT VOICE OUTPUTTING METHOD, APPARATUS, AND INTELLIGENT COMPUTING DEVICE;Provided are an intelligent voice outputting method and apparatus and an intelligent computing device. The intelligent voice outputting method includes obtaining a voice from a microphone detection signal, capturing an image in a direction in which the microphone detection signal is received, obtaining a distance to a speaker of the voice on the basis of the microphone detection signal and the image, and outputting a response regarding the voice on the basis of the distance to the speaker, whereby effectively transferring a response regarding the voice of the speaker only by the voice outputting apparatus without the help of an external device. At least one of the voice outputting apparatus, the intelligent computing device, and a server may be associated with an artificial intelligence (AI) module, an unmanned aerial vehicle (UAV) (or drone), a robot, an augmented reality (AR) device, a virtual reality (VR) device, and a device related to a 5G service.;1
860;2020004790;2019;;UBERPLE CO., LTD.;UBERPLE CO., LTD.;G06F16/313,G06F16/93;METHOD AND SYSTEM FOR EXTRACTING SENTENCES;"Methods and apparatus for extracting sentences are provided, one of methods comprises, receiving a document summarization request, which does not comprise a keyword for extracting important sentences from a document, from a user terminal, selecting a keyword, which reflects the preference of a user of the user terminal, based on a profile of the user, extracting important sentences from the document based on the selected keyword; and generating a summary of the document based on the extracted important sentences.";2
861;2020013395;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L15/20,G10L15/063,G10L21/0232,G10L15/22,G10L21/0208,G10L2015/0635,G10L25/84,G10L15/00;INTELLIGENT VOICE RECOGNIZING METHOD, APPARATUS, AND INTELLIGENT COMPUTING DEVICE;Disclosed are an intelligent voice recognizing method, a voice recognizing device, and an intelligent computing device. According to an embodiment of the present invention, a method of intelligently recognizing a voice by a voice recognizing device obtains a microphone detection signal via at least one microphone, removes noise from the microphone detection signal based on a noise removal model, recognizes a voice from the noise-removed microphone detection signal, and updates the noise removal model based on the type of the noise detected from the microphone detection signal, thereby preventing deterioration of speech recognition performance. According to the present invention, one or more of the voice recognizing device, intelligent computing device, and server may be related to artificial intelligence (AI) modules, unmanned aerial vehicles (UAVs), robots, augmented reality (AR) devices, virtual reality (VR) devices, and 5G service-related devices.;1
863;2020019370;2018;;Disney Enterprises, Inc.;Disney Enterprises, Inc.;G10L15/1815,G06N3/047,G06N3/045,G06N3/006,G06F40/284,G06F3/16,G10L13/00,G10L15/26,G10L15/22,G10L15/1822,G06F40/205,G06N5/01,G06F40/211,G06F3/167,G06N3/044,G06F16/24578;COLLABORATIVE AI STORYTELLING;"Implementations of the disclosure describe AI systems that offer an improvisational story telling AI agent that may interact collaboratively with a user. In one implementation, a story telling device may be implemented using i) a natural language understanding (NLU) component to process human language input (e.g., digitized speech or text input); ii) a natural language processing (NLP) component to parse the human language input into a story segment or sequence; iii) a component for storing/recording the story as it is created by collaboration; iv) a component for generating AI-suggested story elements; and v) a natural language generation (NLG) component to transform the AI-generated story segment into natural language that may be presented to the user.";3
864;2020020337;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G06V40/174,G10L2015/223,G10L15/08,G10L15/22,G10L2015/088,G06F3/013,G06F3/017;INTELLIGENT VOICE RECOGNIZING METHOD, APPARATUS, AND INTELLIGENT COMPUTING DEVICE;An intelligent voice recognizing method and apparatus are disclosed. The voice recognizing apparatus obtains a first microphone detection signal in wake-up recognition mode, switches to continuous word recognition mode upon recognizing a wake-up word from the first microphone detection signal, obtains a second microphone detection signal, performs a function corresponding to a continuous word upon recognizing the continuous word from the second microphone detection signal, and switches to the continuous word recognition mode upon detecting a preset first gesture. This allows a user to activate the voice recognizing apparatus by saying as few wake-up words as possible, thereby providing convenience to the user. One or more between a voice recognizing apparatus and intelligent computing device according to the present disclosure may be associated with an artificial intelligence module, an unmanned aerial vehicle (UAV), an augmented reality (AR) device, a virtual reality (VR) device, a 5G service-related device, etc.;1
865;2020026766;2016;;SYSTRAN INTERNATIONAL CO., LTD.;SYSTRAN INTERNATIONAL CO., LTD.;G06F40/56,G06F40/289,G06V30/153,G06F40/44,G06F40/58,G06V30/10,G06V30/158;METHOD FOR TRANSLATING CHARACTERS AND APPARATUS THEREFOR;"A character translation method performed by a character translation apparatus according to one embodiment of the present invention may comprise the steps of: obtaining image contents; recognizing characters of a first language on the image contents and a sentence determination symbol of the first language; extracting a sentence of the first language composed of the recognized characters, on the basis of the recognized sentence determination symbol; producing, on the basis of the extracted sentence of the first language, a sentence to be translated using user event information; and translating the generated sentence to be translated into a second language and displaying the sentence translated into the second language.";1
866;2020026767;2018;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;G06N3/08,G06N3/044,G06N3/045,G06F16/338,G06F16/93,G06F16/345,G06N3/084,G06F16/334,G06N3/048;SYSTEM AND METHOD FOR GENERATING TITLES FOR SUMMARIZING CONVERSATIONAL DOCUMENTS;"A method and system of generating titles for documents in a storage platform are provided. The method includes receiving a plurality of documents, each document having associated content features, applying a title generation computer model to each of the plurality of documents to generate a title based on the associated content features, appending the generated title to each of the plurality of documents, wherein the title generation computer model is created by training a neural network using a combination of a first set of unlabeled data from a first domain related to content features of the plurality of documents; and a second set of pre-labeled data from a second domain different from the first domain.";2
867;2020027439;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G06N3/08,G10L13/047,H04N23/57,G10L13/00,G06V30/40,G06N3/04,G06V30/127,G06V30/10,G06V30/1463;INTELLIGENT TEXT TO SPEECH PROVIDING METHOD AND INTELLIGENT COMPUTING DEVICE FOR PROVIDING TTS;An intelligent TTS providing method and an intelligent computing device providing TTS are disclosed. An intelligent TTS providing method according to an embodiment of the present disclosure can seamlessly provide continuous TTS by receiving a text read command, adjusting a photographing angle of a camera such that a position of an object on which text is written is included in the photographing angle, photographing the object, converting the text written on the object into a speech and outputting the speech. One or more of the intelligent computing device and artificial intelligent speaker of the present disclosure can be associated with artificial intelligence (AI) modules, unmanned aerial vehicle (UAV) robots, augmented reality (AR) devices, virtual reality (VR) devices, 5G service related devices, etc.;1
869;2020035241;2019;;Baidu Online Network Technology (Beijing) Co. Ltd.;Baidu Online Network Technology (Beijing) Co. Ltd.;G10L17/00,G10L15/26,G10L17/02,G10L13/033,G10L15/22;METHOD, DEVICE AND COMPUTER STORAGE MEDIUM FOR SPEECH INTERACTION;"A method, a device and a computer storage medium for speech interaction are disclosed. The method includes: receiving speech data transmitted by a first terminal device; obtaining a speech recognition result and a voiceprint recognition result of the speech data; obtaining a response text for the speech recognition result, and performing speech conversion for the response text with the voiceprint recognition result; and transmitting audio data obtained from the conversion to the first terminal device. Speech self-adaptation of human-machine interaction may be achieved, and the real feeling and interest of human-machine speech interaction may be enhanced and improved, respectively.";1
870;2020035350;2018;;KOH YOUNG TECHNOLOGY INC.;KOH YOUNG TECHNOLOGY INC.;G16H50/20,G06T2207/30024,G06N3/08,G06T11/001,G06N20/00,G06T7/0014,G16H30/20,G06T2210/41,G16H70/00,G06T2207/20081,G16H10/40,G16H30/40,G16H50/50,G06T2207/20084;METHOD AND APPARATUS FOR PROCESSING HISTOLOGICAL IMAGE CAPTURED BY MEDICAL IMAGING DEVICE;A method for processing one or more histological images captured by a medical imaging device is disclosed. In this method, the histological image is received, and target regions each of which corresponds to a candidate type of tissue are identified based on a predictive model as sociating one more sample histological images with one or more sample target histological images. One or more display characteristics associated with the identified at least one target histological image is applied to the histological image.;1
872;2020043492;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L15/30,G06F3/167,G10L2015/088,G10L2015/223,G10L15/08,G10L15/22;SPEECH RECOGNITION METHOD AND APPARATUS;Disclosed are a speech recognition apparatus for speech recognition, and a method therefor. A speech recognition method for speech recognition includes detecting an event during a first spoken utterance, transmitting a suspension request signal requesting suspension of signal processing for the first spoken utterance at the point in time when the event is detected, and waiting for recognition of a second spoken utterance. According to the present disclosure, by canceling an erroneously spoken utterance through 5G network service and AI algorithm, a speech recognition process can proceed rapidly.;1
874;2020053395;2017;;Panasonic Intellectual Property Management Co., Ltd.;Panasonic Intellectual Property Management Co., Ltd.;G11B27/00,H04N21/44218,H04N21/231,H04N21/214,H04N7/18,G06F13/00,H04N21/44,H04N21/234,H04N21/4728,H04N21/6587,H04N21/21805;VIDEO DISTRIBUTION SYSTEM, USER TERMINAL DEVICE, AND VIDEO DISTRIBUTION METHOD;"A video distribution system that generates a distribution video from a captured video obtained by capturing an event venue and distributes the distribution video to user terminal via a network is provided and includes a camera installed at a predetermined position in the event venue and capturing the event venue in a predetermined direction; a captured video storage accumulating a captured video output from the camera; a video processing computer that cuts out regions respectively corresponding to a plurality of viewpoints with respect to a subject from the captured video, and generates a plurality of distribution videos different in viewpoint; and a video distribution server that distributes the distribution video corresponding to a distribution request to the user terminal device when receiving the distribution request including information on the viewpoint selected by a user from the user terminal device.";0
875;2020058288;2019;;National Taiwan University of Science and Technology;National Taiwan University of Science and Technology;G06F9/451,G10L13/033,G10L21/10,G10L13/04,G10L21/003,G06F16/31;TIMBRE-SELECTABLE HUMAN VOICE PLAYBACK SYSTEM, PLAYBACK METHOD THEREOF AND COMPUTER-READABLE RECORDING MEDIUM;A timbre-selectable human voice playback system and a timbre-selectable human voice playback method thereof are provided. The timbre-selectable human voice playback system includes a speaker, a storage and a processing apparatus. The storage saves a text database. The processing apparatus is connected to the speaker and the storage. The processing apparatus obtains real human voice signals, converts the text of the text database into original synthetic human voice signals with the text-to-speech technology, and transforms the original synthetic voice signals into timbre-specific human voice signals with a timbre transformation model. The timbre-transformation model is trained with the real human voice signals collected from a specific person. Then, the processing apparatus plays the transformed human voice signals with the speaker. Accordingly, a user can listen to his favorite voice timbre and the transformed voice signal carrying selected content anytime and anywhere.;3
876;2020058290;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L2015/025,G10L15/02,G10L2015/027,G06N3/08,G06N20/00,G06N3/04,G10L13/00,G10L13/10,G10L15/16;ARTIFICIAL INTELLIGENCE APPARATUS FOR CORRECTING SYNTHESIZED SPEECH AND METHOD THEREOF;Disclosed herein is an artificial intelligence apparatus includes a memory configured to store learning target text and human speech of a person who pronounces the text, a processor configured to generate synthesized speech in which the text is pronounced by synthesized sound and extract a synthesized speech feature set including information on a feature pronounced in the synthesized speech and a human speech feature set including information on a feature pronounced in the human speech, and a learning processor configured to train a speech correction model for outputting a corrected speech feature set to allow predetermined synthesized speech to be corrected based on a human pronunciation feature when a synthesized speech feature set extracted from predetermined synthesized speech is input, based on the synthesized speech feature set and the human speech feature set.;1
878;2020065813;2018;;Capital One Services, LLC;Capital One Services, LLC;G06N3/045,G06N3/047,G06N3/08,G06Q20/4016,G06N3/044,G06N3/084;METHODS AND ARRANGEMENTS TO DETECT FRAUDULENT TRANSACTIONS;Logic may detect fraudulent transactions. Logic may determine, by a neural network based on the data about a transaction, a deviation of the transaction from a range of purchases predicted for the customer, wherein the neural network is pretrained to predict purchases by the customer based on a purchase history of the customer. Logic may compare the deviation of the transaction from purchases predicted by the customer against a deviation threshold to determine whether the transaction is within the range of purchases predicted by the neural network. Logic may generate a notification in response to a determination that the deviation of the transaction from the range of purchases predicted exceeds a deviation threshold, the notification to identify the transaction as a potentially fraudulent transaction. Logic may train the neural network based on the transaction in response to a determination that transaction is not a fraudulent transaction.;0
879;2020066250;2019;;Toshiba Digital Solutions Corporation;Toshiba Digital Solutions Corporation;G10L13/047,G06Q30/0283,G10L13/033;SPEECH SYNTHESIS DEVICE, SPEECH SYNTHESIS METHOD, AND COMPUTER PROGRAM PRODUCT;A speech synthesis device according to an embodiment includes a speech synthesizing unit, a speaker parameter storing unit, an availability determining unit, and a speaker parameter control unit. Based on a speaker parameter value representing a set of values of parameters related to the speaker individuality, the speech synthesizing unit is capable of controlling the speaker individuality of synthesized speech. The speaker parameter storing unit is used to store already-registered speaker parameter values. Based on the result of comparing an input speaker parameter value with each already-registered speaker parameter value, the availability determining unit determines the availability of the input speaker parameter value. The speaker parameter control unit prohibits or restricts the use of the input speaker parameter value that is determined to be unavailable by the availability determining unit.;1
880;2020074498;2019;;SPRINKLR, INC.;SPRINKLR, INC.;G06Q30/0244,G06Q50/01,G06Q30/0277;SYSTEMS AND METHODS FOR IMPROVING SOCIAL MEDIA ADVERTISING EFFICIENCY;Systems and methods that enable enhanced social media advertising efficiency using mixed model equations to process advertisement data. A model is described that calculates enhanced advertisement data based on calculations using and re-using variables to isolate keys for effective advertisement.;0
881;2020075000;2018;;Halloo Incorporated;Halloo Incorporated;H04L51/214,H04L51/222,H04R3/12,G10L15/30,G06F40/58,G10L13/00,G10L15/26,G10L15/22;SYSTEM AND METHOD FOR BROADCASTING FROM A GROUP OF SPEAKERS TO A GROUP OF LISTENERS;"A processor implemented method for broadcasting from a group of speakers having speaker devices to a group of listeners having listener devices is provided. The method includes: obtaining voice inputs associated with a common topic from the speaker devices associated with the group of speakers; automatically transcribing the voice inputs to obtain text segments; obtaining at least one of a speaker rating score for at least one speaker in the group of speakers and a relevance rating score with respect to the group of listeners and a common topic for at least one of the text segments or the voice inputs; selecting at least a subset of the text segments to produce a selected subset of text segments; converting the selected subset of text segments into a selected subset of voice outputs and serially broadcasting the selected subset of voice outputs to the listener devices of the group of listeners.";1
883;2020092519;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;H04N7/152,H04N7/15,G06F3/167,G06N3/006,G06N7/01,G10L15/1822,G10L17/00,G06N3/08;VIDEO CONFERENCE SYSTEM USING ARTIFICIAL INTELLIGENCE;Disclosed is an artificial intelligence video conference system. The artificial intelligence video conference system learns content of speech of a speaker and a displayed screen using an artificial intelligence during video conference and performs various functions required for the video conference or search various information related to the video conference, thereby conducting the video conference more smoothly. At least one device of the artificial intelligence video conference system of the present disclosure may be associated with an artificial intelligence module, a robot, an augmented reality (AR) device, a virtual reality (VR) device, a device related to a 5G service, and the like.;0
884;2020099728;2019;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;H04L51/52,H04L51/18,H04L51/046,H04L65/403;CONTROL APPARATUS AND NON-TRANSITORY COMPUTER READABLE MEDIUM;There is provided a control apparatus including a processor, in which in a talk room, each of plural participating users among plural users registered for each user group is able to post information, and the processor is configured, when in the talk room, during an execution of a specific process, a second user who has been registered in a user group in which a first user is registered posts second information after the first user posts first information among a series of information related to the specific process, to process the second information as the series of information.;0
886;2020111476;2019;;Fujitsu Limited;Fujitsu Limited;G10L15/02,G10L15/005,G06F40/58,G10L2015/025,G06F40/263;RECORDING MEDIUM, LANGUAGE IDENTIFICATION METHOD, AND INFORMATION PROCESSING DEVICE;"A non-transitory computer-readable recording medium stores therein a program for causing a computer to execute processing including: converting a speech recognition result of speech recognition performed on an input voice for each of a plurality of languages into a phoneme string; calculating a phoneme count for each of the plurality of languages from the corresponding one of the phoneme strings obtained by the conversion for the respective languages; and identifying a type of language matched with the input voice based on the phoneme counts calculated for the respective languages.";1
888;2020126557;2018;;Inha University Research and Business Foundation;Inha University Research and Business Foundation;G10L15/25,G10L15/24,G10L15/22,G10L2015/227,G06F3/015,G10L21/06,G06F3/011;SPEECH INTENTION EXPRESSION SYSTEM USING PHYSICAL CHARACTERISTICS OF HEAD AND NECK ARTICULATOR;The present invention provides a speech intention expression system including a sensor part which is adjacent to one surface of the head and neck of a speaker and measures physical characteristics of articulators, a data interpretation part which grasps articulatory features of the speaker on the basis of the position of the sensor part and the physical characteristics of the articulators, a data conversion part which converts the position of the sensor part and the articulatory features to speech data, and a data expression part which expresses the speech data to the outside, wherein the sensor part includes an oral tongue sensor corresponding to the oral tongue.;1
889;2020137001;2017;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06F40/56,G06F40/35,G06F40/253,G06F40/216,G06F40/44,G06F40/30,H04L51/02;GENERATING RESPONSES IN AUTOMATED CHATTING;The present disclosure provides method and apparatus for generating responses in automated chatting. A message may be received in a session. Personality comparison between a first character and a user may be performed. A response may be generated based at least on the personality comparison, the response being in a language style of a second character.;3
890;2020142926;2019;;BEAMZ IP, LLC;BEAMZ IP, LLC;G06F16/639,G10H2210/525,G10H1/0066,G10H1/20,G10H1/0025,G10H2240/056,G10H1/36,G10H1/0008,G10H2210/066,G10H2210/081,G10H2210/076;ARTIFICIAL INTELLIGENCE METHODOLOGY TO AUTOMATICALLY GENERATE INTERACTIVE PLAY ALONG SONGS;A method and system of using Artificial Intelligence to automatically create and generate an interactive play-along song from a selected audio file/song imported from either a music streaming service, a personal music library and the like, that can later be played on an interactive music engine.;2
891;2020143384;2019;;Comenity LLC;Comenity LLC;G06Q30/01,G06N3/08,G06N20/00,G06F16/9538;TERMS AND CONDITIONS SUMMARIZING;A method of summarizing the terms and conditions of a customer agreement using the artificial intelligence of a computer system that receives a plurality of sets of terms and conditions for customer agreements. The artificial intelligence of the computer system also receives customer feedback on the plurality of sets of terms and conditions. The artificial intelligence trains to become a smart summarizer configured to summarize aspects of the plurality of sets of terms and conditions based on the customer feedback. The smart summarizer receives a new set of terms and conditions for a customer agreement. The smart summarizer creates a summary of the new set of terms and conditions. The smart summarizer outputs the summary in a human-readable format. The summary may be sent to a customer who is subject to the terms and conditions.;1
892;2020151258;2019;;Baidu Online Network Technology (Beijing) Co. Ltd.;Baidu Online Network Technology (Beijing) Co. Ltd.;G10L2015/227,G10L15/26,G10L2015/225,G10L15/1815,G10L25/78,G10L15/22,G10L15/1822,G06F40/30,G10L25/63;METHOD, COMPUTER DEVICE AND STORAGE MEDIUM FOR IMPEMENTING SPEECH INTERACTION;"The present disclosure provides a method, apparatus, computer device and storage medium for implementing speech interaction, wherein the method comprises: a content server obtaining a user's speech information from a client device, and completing the speech interaction in a first manner; the first manner comprises: sending the speech information to an automatic speech recognition server and obtaining a partial speech recognition result returned by the automatic speech recognition server each time; after determining that voice activity detection starts and if it is determined through semantic understanding that the partial speech recognition result obtained each time already includes entire content that the user hopes to express, taking the partial speech recognition result as a final speech recognition result, obtaining a response speech corresponding to the final speech recognition result, and returning the response speech to the client device. The solution of the present disclosure can be applied to improve the speech interaction response speed.";1
893;2020151458;2019;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G06N3/044,G06N3/08,G06F16/75,G06F16/787,G06F16/7867,G06V20/49,G06F16/783,G06N3/047,G06V20/46,G06N3/045;APPARATUS AND METHOD FOR VIDEO DATA AUGMENTATION;"A method and apparatus for video data augmentation that automatically constructs a large amount of learning data using video data. An apparatus for augmenting video data according to an embodiment of this disclosure, the apparatus including: a feature information check unit checking feature information including a content feature, a flow feature, and a class feature of a sub video of a predetermined unit constituting an original video; a section check unit selecting a video section including at least one sub video on the basis of the feature information of the sub video; and a video augmentation unit extracting at least one substitute sub video corresponding to the selected video section from multiple pre-stored sub videos, and applying the extracted at least one sub video to the selected video section to generate an augmented video.";3
898;2020160837;2020;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L15/005,G10L15/22,G10L2015/025,G10L15/187,G10L15/30;METHOD FOR CONTROLLING ARTIFICIAL INTELLIGENCE SYSTEM THAT PERFORMS MULTILINGUAL PROCESSING;"This specification relates to a method for controlling an artificial intelligence system which performs a multilingual processing based on artificial intelligence technology. The method for controlling an artificial intelligence system which performs a multilingual processing includes: receiving voice information through a microphone; determining a language of the voice information, based on a preset reference; selecting a specific voice recognition server from a plurality of voice recognition servers which process different languages, based on a result of the determination; and transmitting the voice information to the selected specific voice recognition server.";1
899;2020162736;2019;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;H04N19/18,H04N19/184,H04N19/124,H04N19/154;METHOD AND APPARATUS FOR IMAGE PROCESSING USING QUANTIZATION PARAMETER;Disclosed herein are a video decoding method and apparatus and a video encoding method and apparatus. In image encoding, encoding using multiple quantization parameters is performed on a transform coefficient. Two quantized coefficients are generated through encoding that uses two quantization parameters. Based on the two quantized coefficients, a quantized coefficient difference is generated. Information about the quantized coefficient difference is transmitted from an encoding apparatus to a decoding apparatus through binary encoding. The information about the quantized coefficient difference may be selectively transmitted. By means of the information about the quantized coefficient difference, a high-quality video may be provided.;1
900;2020167864;2019;;loanDepot.com, LLC;loanDepot.com, LLC;G06Q40/03,G06Q50/167,G06Q50/165,G06Q20/0855;INTEGRATED LENDING-AND-BROKERING ENVIRONMENT WITH ENTITY-RELATIONSHIP MANAGEMENT AND METHODS THEREOF;Disclosed herein is an integrated lending-and-brokering environment including, in some embodiments, a lending platform, a brokering platform, third-party integration, and an entity-relationship management layer configured for information sharing among the lending platform, the brokering platform, and one or more third parties. The lending platform is configured to facilitate processing of lending-related information among lending personnel. The brokering platform is configured to facilitate processing of brokering-related information among brokering personnel. The third-party integration includes one or more interfaces with the lending-and-brokering environment, which allows the one or more third-parties to at least contribute additional information for the processing of the lending-and-brokering-related information. Information sharing among the lending platform, the brokering platform, and the one or more third parties facilitates selling one or more assets, buying one or more assets, or a combination thereof for a customer without a need for the customer to provide duplicative customer information.;0
901;2020170542;2018;;Ice Neurosystems, Inc.;Ice Neurosystems, Inc.;A61B5/293,A61B5/7435,A61B5/24,A61B2562/02,A61B5/4064,A61B5/02,A61B5/0042,A61B5/031,A61B2560/0468,A61M27/006,A61B5/065,A61B5/6868;SYSTEMS AND METHODS FOR POSITIONING AN INTRACRANIAL DEVICE USING BRAIN ACTIVITY;Systems and methods for positioning an intracranial device are disclosed. Certain embodiments of the invention encompass devices configured for implantation within the body that include elements responsible for detecting and transmitting electrical activity from the surrounding tissues and fluids. The system may include associated hardware and software designed for transmitting, processing, analyzing, and displaying relevant aspects of detected electrical activity. This information can be used throughout or following an insertion procedure to optimize or confirm device position within a particular intracranial location or tissue compartment.;0
902;2020175383;2018;;Clover Health;Clover Health;G06F16/252,G06N3/045,G06N5/02,G06F16/2379,G06N3/088,G06N5/025,G06N7/01;Statistically-Representative Sample Data Generation;Systems and methods for statistically-representative sample data generation are disclosed. For example, a sample-data generator and/or a data discriminator may be received by a system, which may utilize the sample-data generator to generate sample data. The data discriminator may be utilized to train the sample-data generator until the data discriminator cannot discriminate between data received from the sample-data generator and data received by a database associated with the system. The trained sample-data generator may be sent to other systems, which may generate and utilize, such as for prediction model training, statistically-representative sample data generated by the trained sample-data generator.;3
905;2020211134;2019;;Social Equity Incorporated;Social Equity Incorporated;G06Q30/06,G06Q30/04,G06Q30/0278,G06Q50/16,G06Q40/06,G06Q50/167;SYSTEM AND METHOD FOR PROVIDING ONBOARDING, CONFIGURATION AND EXCHANGE OF REAL ESTATE UNITS INTO A REAL ESTATE OWNERSHIP MOBILITY PLATFORM;The present disclosure relates to methods and systems for providing onboarding, configuration and exchange of real estate units into a real estate ownership mobility platform. More specifically embodiments of the present invention are directed to systems and methods for factionalizing real estate units into tokenizable commodities and managing the exchange of transactions associated with those tokenized commodities to allow for home ownership mobility.;0
914;2020226216;2019;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06N20/00,G10L15/22,G06F16/24578,G06F40/30,G06F40/253,G06F16/345;CONTEXT-SENSITIVE SUMMARIZATION;This document relates to compression of information into a human-readable format, such as a sentence or phrase. Generally, the disclosed techniques can extract values, such as purposes and topics, from information items and generate compressed representations of the information items that include the extracted values. In some cases, machine learning models can be employed to extract the values, and also to rank the values for inclusion in the compressed representations.;2
915;2020226327;2020;;Applications Technology (AppTek), LLC;Applications Technology (AppTek), LLC;G06N3/08,G06N3/044,G10L15/005,G06F40/58,G06N7/01,G06F40/45,G06F40/42,G06F18/214,G06N3/045,G10L25/90,G10L15/26,G06N20/00,G10L13/00;SYSTEM AND METHOD FOR DIRECT SPEECH TRANSLATION SYSTEM;A system for translating speech from at least two source languages into another target language provides direct speech to target language translation. The target text is converted to speech in the target language through a TTS system. The system simplifies speech recognition and translation process by providing direct translation, includes mechanisms described herein that facilitate mixed language source speech translation, and punctuating output text streams in the target language. It also in some embodiments allows translation of speech into the target language to reflect the voice of the speaker of the source speech based on characteristics of the source language speech and speaker's voice and to produce subtitled data in the target language corresponding to the source speech. The system uses models having been trained using (i) encoder-decoder architectures with attention mechanisms and training data using TTS and (ii) parallel text training data in more than two different languages.;1
916;2020226752;2020;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G06T2207/20081,G06T7/0012,A61B5/055,G06T2207/10116,A61B5/7264,G06T2207/20084,G06T2207/20221,G06T2207/30096,A61B5/4842,G06T2207/30061,G06T2207/10072,G06T5/60,A61B5/7425,G06T5/77;APPARATUS AND METHOD FOR PROCESSING MEDICAL IMAGE;"A medical image processing apparatus includes: a data acquisition unit configured to acquire at least one normal medical image and at least one abnormal medical image; and one or more processors configured to perform first processing for generating at least one first medical image by using a neural network and second processing for determining whether the at least one first medical image is a real image, based on the at least one abnormal medical image, wherein the first processing includes generating a virtual lesion image based on a first input and generating the at least one first medical image by synthesizing the virtual lesion image with the at least one normal medical image, and the one or more processors are further configured to train the neural network used in the first processing, based on a result of the second processing.";3
917;2020227069;2019;;Baidu Online Network Technology (Beijing) Co. Ltd.;Baidu Online Network Technology (Beijing) Co. Ltd.;G10L17/04,G10L15/07,G10L15/22,G10L17/00,G10L17/26,G10L25/51,G10L15/063,G10L15/32;METHOD, DEVICE AND APPARATUS FOR RECOGNIZING VOICE SIGNAL, AND STORAGE MEDIUM;"A method, device and apparatus for recognizing a voice signal, and a storage medium are provided. The method includes: collecting a voice signal; extracting the voiceprint feature of the voice signal; comparing the voiceprint feature with a pre-stored reference voiceprint feature; and recognizing a content of the voice signal with a voice recognition model, in response to a consistence of the voiceprint feature with the pre-stored reference voiceprint feature. Embodiments of the present application can improve the accuracy of recognizing voice signals.";1
919;2020251105;2017;;TOROOC INC.;TOROOC INC.;G06V40/174,G10L2015/225,G06V40/50,B25J11/0005,G10L15/22,G06V40/50,G06V40/172,G06V40/50,G06Q50/40,B25J9/16;METHOD AND SYSTEM FOR PROVIDING CONVERSATION SERVICE BY USING AUTONOMOUS BEHAVIOR ROBOT, AND NON-TRANSITORY COMPUTER-READABLE RECORDING MEDIUM;"According to one aspect of the present invention, provided is a method for providing a conversation service by using an autonomous behavior robot, comprising the steps of: recognizing a user corresponding to acquired face information; determining conversation content to be provided to the user on the basis of a personal attribute related to the recognized user and/or the reliability of the personal attribute; and updating the personal attribute and/or the reliability of the personal attribute on the basis of the user's feedback for the conversation content.";1
920;2020257962;2019;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;G06V10/764,G06V10/454,G06N3/047,G06N3/084,G06V10/82,G06N3/044,G06N3/088,G06F18/214,G06F18/2178,G06N3/08,G06N3/02,G06V20/20,G06N3/045;CONTROLLABLE AND INTERPRETABLE CONTENT CONVERSION;Systems and methods are described for converting input content. A first model may convert input content to an output content that exhibits one or more desired properties. A second model may determine if the conversion meets a desired quality of conversion using a discriminating function. The discriminating function may determine a difference between properties of the output content and properties of desired content, where the difference corresponds to the success of the conversion applying the desired properties. Updated control data may be generated by a third model using information from the second model, where the updated control data may be used by the first model to reduce the determined difference. After updated control data has been generated, the foregoing steps may be repeated based upon the updated control data. One of a plurality of different actions may be determined in response to the difference.;3
921;2020258608;2020;;DecaWave, Ltd.;DecaWave, Ltd.;G16Z99/00,G16H20/10,G16H10/60,G16H50/70,G16H50/20,G06Q10/10;MEDICAL DATABASE AND SYSTEM;A medical database and system and method using same are provided. The medical database includes a data unit for storing modules representing medical records of subjects. Each module includes a plurality of module elements each representing a medically-relevant parameter of the subject with each element assigned a specific identifier in the module and a numerical value corresponding to the medically-relevant parameter.;0
922;2020265394;2020;;LINE PAY CORPORATION;LINE PAY CORPORATION;G06Q20/085,G06Q20/386,G06Q20/065,G06Q20/227,G06Q20/3223,G06Q20/405,G06Q20/14,G06Q20/326,G06Q20/102;INFORMATION PROCESSING PROGRAM, METHOD, DEVICE, AND SYSTEM;Payment processing techniques and management control techniques concerning improved split bills and cost sharing are provided. A method for performing adjustment of payment between a first user associated with a first information processing terminal registered in a group managed on a server and one or more second users associated with one or more second information processing terminals registered in the group includes displaying the one or more second users associated with the one or more second information processing terminals on a screen of the first information processing terminal, and selecting persons who accept payment from the displayed one or more second users, via an interface on the screen of the first information processing terminal.;0
923;2020272422;2018;;NIPPON TELEGRAPH AND TELEPHONE CORPORATION;NIPPON TELEGRAPH AND TELEPHONE CORPORATION;G06F17/16,G06F7/582,G06F17/18,G06F7/588;SYNTHETIC DATA GENERATION APPARATUS, METHOD FOR THE SAME, AND PROGRAM;"A synthetic data generation apparatus includes: a random number generating unit that generates first synthetic data with a ratio of a frequency distribution of each attribute being approximate to the ratio of the frequency distribution of that attribute in target data for which synthetic data is to be generated; and a data formatting unit that formats the first synthetic data using a matrix given by Cholesky decomposition of a variance-covariance matrix of the target data or a scaling matrix given by singular value decomposition of the variance-covariance matrix of the target data such that a mean vector and a correlation matrix of the first synthetic data agree with a mean vector and a correlation matrix of the target data and that a minimum and a maximum of the first synthetic data are present in ranges of a minimum and a maximum of the target data, and provides the first synthetic data after formatting as synthetic data.";3
924;2020279024;2019;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;G06F40/47,G06F40/58,G06F40/279,G06F40/56,G06F40/194,G06N3/044,G06N3/08,G06N3/045;NON-TRANSITORY COMPUTER READABLE MEDIUM;"A non-transitory computer readable medium stores a program causing a computer to execute a process for learning. The process includes: generating, from an input text, an output text related to content of the input text and different from the input text by using a generation model that generates the output text from the input text; reconstructing the input text from the output text by using a reconstruction model that reconstructs the input text from the output text; and updating at least one of the generation model and the reconstruction model by causing the at least one of the generation model and the reconstruction model to perform learning by using a difference between the input text and a reconstructed text reconstructed in the reconstructing.";1
925;2020279550;2020;;FUJITSU CLIENT COMPUTING LIMITED;FUJITSU CLIENT COMPUTING LIMITED;G10L15/22,G10L2021/0135,G10L21/007,G10L15/26,G10L13/027,G10L13/033,G10L13/047,G06V40/174,G10L13/00,G10L13/08,G10L15/24;VOICE CONVERSION DEVICE, VOICE CONVERSION SYSTEM, AND COMPUTER PROGRAM PRODUCT;"A voice conversion device includes: a voice converter that converts an input voice into a voice conversion signal for output; a voice processing unit that performs speech recognition of the input voice in parallel with the voice conversion, and sequentially outputs text data for voice synthesis; a storage that stores therein the text data; an input operation unit that receives designation of the text data and an output instruction; a voice synthesizer that outputs a voice synthesis signal based on the designated text data; and a voice output that outputs a voice based on the voice conversion signal, and outputs a voice based on the voice synthesis signal, in response to the designation of the text data and the output instruction.";1
926;2020285810;2020;;App2Check S.r.l.;App2Check S.r.l.;G06N5/01,G06F16/2379,G06N20/00,G06F40/30,G06F40/279,G06N5/04,G06F16/3329,G06F16/90332;SYSTEM AND METHOD FOR EXTRACTING INFORMATION FROM UNSTRUCTURED OR SEMI-STRUCTURED TEXTUAL SOURCES;A method for extracting and realizing from a non-structured or semi-structured textual source a Knowledge Base for chatbot having the phases of applying a process to the textual source is provided. The process has at least the phase of automatically finding âquestionâ nodes in the textual source, and the phase having the sub-phases of: generating a representative tree of text nodes present in the textual source, extracting, by way of heuristics and/or a predictive model, certain features in the text node as the more recurring features and selectively attributing to the text nodes that comprise the most recurring characteristics, the âquestionâ node feature, regardless of the fact that the text nodes have a question mark â?â among the extracted features. The invention also refers to a system arranged to implement the method.;1
927;2020293719;2020;;Toyota Jidosha Kabushiki Kaisha;Toyota Jidosha Kabushiki Kaisha;G06F40/253,G06Q50/01,G06N20/00,G06F40/56,G06F40/279,G06N5/04,G06F40/30;RECOMMENDATION SENTENCE GENERATION DEVICE, RECOMMENDATION SENTENCE GENERATION METHOD, AND RECOMMENDATION SENTENCE GENERATION PROGRAM;A recommendation sentence generation device according to the disclosure is a recommendation sentence generation device that generates a recommendation sentence about a facility. This recommendation sentence generation device is equipped with a selection unit that selects document data written about the facility, based on an appearance frequency of a topic word that is associated with the facility, and a correction unit that corrects a predetermined word that is included in the selected document data.;1
930;2020304446;2019;;FUJI XEROX CO., LTD.;FUJI XEROX CO., LTD.;H04L51/02,H04L51/226,H04L51/046,H04L51/216,H04L51/18;MESSAGE COMMUNICATION APPARATUS, NON-TRANSITORY COMPUTER READABLE MEDIUM, AND DISPLAY CONTROL METHOD;A message communication apparatus includes an operating unit, and a controller. The operating unit activates a chat bot. The chat bot runs in a messaging service in which users exchange a message. The chat bot exchanges a message with a user. The controller causes, if the chat bot includes multiple chat bots activated through operation of the operating unit by multiple users, a chat bot activated by a specific user to respond to a message from the specific user.;3
931;2020311350;2020;;Fujitsu Limited;Fujitsu Limited;G06F40/216,G06N3/044,G06N3/08,G06F17/18,G06F40/284,G06F40/30,G06N3/045,G06F40/242,G06F40/295;GENERATING METHOD, LEARNING METHOD, GENERATING APPARATUS, AND NON-TRANSITORY COMPUTER-READABLE STORAGE MEDIUM FOR STORING GENERATING PROGRAM;"A generating method includes: obtaining input text; calculating, for each encoder time corresponding to a word string in the input text, a hidden state at the encoder time from a hidden state at one previous encoder time based on a word in the input text and a label of a named entity corresponding to the encoder time; executing an input processing that includes inputting the hidden state output from the encoder to a decoder; calculating, for each decoder time corresponding to the word string in a summary output from the decoder, a hidden state at the decoder time from a hidden state at one previous decoder time based on the word and label of the named entity in the summary generated at the one previous decoder time.";2
932;2020312457;2020;;The Trustees of Indiana University;The Trustees of Indiana University;Y02A90/10,G06N5/01,G16H50/50,G16H50/70,G16H50/20,G06N3/047,G06N3/088,G06N20/00,G06N3/045,G06N20/20,G16H10/60;METHOD AND SYSTEM FOR CREATING SYNTHETIC UNSTRUCTURED FREE-TEXT MEDICAL DATA FOR TRAINING MACHINE LEARNING MODELS;A method is provided for creating synthetic unstructured free-text medical data that closely mimics real data for enabling machine learning research, but with limited re-identification risk. The method includes leveraging two neural networks that compete with each other (adversarial networks) to create a synthetic message dataset that closely mimics the real medical data. Machine learning models trained using the synthetic data yield performance metrics that are statistically similar to models trained using the real dataset, ensuring that our approach can be used to replicate machine learning studies. Further, the synthetic message datasets can be easily shared with researchers with limited re-identification risk.;3
939;2020358967;2018;;SAMSUNG ELECTRONICS CO., LTD.;SAMSUNG ELECTRONICS CO., LTD.;H04N5/278,G10L13/00,H04N21/4355,G10L13/08,H04N21/439,H04N21/4884,H04N21/44008;DISPLAY DEVICE AND CONTROL METHOD THEREFOR;Disclosed herein is a display apparatus and a control method thereof, and more particularly, to a technology configured to extract captions displayed on a display screen, convert the captions into voice, and output the voice. The display apparatus includes a display, a sound outputter configured to output sound, and a controller configured to select a caption data acquisition method based on the type of caption displayed on the display, configured to convert the caption data, which is obtained according to the selected caption data acquisition method, into voice data, and configured to allow the sound outputter to output a content of the displayed caption as voice based on the voice data that is converted.;1
944;2020394258;2019;;SoundHound, Inc.;SoundHound, Inc.;G06F40/166,G10L15/22,G06F40/284,G10L15/26,G10L2015/228,G10L15/1815;GENERATION OF EDITED TRANSCRIPTION FOR SPEECH AUDIO;"The present disclosure is intended to provide method, manufacture products, and apparatuses for generating an edited transcription of a speech audio in a SR-NLU system. A method for generating an edited transcription of a speech audio may include performing automatic speech recognition on the speech audio to produce a transcription having one or more tokens; interpreting the transcription according to each of a plurality of natural language domains to produce a plurality of interpretation results; identifying, based on the plurality of interpretation results, a natural language domain that matches the transcription; and replacing a token of interest in the transcription with a replacement token according to a predefined mapping specific to the identified natural language domain to generate the edited transcription of the speech audio.";1
945;2020395008;2019;;Very Important Puppets Inc.;Very Important Puppets Inc.;G06F40/284,G10L15/22,G10L15/1815,G10L15/30,G10L15/1822,G10L15/19,G10L15/16,G06F40/30,G10L13/047,G06F40/253,G10L2015/223,G06F40/205,G10L13/027,G06F40/268,G10L13/033;Personality-Based Conversational Agents and Pragmatic Model, and Related Interfaces and Commercial Models;"Whereas contemporary chatbots use conversation as a means to execute a task, the present invention generates conversation as an enjoyable interaction central to the human experience. A conversational API is modeled on speech from real or fictitious personalities, and enables humorous and useful conversation, music streaming, digital assistant tasks with humans or other agents. The present invention affords a richer, more human level of conversation over corporate, generic digital devices and assistants. The API is comprised of speech input, which is fed into a natural language understanding (NLU) pipeline, which is trained on a corpus of labeled speech samples harvested from the speaker by means of a neural network, or pragmatic model; the speech is then fed into a personality model, then into a natural language generation (NLG) pipeline, from which speech is selected from a database and modified, to emit a reply. The pragmatic model consists of a detailed and subtle labeling model, and pairing model, wherein input and output sentences are labeled according to a rich classification system of tonal and semantic nuances. The personality model exhibits a predetermined preference for certain tonal, intentional and functional labels according to that personality, which has been trained on labeled speech input in the pragmatic model. Labels include lexical, semantic, syntactic, demographic, contextual and voice attributes, to create a range of identifiable personas. Varied instances of personality models create a library of artificially intelligent conversational models, or personality fonts, which are distinct from each other in terms of conversation style. A user may interact with a conversational agent as a formless digital agent, or chatbot. These caricatured personalities may also take on a skeuomorphic or anthropomorphic form, as a talking physical device which caricatures a known person or fictitious character. Users may further personalize their agent instance from the library, by means of adding digital swag or assets to a digital representation of the avatar; or operating their avatar in a simulation game room or chat lobby, whereby accumulating points, audience, or experiences specific to their instance. The API may also stream music playlists, selected according to common themes in the music and the personality model; or, if the personality model is based on a musician, API may stream the musician's works.";3
946;2020396190;2020;;Darktrace Holdings Limited;Darktrace Holdings Limited;H04L51/212,H04L63/1433,G06N20/00,H04L63/1483,H04L67/10,H04L63/1425,H04L63/1441,H04L63/14,H04L51/18,H04L67/306;ENDPOINT AGENT EXTENSION OF A MACHINE LEARNING CYBER DEFENSE SYSTEM FOR EMAIL;An endpoint agent extension of a cyber defense system for email that includes modules and machine learning models. An integration module integrates with an email client application to detect email cyber threats in emails in the email client application as well as regulate emails. An action module interfaces with the email client application to direct autonomous actions against an outbound email and/or its files when a cyber threat module determines the email and/or its files (a) to be a data exfiltration threat, (b) to be both malicious and anomalous behavior as compared to a user's modeled email behavior, and (c) any combination of these. The autonomous actions can include actions of logging a user off the email client application, preventing the sending of the email, stripping the attached files and/or disabling the link to the files from the email, and sending a notification to cyber security personnel regarding the email.;0
952;2020410285;2020;;"The Board of Trustees of the Leland Stanford Junior University;Ford Global Technologies, LLC";The Board of Trustees of the Leland Stanford Junior University;G06N3/047,G06N3/084,G06N3/088,G06V10/82,G06V10/764,G06V10/7747,G06F18/2433,G06N3/045,G06F18/2148;Anomaly Augmented Generative Adversarial Network;Systems and methods for anomaly detection in accordance with embodiments of the invention are illustrated. One embodiment includes a method for training a system for detecting anomalous samples. The method draws data samples from a data distribution of true samples and an anomaly distribution and draws a latent sample from a latent space. The method further includes steps for training a generator to generate data samples based on the drawn data samples and the latent sample, and training a cyclic discriminator to distinguish between true data samples and reconstructed samples. A reconstructed sample is generated by the generator based on an encoding of a data sample. The method identifies a set of one or more true pairs, a set of one or more anomalous pairs, and a set of one or more generated pairs. The method trains a joint discriminator to distinguish true pairs from anomalous and generated pairs.;1
955;2021012764;2020;;MINDS LAB INC.;MINDS LAB INC.;G06N3/08,G10L13/047,G06N3/045,G10L21/0272,G10L25/30,G06N3/044,H04R3/12;METHOD OF GENERATING A VOICE FOR EACH SPEAKER AND A COMPUTER PROGRAM;A method of generating a voice for each speaker from audio content including a section in which at least two or more speakers simultaneously speak is provided. The method includes dividing the audio content into one or more single-speaker sections and one or more multi-speaker sections, determining a speaker feature value corresponding to each of the one or more single-speaker sections, generating grouping information by grouping the one or more single-speaker sections based on a similarity of the determined speaker feature value, determining a speaker feature value for each speaker by referring to the grouping information, and generating a voice of each of multiple speakers in each section from each of the one or more multi-speaker sections by using a trained artificial neural network and the speaker feature value for each individual speaker.;2
958;2021035340;2019;;Rensselaer Polytechnic Institute;Rensselaer Polytechnic Institute;A61B6/032,G06N3/10,G06N3/04,A61B5/0033,G16H30/40,G06V2201/03,G06V10/82,G16H50/20,A61B6/5211,G06N20/00,G16H50/50,G06T11/006,G06F18/217,G06N3/08,G06F18/214;CT BIG DATA FROM SIMULATION, EMULATION AND TRANSFER LEARNING;"In some embodiments, a method of machine learning includes identifying, by an auto encoder network, a simulator feature based, at least in part, on a received first simulator data set and an emulator feature based, at least in part, on a received first emulator data set. The method further includes determining, by a synthesis control circuitry, a synthesized feature based, at least in part, on the simulator feature and based, at least in part, on the emulator feature; and generating, by the auto encoder network, an intermediate data set based, at least in part, on a second simulator data set and including the synthesized feature. Some embodiments of the method further include determining, by a generative artificial neural network, a synthesized data set based, at least in part, on the intermediate data set and based, at least in part, on an objective function.";3
960;2021042622;2020;;BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT CO., LTD.;BEIJING DIDI INFINITY TECHNOLOGY AND DEVELOPMENT CO., LTD.;G01C21/3446,G06Q10/047,G08G1/0129,G06N3/045,G08G1/096816,G06Q50/40,G08G1/012,G08G1/0145,G06N3/047,G06N3/08,G08G1/202,G06N3/088;SYSTEMS AND METHODS FOR PROVIDING A TRAVELLING SUGGESTION;"Systems and methods for providing a travelling suggestion to an interface on a user terminal in an online on-demand transportation service are provided. A method includes: receiving a service request from a user terminal; obtaining a prediction model combining at least one Generative Adversarial Networks (GAN) and at least one Restricted Boltzmann Machines (RBM); determining at least one recommended route for the user terminal based on the service request and the prediction model; generating electronic signals including the recommended route and a triggering code; and sending the electronic signals to at least one antenna to direct the antenna to send the electronic signals to the user terminal, wherein the triggering code is: in a format recognizable by an operation system of the user terminal, and configured to render the operation system of the user terminal to generate a presentation of the recommended route on an interface of the user terminal.";1
962;2021049452;2020;;INTUIT INC.;INTUIT INC.;G06F11/0793,G06F11/079,G06N3/088,G06N3/044,G06N3/045,G06N3/047,G06N3/063,G06F11/0709,G06N3/08,G06N3/049;CONVOLUTIONAL RECURRENT GENERATIVE ADVERSARIAL NETWORK FOR ANOMALY DETECTION;An anomaly detection service executed by a processor may receive multivariate time series data and format the multivariate time series data into a final input shape configured for processing by a generative adversarial network (GAN). The anomaly detection service may generate a residual matrix by applying the final input shape to a generator of the GAN, the residual matrix comprising a plurality of tiles. The anomaly detecting service may score the residual matrix by identifying at least one tile of the plurality of tiles having a value beyond a threshold indicating an anomaly. The processor may perform at least one remedial action for the anomaly in response to the scoring.;1
964;2021065006;2019;;Hexagon Technology Center GmbH;Hexagon Technology Center GmbH;G06F30/27,G06F30/13,G06N3/084,G06N3/047,G06F2111/02,G06N20/00,G06N3/088,G06N7/01,G06N3/045,G06N3/126,G06N3/006;CONSTRUCTION SEQUENCING OPTIMIZATION;Disclosed are methods and systems for training an artificial-intelligence structure based on designs of past fabrication or construction projects, and for automatically generating, by the trained artificial-intelligence structure and based on inputs related to an actual fabrication or construction project, designs for the actual project.;3
965;2021065033;2020;;TATA CONSULTANCY SERVICES LIMITED;TATA CONSULTANCY SERVICES LIMITED;G06F17/18,G06N7/01,G06N20/00;SYNTHETIC DATA GENERATION USING BAYESIAN MODELS AND MACHINE LEARNING TECHNIQUES;Synthetic data generation using conventional statistical approaches or Machine Learning based approaches are not effective as each of them used independently does not capture the features/advantages of the other approach. The method disclosed provides a hybrid approach. A Bayesian model is used for generating synthetic data based on a single behavioral user trait for a plurality of rows. Further, a Machine learning (ML) model based approach is used to incrementally generate the remaining columns of the data set providing values of other features of interest.;3
966;2021065695;2020;;Fujitsu Limited;Fujitsu Limited;G10L25/48,G10L15/26,G06F40/53,G06F40/30,G06F40/35,G10L25/45,G06F40/284,G10L15/1815,G06F40/129;PROGRAM STORAGE MEDIUM, METHOD, AND APPARATUS FOR DETERMINING POINT AT WHICH TREND OF CONVERSATION CHANGED;"A program causes a computer to execute a process for determining points at which the trend of conversation changed. The process includes: obtaining conversation data indicating the content that users spoke in a specified period of time; and determining a point at which the trend of conversation changed in the specified period, based on at least one of the following information items: information on the number of uttered words or characters, information on the number of speakers, and information on the frequency of utterances of positive words or negative words, determined for each unit time based on the obtained conversation data.";1
967;2021073476;2020;;Fujitsu Limited;Fujitsu Limited;G06K19/06037,G06Q30/0613,G06Q50/26,H04L51/02,G06F13/00,G06Q30/0631,G06K7/1417,G06F40/35;RECORDING MEDIUM, CONVERSATION CONTROL METHOD, AND INFORMATION PROCESSING APPARATUS;"A computer-readable recording medium stores therein a program causing a computer to execute a process including receiving, from a first information processing terminal, a start instruction for a chat and identification information corresponding to the first information processing terminal; determining, based on the received identification information, a first script to be applied to a first conversation performed by a chatbot via the first information processing terminal, and starting the first conversation performed by the chatbot via the first information processing terminal, using the determined first script; and when receiving, from a second information processing terminal, transfer information that indicates transfer from the first conversation, determining, based on the received transfer information, a second script to be applied to a second conversation performed by the chatbot via the second information processing terminal, and starting the second conversation, using the determined second script, the second conversation being performed, following the first conversation.";1
970;2021074377;2020;;WISCONSIN ALUMNI RESEARCH FOUNDATION;WISCONSIN ALUMNI RESEARCH FOUNDATION;G16B15/00,C12N15/1027,G16B40/20,C40B40/08,C12N15/1089,G16B40/00;SYSTEMS AND METHODS FOR FULLY AUTOMATED PROTEIN ENGINEERING;Systems and methods for protein engineering. The systems include a sequence testing subsystem and a machine learning subsystem. The sequence testing subsystem is configured to express proteins and test the expressed proteins for a given property. The machine learning subsystem is configured to model the activities of a set of possible proteins in light of the properties of the tested proteins and provide one or more untested proteins in the set to the sequence testing subsystem for subsequent testing. The system can be run in an iterative fashion and be fully automated. Methods of using the systems are provided.;1
971;2021078180;2020;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G05D1/0217,B25J13/003,B25J13/081,G05D1/0246,B25J19/026,B25J9/1664,B25J19/02,B25J11/0005,B25J11/008,B25J9/161,B25J5/007;ROBOT SYSTEM AND CONTROL METHOD OF THE SAME;A robot system includes a mobile robot configured to travel by driving wheels, a user interface, via which user service information and user information are input, and a controller configured to select one of at least two paths including a path including a moving walkway by using the user information and generate a map of a selected path, if the user service information and the user information are input via the user interface, and move the mobile robot to the path of a generated map.;0
972;2021081805;2019;;NIPPON TELEGRAPH AND TELEPHONE CORPORATION;NIPPON TELEGRAPH AND TELEPHONE CORPORATION;G01M99/00,G06N3/047,G10L25/30,G06N3/088,G06N3/045;MODEL LEARNING APPARATUS, MODEL LEARNING METHOD, AND PROGRAM;The present disclosure relates to a method of machine learning regardless of the number of dimensions of the samples. The method provides model learning of a variational auto-encoder that uses AUC optimization criteria. The method includes learning parameters Î¸{circumflex over (â)} and Ï{circumflex over (â)} of the a variational auto-encoder. The variational auto-encoder includes an encoder for constructing a latent variable from an observed variable and a decoder for reconstructing the observed variable. The method uses learning data set defined using based on normal data generated from sounds observed during normal operation and abnormal data generated from sounds observed during abnormal operation. The AUC value is based in part on a reconstruction probability. Incorporating aspects of the reconstruction error into the AUC value prevents the variational auto-encoder from divergence of the abnormality degree regarding the abnormal data.;1
973;2021082421;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G10L2015/223,G10L25/51,G10L13/00,G10L2015/228,G10L13/033,G10L25/84,G10L15/22,G10L15/30;METHOD AND DEVICE FOR SPEECH PROCESSING;Disclosed are a speech processing method and a speech processing apparatus, characterized in that a speech processing is carried out by executing an artificial intelligence (AI) algorithm and/or a machine learning algorithm, such that the speech processing apparatus, a user terminal, and a server can communicate with each other in a 5G communication environment. The speech processing method according to one exemplary embodiment of the present invention includes converting a response text, which is generated in response to a spoken utterance of a user, to a spoken response utterance, obtaining external situation information while outputting the spoken response utterance, generating a dynamic spoken response utterance by converting the spoken response utterance on the basis of the external situation information, and outputting the dynamic spoken response utterance.;3
974;2021085558;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G06V40/176,A61H23/02,G06V10/82,A61H2205/10,A61H2205/12,G06N3/045,G06V40/164,G06N3/044,A61H2201/1633,G06V40/169,A61H2201/5048,A61H2201/5041,A61H2205/088,A61H2205/081,A61H2201/0149,A61H2201/1238,G06F3/167,A61H2205/02,A61H2201/1215,A61H1/00,A61H2201/5043,G10L2015/223,G06V10/764,G06N3/08,G10L15/22,G10L15/1822,G10L25/63,A61H2201/501,A61H2205/106,A61H2203/0431,A61H2201/5058,G06V40/161,A61H9/0078,A61H2205/06;ARTIFICIAL INTELLIGENCE MASSAGE APPARATUS AND METHOD FOR CONTROLLING MASSAGE OPERATION IN CONSIDERATION OF FACIAL EXPRESSION OR UTTERANCE OF USER;An artificial intelligence massage apparatus for controlling a massage operation according to an embodiment of the present disclosure includes a microphone, a camera, a driver comprising at least one motor, and a processor configured to obtain image data including a face of a user via the camera, determine whether the user is uttering using the obtained image data, obtain speech data via the microphone, if it is determined that the user is uttering, generate intention information corresponding to the obtained speech data, and control a massage operation based on the generated intention information by controlling the driver.;1
975;2021089903;2020;;NAVER CORPORATION;NAVER CORPORATION;G06T3/4046,G06F18/2132,G06N3/08,G06N3/047,G06N3/088,G06N3/063,G06N20/20,G06T3/4053,G06N3/084,G06N3/045;NEURAL NETWORK FOR GENERATING IMAGES TRAINED WITH A GENERATIVE ADVERSARIAL NETWORK;A generative adversarial network, a method of training a generator neural network, and a method of generating images using the generator network is provided. The generator neural network is configured to process an input comprising a noise vector and a pair of conditioning variables to generate an image according to the conditioning variables. The generator neural network includes a mixed-conditional batch normalization layer. The mixed-conditional batch normalization layer is configured to normalize a network layer output to generate a normalized network layer output, comprising transforming the network layer output in accordance with mixed-conditional batch normalization layer parameters to generate the normalized network layer output, wherein the mixed-conditional batch normalization layer parameters are computed by applying an affine transformation to the conditioning variables.;3
976;2021089904;2020;;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY;G06F17/18,G06N3/044,G06F40/56,G06F40/30,G06N3/045,G06N3/08,G06N3/04;LEARNING METHOD OF NEURAL NETWORK MODEL FOR LANGUAGE GENERATION AND APPARATUS FOR PERFORMING THE LEARNING METHOD;The present invention provides a new learning method where regularization of a conventional model is reinforced by using an adversarial learning method. Also, a conventional method has a problem of word embedding having only a single meaning, but the present invention solves a problem of the related art by applying a self-attention model.;1
978;2021090690;2019;;BenevolentAI Technology Limited;BenevolentAI Technology Limited;G16C20/70,G16C20/50;MOLECULAR DESIGN USING REINFORCEMENT LEARNING;"Method(s), apparatus and system(s) are provided for designing a compound exhibiting one or more desired property(ies) using a machine learning (ML) technique. This may be achieved by generating a second compound using the ML technique to modify a first compound based on the desired property(ies) and a set of rules for modifying compounds; scoring the second compound based on the desired property(ies); determining whether to repeat the generating step based on the scoring; and updating the ML technique based on the scoring prior to repeating the generating step.";3
980;2021097274;2019;;UiPath, Inc.;UiPath, Inc.;G06V30/19167,G06F40/279,G06F40/106,G06F8/20,G06F18/2431,G06N20/00,G06V30/416,G06V30/19113,G06V30/10,G06F40/30,G06F40/10,G06F8/24,G06F18/217,G06V30/412,G06V30/153,G06F40/123,G06F40/40,G06F18/259,G06F40/143,G06F16/93,G06F18/285;DOCUMENT PROCESSING FRAMEWORK FOR ROBOTIC PROCESS AUTOMATION;A document processing framework (DPF) for robotic process automation (RPA) is provided. The DPF may allow plug-and-play use of different vendor products on same platform, where users can setup a basic schema for document processing and document understanding workflow. The DPF may allow users to define a taxonomy, digitize a file, classify the file into one or more document types, validate the classification, extract data, validate the extracted data, train classifiers, and/or train extractors. A public package may be provided that can be used by software developers to manage the DPF and build their own classifier and extractor components.;0
981;2021097402;2020;;Fujitsu Limited;Fujitsu Limited;G06N3/045,G06N3/047,G06N3/088;STORAGE MEDIUM, OPTIMUM SOLUTION ACQUISITION METHOD AND INFORMATION PROCESSING APPARATUS;"A non-transitory computer-readable storage medium storing a program that causes a computer to execute a process, the process includes obtaining a machine learning model having learned characteristic amounts of a plurality of training data including an objective function; calculating similarities between the characteristic amounts of the plurality of training data by inputting the plurality of training data to the obtained machine learning model; specifying a data group having a high similarity with a desired objective function from the characteristic amounts of the plurality of training data based on distances of the calculated similarities; and acquiring an optimum solution for the desired objective function by using the specified data group.";0
982;2021097644;2019;;Microsoft Technology Licensing, LLC;Microsoft Technology Licensing, LLC;G06V10/82,G06T5/60,G06T2207/30168,G06T5/73,G06T2207/10016,G06T2207/30201,G06V40/193,G06V40/161,G06V40/18,G06T7/0002,G06T2207/20081,G06T5/77,G06T3/20,G06V10/454,G06T2207/20084;GAZE ADJUSTMENT AND ENHANCEMENT FOR EYE IMAGES;A method for image enhancement on a computing device includes receiving a digital input image depicting a human eye. From the digital input image, the computing device generates a gaze-adjusted image via a gaze adjustment machine learning model by changing an apparent gaze direction of the human eye. From the gaze-adjusted image and potentially in conjunction with the digital input image, the computing device generates a detail-enhanced image via a detail enhancement machine learning model by adding or modifying details. The computing device outputs the detail-enhanced image.;3
983;2021103645;2019;;UiPath, Inc.;UiPath, Inc.;G06F21/32,G06V40/166,G05B2219/24162,G06V10/764,G06V40/172,G06V10/82,G06F9/45512,G05B19/042;FACIAL RECOGNITION FRAMEWORK USING DEEP LEARNING FOR ATTENDED ROBOTS;A facial recognition framework may be configured for robotic process automation (RPA) to automate a workflow for an application interface. A set of images of a user may be captured after the robot is initiated for the automated workflow. The set of images may be utilized by a deep learning neural network model to identify facial characteristics. The automated workflow may be performed by an attended robot based on successful validation of the user with the facial characteristics and credentials of the user for the attended robot.;0
984;2021103834;2020;;TATA CONSULTANCY SERVICES LIMITED;TATA CONSULTANCY SERVICES LIMITED;G06F8/35,G06N20/00,G06F11/0793,G06F11/079,G06N5/04;METHOD AND SYSTEM FOR GENERATING MODEL DRIVEN APPLICATIONS USING ARTIFICIAL INTELLIGENCE;This disclosure relates generally to a system and method to generate model driven applications using an artificial intelligence. The system and method uses the artificial intelligence in the model driven framework to bring productivity benefits in the model driven application development. It comprises an easy to use user interface to share context-specific questions to a user and to capture their responses. The system performs its user interactions based on the user's responses and the output from the recommendation module which are based on application models, user interaction history and the system usage pattern. The system interprets user inputs as one or more actions to be executed with the help of usage patterns in the model database and executes them and also performs error identification and recovery when required. The results of execution or error recovery along with recommendations for subsequent actions are communicated back to the user through recommendation module.;0
985;2021110158;2019;;LG ELECTRONICS INC.;LG ELECTRONICS INC.;G06T11/00,G06V20/20,G06N3/08,G06T7/73,G06Q10/087,G06V20/10,G06N20/00,G06T2207/20084,G06N3/047,G06V20/52,G06N3/045,G06T7/90,G06N3/088,G06T2207/30244;METHOD AND APPARATUS FOR ESTIMATING LOCATION IN A STORE BASED ON RECOGNITION OF PRODUCT IN IMAGE;A method of estimating an indoor location includes loading an image captured by a first terminal, recognizing a product by applying a first machine learning model based on machine learning to the loaded image, acquiring product information related to the product from the recognized product, estimating a location of the first terminal based on a database including location information of the product and the product information, and controlling the first terminal to display information related to the location on the first terminal. A neural network for processing an image is a deep neural network generated through machine learning, and the image is inputted and outputted in an Internet of things (IoT) environment using a 5G network.;0
986;2021110207;2019;;UiPath, Inc.;UiPath, Inc.;G06F18/2148,G06N5/043,G06N3/08,G06N3/006,G06N20/20,G06F18/25,G06N7/01,G06F18/2178,G06N20/00,G06V10/7784,G06N3/047;AUTOMATIC ACTIVATION AND CONFIGURATION OF ROBOTIC PROCESS AUTOMATION WORKFLOWS USING MACHINE LEARNING;Automatic activation and configuration of robotic process automation (RPA) workflows using machine learning (ML) is disclosed. One or more parts of an RPA workflow may be turned on or off based on one or more probabilistic ML models. RPA robots may be configured to modify parameters, determine how much of a certain resource to provide, determine more optimal thresholds, etc. Such RPA workflows implementing ML may thus be hybrids of both deterministic and probabilistic logic, and may learn and improve over time by retraining the ML models, adjusting the confidence thresholds, using local/global confidence thresholds, providing or adjusting modifiers for the local confidence thresholds, implement a supervisor system that monitors ML model performance, etc.;0
987;2021110318;2019;;UiPath, Inc.;UiPath, Inc.;B25J9/163,G06Q10/0633,G06Q10/04,G06Q10/06393,G06N5/04,G06Q10/10;AUTOMATIC ANALYSIS, PRIORITIZATION, AND ROBOT GENERATION FOR ROBOTIC PROCESS AUTOMATION;Systems and methods for analyzing, prioritizing, and potentially automatically generating robots implementing processes and/or process flows for robotic process automation (RPA) are disclosed. Artificial intelligence (AI) may be used to analyze business processes and/or process flows and look for possible candidates for automation or improvement of existing automations. Listeners (e.g., robots, separate software applications, operating system extensions, etc.) may be employed to listen in the background on user computing systems to mine data pertaining to workflow effectiveness and/or to identify new processes and/or process flows that may improve return on investment (ROI) for RPA. For example, when automations are placed into production via robots implementing RPA workflows on user computing systems, listeners may be added to ensure that the process(es) and/or process flow(s) are correctly and accurately performing what they are intended for and/or provide data for automation of new processes and/or process flows.;1
989;2021117733;2018;;NEC CORPORATION;NEC CORPORATION;G06N3/088,G06F18/2185,G06N20/10,G06F18/2193,G06N3/0475,G06F18/2415,G06F18/2411,G10L17/04,G06N3/047,G10L17/18,G06F18/24133,G10L17/20,G06F18/21343,G06F18/10,G06N3/094,G06F18/2132,G06N3/045;PATTERN RECOGNITION APPARATUS, PATTERN RECOGNITION METHOD, AND COMPUTER-READABLE RECORDING MEDIUM;An apparatus for pattern recognition includes a generator which transforms noisy feature vectors into denoised feature vectors, a discriminator which takes the denoised feature vectors and the original clean feature vectors corresponding to the denoised feature vectors as input and predicts probability for both of the input features of being an original clean feature, classifies the input feature vectors into its corresponding classes, an objective function calculator which calculates generator and discriminator losses using the denoised feature vectors, the clean feature vectors from which the noisy feature vectors have been made, the estimated classes and their true classes, and a Parameter updater which updates parameters of the generator and the discriminator according to loss minimization.;1
991;2021117842;2020;;Unlearn.AI, Inc.;Unlearn.AI, Inc.;G06N3/045,G06N5/046,G06N20/00,G06N7/01,G06N3/047,G06F18/24133,G06N3/088,G06F18/211,G06N7/08,G06N3/044;Systems and Methods for Training Generative Models Using Summary Statistics and Other Constraints;Systems and methods for training and utilizing constrained generative models in accordance with embodiments of the invention are illustrated. One embodiment includes a method for training a constrained generative model. The method includes steps for receiving a set of data samples from a first distribution, identifying a set of constraints from a second distribution, and training a generative model based on the set of data samples and the set of constraints.;1
992;2021117893;2020;;Affle International Pte. Ltd.;Affle International Pte. Ltd.;G06Q50/205,G06Q40/03,G06Q30/0201,G06Q30/016,G16H40/20,G06Q30/0633,G06Q10/06395,G06N5/022,H04L51/222,G06F40/30,G06N20/00,G06Q50/16,H04L51/02,G06Q40/06,G06N5/01,G06Q20/386,G06Q30/0609,G06Q50/14,G06N3/006,G16H10/20,G06Q20/127,G06N5/043,G06F40/237,G06Q30/0613,G06Q20/123,G06Q40/08,G06Q10/06315,G06F16/3329,G06F16/245;METHOD AND SYSTEM FOR SWITCHING AND HANDOVER BETWEEN ONE OR MORE INTELLIGENT CONVERSATIONAL AGENTS;The present disclosure provides a method and system to perform switching and handover between one or more intelligent conversational agents. The system receives a first set of data in real-time. The system collects a second set of data in real-time. The system fetches one or more queries from a plurality of users for a mega bot. The system analyses the first set of data, the second set of data and the one or more queries using one or more machine learning algorithms. The system selects a suitable intelligent conversational agent from the one or more intelligent conversational agents having a trust score above a threshold level. The system switches between the one or more intelligent conversational agents in the mega bot interacting with the plurality of users based on a plurality of aspects of corresponding query of the one or more queries and a plurality of factors.;1
998;2021125614;2019;;NEC CORPORATION;NEC CORPORATION;G10L15/063,G10L15/22,G10L15/30,G06Q30/0613,G06V40/10,G06Q30/06,G10L15/08,G06V20/52,G10L15/24,G10L15/1822,G10L2015/088;INFORMATION PROCESSING SYSTEM, INFORMATION PROCESSING METHOD, AND STORAGE MEDIUM;"Provided is an information processing system including: a voice information acquisition unit that acquires voice information including an utterance made by a person; a status acquisition unit that acquires status information related to status of the person; and a support information generation unit that generates support information used for supporting operation of the person based on the voice information and the status information.";1
999;2021125691;2020;;Molecule One Sp. z o.o.;Molecule One Sp. z o.o.;G16C20/10,G06N20/00,G16C20/70,G05B2219/32287,G05B19/4155;SYSTEMS AND METHOD FOR DESIGNING ORGANIC SYNTHESIS PATHWAYS FOR DESIRED ORGANIC MOLECULES;Methods and systems provide proposed pathways for synthesizing chemical reactions given a user-proposed target molecule, user-provided reaction constraints, or a combination of both. Embodiments may leverage training the model using both known successful reactions and infeasible reactions, either known or created by a prior use of the model. Chemical reactions for producing the target molecule and substrates are proposed using the model. From the proposed reactions, synthesis pathways are extracted and ranked according to a cost estimation. The ranked synthesis pathways are then provided to the user.;3
1000;2021129325;2019;;Automation Anywhere, Inc.;Automation Anywhere, Inc.;G06F8/38,G06F9/451,G05B19/4155,G06F9/44526,B25J13/00,G06F8/34,B25J9/1661,G05B2219/34348;PRODUCTIVITY PLUGIN FOR INTEGRATION WITH ROBOTIC PROCESS AUTOMATION;Robotic process automation (RPA) tasks for operating on data within a productivity program may be initiated by single user action from within the productivity program. A user device is provided with a plugin program that provides an automation user interface within the productivity program. A request to perform an automation task on data in a productivity file is received along with identification of a software robot to perform the automation task. The request also specifies data from the productivity file. The request is provided to a control room server that controls execution of a plurality of different automation tasks by a plurality of different software robots. Results of the automation task are received from the control room server and are provided to the plugin program, which provides the results of the automation task to the user within a productivity file accessible by the user.;0