###############################################
# Import necessary libraries #
###############################################
# These lines import the various tools and libraries that the script will use.
import \
    pandas as pd  # A powerful library for data manipulation and analysis, especially for working with tables of data.
import \
    numpy as np  # A library that supports large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.
import \
    os  # Provides functions for interacting with the operating system, such as reading or writing files and creating directories.
import \
    matplotlib.pyplot as plt  # A plotting library used for creating static, animated, and interactive visualizations.
import \
    seaborn as sns  # A library built on top of matplotlib that provides a higher-level interface for drawing attractive statistical graphics.
from sklearn.model_selection import \
    StratifiedKFold  # A tool for splitting data into training and testing sets while maintaining the percentage of samples for each class.
from sklearn.metrics import (
    accuracy_score,  # Measures the proportion of correctly classified instances.
    roc_auc_score,  # Evaluates the model's ability to distinguish between classes.
    precision_score,  # Measures the accuracy of positive predictions.
    recall_score,  # Measures the ability of the model to find all the relevant cases (true positives).
    f1_score,  # Combines precision and recall into a single metric.
    confusion_matrix,  # Provides a matrix showing the true vs. predicted classifications.
)
from sklearn.svm import \
    SVC  # Support Vector Classifier, a type of machine learning model used for classification tasks.
from sklearn.naive_bayes import \
    MultinomialNB  # A Naive Bayes classifier for multinomially distributed data, often used for text classification.
from sklearn.linear_model import LogisticRegression  # A linear model for classification tasks.
from sklearn.ensemble import RandomForestClassifier, \
    VotingClassifier  # Ensemble models: Random Forest for classification and VotingClassifier for combining multiple models.
from sklearn.feature_extraction.text import \
    TfidfVectorizer  # Converts a collection of raw documents to a matrix of TF-IDF features.
from tqdm import \
    tqdm  # Provides a progress bar for loops, making it easier to track the progress of long-running operations.
from nltk.tokenize import word_tokenize  # Splits sentences into words (tokens) using Natural Language Toolkit (NLTK).
from spacy.lang.en import STOP_WORDS  # A list of common English stop words (e.g., 'the', 'and') provided by spaCy.
import warnings  # Used to control warning messages generated by the code.

# Warnungen deaktivieren
warnings.filterwarnings("ignore")  # This line tells Python to ignore all warning messages, keeping the output clean.


# Define the PatentClassifier class
class PatentClassifier:
    def __init__(self, data_path, delimiter):
        """
        Initialize the PatentClassifier with data path and delimiter.

        Parameters:
        - data_path: Path to the training data CSV file.
        - delimiter: Character used to separate values in the CSV file (e.g., ',', ';').
        """
        self.data_path = data_path  # Stores the path to the main training data.
        self.delimiter = delimiter  # Stores the delimiter used in the CSV file.
        self.data = None  # Placeholder for the combined dataset.
        self.IDs = None  # Placeholder for patent IDs.
        self.texts = None  # Placeholder for patent abstracts.
        self.labels = None  # Placeholder for labels indicating GenAI patents (1) or not (0).
        self.total_entries = None  # Placeholder for the total number of patents.
        self.total_ratio = None  # Placeholder for the ratio of GenAI to non-GenAI patents.
        self.results = []  # List to store performance results of each model.
        self.classified_values = []  # List to store individual classification results (actual vs. predicted).
        self.classified_values_p = []  # List to store predicted probabilities, if available.
        self.feature_names = None  # Placeholder for the names of the features extracted from the text.
        self.vectorizer = None  # Placeholder for the TF-IDF vectorizer.
        self.text_vectors = None  # Placeholder for the numerical representation of the patent abstracts.
        self.results_table = None  # Placeholder for the table of results.
        self.metrics = ["Accuracy", "AUC", "Precision", "Recall", "F1"]  # List of performance metrics to evaluate.

    def load_data(self):
        """
        Load and preprocess the training data from a CSV file.

        This function reads the CSV file containing patent data, processes it by creating a new label column
        indicating whether each patent is related to Generative AI (GenAI), and stores the processed data.
        """
        # Load CSV file with patent data
        self.data = pd.read_csv(self.data_path, delimiter=self.delimiter)  # Reads the CSV file into a pandas DataFrame.

        # Categorize: if 'Finales Resultat' >= 1, set 'label_genai' to 1, else 0
        self.data["label_genai"] = (self.data["Finales Resultat"] >= 1).astype(
            int)  # Creates a new column 'label_genai' based on the 'Finales Resultat' column.

    def load_anti_seed(self, n):
        """
        Load and integrate anti-seed data to balance the dataset.

        Parameters:
        - n: Number of anti-seed entries to include in the dataset.

        This function reads additional data containing non-GenAI patents to balance the dataset, ensuring
        that there are enough non-GenAI examples for the model to learn effectively.
        """
        # Load AI patents to create an anti-seed set
        anti_seed_path = os.path.join(".", "Training Data",
                                      "4K Patents - AI 20p.csv")  # Defines the path to the anti-seed data CSV.
        anti_seed = pd.read_csv(anti_seed_path, delimiter=",")  # Reads the anti-seed CSV file into a pandas DataFrame.

        # Filter patents without AI content (actual == 0)
        anti_seed = anti_seed[anti_seed["actual"] == 0]  # Filters the DataFrame to include only non-GenAI patents.

        # Limit anti-seed dataset to n patents
        anti_seed = anti_seed[:n]  # Selects the first 'n' entries from the anti-seed DataFrame.

        # Adjust columns in the anti-seed set to combine with training data
        anti_seed["label_genai"] = 0  # Sets the 'label_genai' column to 0 for all anti-seed patents (non-GenAI).

        # Unify column names to match the training data
        anti_seed = anti_seed.rename(
            columns={"app number": "patent_id", "abstract": "patent_abstract"}
        )  # Renames columns to ensure consistency with the main training data.

        # Select relevant columns for merging
        anti_seed = anti_seed[["patent_id", "patent_abstract", "label_genai"]]  # Keeps only the necessary columns.

        # Combine training data with anti-seed data
        self.data = pd.concat(
            [
                self.data[["patent_id", "patent_abstract", "label_genai"]],
                # Selects relevant columns from the main data.
                anti_seed,  # Includes the anti-seed data.
            ],
            ignore_index=True,  # Resets the index in the combined DataFrame.
        )  # Merges the main data with the anti-seed data to create a balanced dataset.

    def preprocess_data(self):
        """
        Prepare data for text classification and vectorization.

        This function extracts the necessary information from the combined dataset, calculates the distribution
        of labels, and transforms the text data into numerical vectors using TF-IDF vectorization.
        """
        # Save data into lists for text classification
        self.IDs = self.data["patent_id"].values  # Extracts patent IDs as a NumPy array.
        self.texts = self.data["patent_abstract"].tolist()  # Extracts patent abstracts as a list.
        self.labels = self.data["label_genai"].tolist()  # Extracts labels (1 or 0) as a list.

        # Calculate the distribution of 'label_genai'
        label_counts = self.data["label_genai"].value_counts(
            normalize=True) * 100  # Calculates the percentage of each label.
        self.total_entries = len(self.data)  # Counts the total number of patents.
        self.total_ratio = f"{label_counts.get(1, 0):.0f}-{label_counts.get(0, 0):.0f}"  # Formats the ratio as "GenAI%-Non-GenAI%".

        # Output the distribution and total count
        print(f"Share of entries with '1': {label_counts.get(1, 0):.2f}%")  # Prints the percentage of GenAI patents.
        print(
            f"Share of entries with '0': {label_counts.get(0, 0):.2f}%")  # Prints the percentage of non-GenAI patents.
        print(f"1/0 Ratio: {self.total_ratio}")  # Prints the ratio of GenAI to non-GenAI patents.
        print(f"Total number of entries: {self.total_entries}")  # Prints the total number of patents.

        # Define model parameters for TF-IDF vectorization
        MINDF = 10  # Minimum number of documents a word must appear in to be included.
        MAXDF = 0.8  # Maximum proportion of documents a word can appear in to be included.
        MAX_FEATURES = 1200  # Maximum number of features (words) to consider.
        NGRAM_RANGE = (1, 2)  # Range of n-grams to include (e.g., single words and pairs of words).

        # Load stop words to ignore during vectorization
        stop_words = list(STOP_WORDS)  # Retrieves a list of common English stop words from spaCy.

        # Define tokenizer function to split text into words and remove non-alphanumeric tokens
        def simple_tokenizer(text):
            tokens = word_tokenize(text.lower())  # Converts text to lowercase and splits into words.
            return [token for token in tokens if token.isalnum()]  # Keeps only alphanumeric tokens.

        # Define TF-IDF vectorizer to convert text data into numerical vectors
        self.vectorizer = TfidfVectorizer(
            max_features=MAX_FEATURES,  # Limits the number of features to the top 'MAX_FEATURES'.
            max_df=MAXDF,  # Ignores words that appear in more than 'MAXDF' proportion of documents.
            min_df=MINDF,  # Ignores words that appear in fewer than 'MINDF' documents.
            stop_words=stop_words,  # Removes common stop words.
            ngram_range=NGRAM_RANGE,  # Considers both single words and pairs of words.
            tokenizer=simple_tokenizer,  # Uses the custom tokenizer defined above.
        )

        # Transform text data into TF-IDF matrix and extract feature names
        self.text_vectors = self.vectorizer.fit_transform(
            self.texts)  # Converts the list of abstracts into a numerical matrix.
        self.feature_names = self.vectorizer.get_feature_names_out()  # Retrieves the names of the features (words) used in the vectorization.

        # Output the first few feature names for verification
        print(f"First features: {self.feature_names[:10]}")  # Prints the first 10 feature names.
        print(
            f"Number of extracted features: {len(self.feature_names)}")  # Prints the total number of features extracted.

    def train_and_evaluate(self):
        """
        Define classifiers, perform cross-validation, and evaluate models.

        This function sets up different machine learning models, trains them using cross-validation,
        evaluates their performance using various metrics, and stores the results.
        """
        # Define base classifiers for the ensemble (with short names)
        ensemble_estimators = [
            ("svc", SVC(probability=True)),  # Support Vector Classifier with probability estimates.
            ("nb", MultinomialNB()),  # Multinomial Naive Bayes classifier.
            ("lr", LogisticRegression()),  # Logistic Regression classifier.
            ("rf", RandomForestClassifier(n_estimators=1000)),  # Random Forest classifier with 1000 trees.
        ]

        # Define the ensemble classifier using VotingClassifier, which combines multiple models
        ensemble_classifier = VotingClassifier(
            estimators=ensemble_estimators,  # List of base classifiers to include in the ensemble.
            voting="soft"  # Uses the average predicted probabilities for voting.
        )

        # Define classifiers to test individually and as an ensemble
        CLASSIFIERS = [
            ("Support Vector Classifier (RBF)", SVC(probability=True)),  # SVC with Radial Basis Function kernel.
            ("Naive Bayes", MultinomialNB()),  # Multinomial Naive Bayes classifier.
            ("Logistic Regression", LogisticRegression()),  # Logistic Regression classifier.
            ("Random Forest", RandomForestClassifier(n_estimators=1000)),  # Random Forest classifier with 1000 trees.
            ("Ensemble", ensemble_classifier),  # Ensemble classifier combining the above models.
        ]

        NUM_OF_SPLITS = 5  # Number of folds for cross-validation.
        Reweight = True  # Option to manually adjust class distribution during training.

        # Convert labels to a NumPy array for efficient processing
        Y = np.array(self.labels)
        IDs = np.array(self.IDs)
        Abstract_Vectors = self.text_vectors  # The numerical representation of patent abstracts.

        # Iterate over each classifier to train and evaluate
        for CL in tqdm(CLASSIFIERS, desc="Evaluating Classifiers"):
            name, Model = CL  # Unpack the classifier name and the model instance.

            y_actual, y_predicted, y_predicted_p = [], [], []  # Lists to store actual labels, predicted labels, and predicted probabilities.
            id_s = []  # List to store patent IDs for the current fold.

            # Stratified K-Fold ensures that each fold has the same proportion of classes as the entire dataset
            KFoldSplitter = StratifiedKFold(
                n_splits=NUM_OF_SPLITS,  # Number of folds.
                shuffle=True,  # Shuffles the data before splitting.
                random_state=1  # Ensures reproducibility.
            )

            # Perform cross-validation
            for train_i, test_i in tqdm(
                    KFoldSplitter.split(Abstract_Vectors, Y),
                    desc="Cross-Validating",
                    leave=True,
                    total=NUM_OF_SPLITS,
            ):

                # Split data into training and test sets based on current fold
                train_X, test_X = Abstract_Vectors[train_i], Abstract_Vectors[test_i]
                train_y, test_y = Y[train_i], Y[test_i]
                Train_IDs, Test_IDs = IDs[train_i], IDs[test_i]

                # Balance the training data if reweighting is enabled
                temp_y = list(train_y)  # Create a temporary list of training labels.
                temp_X = train_X.todense().tolist()  # Convert the sparse matrix to a dense list of lists.

                if Reweight:
                    # Repeat balancing up to three times to achieve approximately equal class proportions
                    for _ in range(3):
                        for i in range(len(train_y)):
                            if (train_y[i] != 0) and (np.mean(temp_y) < 0.5):
                                temp_y.append(train_y[i])  # Add a GenAI label.
                                temp_X.append(temp_X[i])  # Add the corresponding text vector.

                # Train the model with the (possibly balanced) training data
                Model.fit(temp_X, temp_y)  # Fits the model to the training data.

                # Make predictions on the test data
                y_pred = Model.predict(np.asarray(test_X.todense()))  # Predicts labels for the test set.

                # Convert predictions to binary values (0 or 1)
                y_pred = [1 if y > 0.5 else 0 for y in y_pred]

                # Save actual and predicted values and IDs
                y_actual += list(test_y)  # Append actual labels.
                y_predicted += y_pred  # Append predicted labels.
                id_s += list(Test_IDs)  # Append patent IDs.

                # Save predicted probabilities if available
                try:
                    y_pred_p = Model.predict_proba(
                        np.asarray(test_X.todense()))  # Predicts probabilities for each class.
                    y_pred_p = [y[1] for y in y_pred_p]  # Extracts the probability of the positive class (GenAI).
                    y_predicted_p += y_pred_p  # Appends predicted probabilities.
                except AttributeError:
                    pass  # If the classifier does not support probability predictions, skip this step.

            # After all folds are processed, evaluate the overall performance
            Share = np.round(np.mean(y_predicted), 3)  # Calculates the average share of patents classified as GenAI.

            # Calculate key performance metrics
            Accuracy = accuracy_score(y_actual, y_predicted)  # Overall accuracy of the model.
            ROC = roc_auc_score(y_actual, y_predicted)  # Ability of the model to distinguish between classes.
            Precision = precision_score(y_actual, y_predicted)  # Accuracy of positive predictions.
            Recall = recall_score(y_actual, y_predicted)  # Ability to find all positive instances.
            F1 = f1_score(y_actual, y_predicted)  # Harmonic mean of precision and recall.

            # Create confusion matrix to analyze types of errors
            CM = confusion_matrix(y_actual, y_predicted)  # Matrix showing true vs. predicted classifications.

            # Calculate proportions of error types (rounded to 3 decimal places)
            try:
                FN = np.round(CM[0][0] / (CM[0][0] + CM[1][0]), 3)  # False Negatives proportion.
                FP = np.round(CM[0][1] / (CM[0][1] + CM[1][1]), 3)  # False Positives proportion.
                TN = np.round(CM[1][0] / (CM[0][0] + CM[1][0]), 3)  # True Negatives proportion.
                TP = np.round(CM[1][1] / (CM[0][1] + CM[1][1]), 3)  # True Positives proportion.
            except ZeroDivisionError:
                FN = FP = TN = TP = None  # If division by zero occurs, set all to None.

            # Add metrics to the results list
            self.results.append(
                [
                    name,  # Name of the classifier.
                    Share,  # Share of predictions classified as GenAI.
                    TP,  # True Positives.
                    FN,  # False Negatives.
                    FP,  # False Positives.
                    TN,  # True Negatives.
                    np.round(Accuracy, 3),  # Rounded Accuracy.
                    np.round(ROC, 3),  # Rounded ROC AUC.
                    np.round(Precision, 3),  # Rounded Precision.
                    np.round(Recall, 3),  # Rounded Recall.
                    np.round(F1, 3),  # Rounded F1 Score.
                ]
            )

            # Save classification results (actual vs. predicted) for each model
            self.classified_values.append(
                list(zip(len(id_s) * [name], id_s, y_actual, y_predicted))
            )

            # Save predicted probabilities if available
            if y_predicted_p:
                self.classified_values_p.append(
                    list(zip(len(id_s) * [name], id_s, y_actual, y_predicted_p))
                )

        # After evaluating all classifiers, create a table of results
        # Convert the list of model performance metrics into a DataFrame
        self.results_table = pd.DataFrame(
            self.results,
            columns=[
                "Name",  # Name of the classifier.
                "Share",  # Share of predictions classified as GenAI.
                "True-Positives",  # Correctly predicted GenAI patents.
                "False-Negatives",  # GenAI patents that were not identified.
                "False-Positives",  # Non-GenAI patents incorrectly identified as GenAI.
                "True-Negatives",  # Correctly identified non-GenAI patents.
                "Accuracy",  # Overall accuracy of the model.
                "AUC",  # Area Under the ROC Curve.
                "Precision",  # Accuracy of positive predictions.
                "Recall",  # Ability to find all positive instances.
                "F1",  # Harmonic mean of precision and recall.
            ],
        )

        # Add a column indicating the type of feature extraction used
        self.results_table["Type"] = "Bag of Words"  # Specifies that the Bag of Words technique was used.

        # Rearrange columns for better readability
        self.results_table = self.results_table[
            [
                "Name",
                "Type",
                "Share",
                "True-Positives",
                "False-Negatives",
                "False-Positives",
                "True-Negatives",
                "Accuracy",
                "AUC",
                "Precision",
                "Recall",
                "F1",
            ]
        ]

        # Sort the table by accuracy in descending order and save the results as a CSV file
        self.results_table.sort_values("Accuracy", ascending=False).to_csv(
            f"./Output/GenAI_BOW_Model_Classification_Performance_{self.total_entries}_{self.total_ratio}.csv",
            index=False,
        )
        print(self.results_table.sort_values("Accuracy", ascending=False))  # Print the sorted results table.

        # Output the prediction results for the training dataset

        # Create a DataFrame for each classification result and merge them
        for i, classified_values in enumerate(self.classified_values):
            Temp = pd.DataFrame(
                classified_values, columns=["Model", "id", "Actual", "Predicted"]
            )

            # Rename columns based on the model name
            if i == 0:
                name = Temp["Model"].iloc[0]  # Get the model name from the first row.
                Temp = Temp[["id", "Actual", "Predicted"]]  # Select relevant columns.
                Temp.columns = ["id", "Actual", name]  # Rename columns to include the model name.
                Final = Temp  # Initialize the Final DataFrame.
            else:
                name = Temp["Model"].iloc[0]  # Get the model name from the first row.
                Temp = Temp[["id", "Predicted"]]  # Select relevant columns.
                Temp.columns = ["id", name]  # Rename columns to include the model name.
                Final = Final.merge(Temp, on="id")  # Merge with the Final DataFrame based on 'id'.

        # Save the merged classification results as a CSV file
        Final.to_csv(
            f"./Output/GenAI_BOW_Classification_Results_{self.total_entries}_{self.total_ratio}.csv",
            index=False,
        )

        # Output the predicted probabilities if available
        if self.classified_values_p:
            # Create a DataFrame for the probabilities and merge them
            for i, classified_values_p in enumerate(self.classified_values_p):
                Temp = pd.DataFrame(
                    classified_values_p, columns=["Model", "id", "Actual", "Predicted"]
                )

                if i == 0:
                    name = Temp["Model"].iloc[0]  # Get the model name from the first row.
                    Temp = Temp[["id", "Actual", "Predicted"]]  # Select relevant columns.
                    Temp.columns = ["id", "Actual", name]  # Rename columns to include the model name.
                    Final = Temp  # Initialize the Final DataFrame.
                else:
                    name = Temp["Model"].iloc[0]  # Get the model name from the first row.
                    Temp = Temp[["id", "Predicted"]]  # Select relevant columns.
                    Temp.columns = ["id", name]  # Rename columns to include the model name.
                    Final = Final.merge(Temp, on="id")  # Merge with the Final DataFrame based on 'id'.

            # Save the probabilities as a CSV file
            Final.to_csv(
                f"./Output/GenAI_BOW_Predicted_Probabilities_{self.total_entries}_{self.total_ratio}.csv",
                index=False,
            )

    # The following plotting functions are included within the class for simplicity
    def plot_all_metrics(self):
        """
        Creates and saves a grouped bar chart for all metrics,
        displaying individual metric values on the bars.

        Visualizes the performance metrics of the model.
        """
        df_plot = self.results_table[["Name"] + self.metrics]  # Selects the 'Name' and metric columns for plotting.

        num_models = len(df_plot["Name"])  # Counts the number of models.
        num_metrics = len(self.metrics)  # Counts the number of metrics.

        ind = np.arange(num_models)  # The x locations for the groups.
        total_width = 0.8  # Total width for all bars in a group.
        width = total_width / num_metrics  # Width of each individual bar.

        fig, ax = plt.subplots(figsize=(12, 6))  # Creates a figure and a set of subplots with a specified size.

        for i, metric in enumerate(self.metrics):
            # Positioning of bars within each group.
            offset = (i - num_metrics / 2) * width + width / 2
            positions = ind + offset
            bars = ax.bar(positions, df_plot[metric], width, label=metric)  # Creates a bar for each metric.

            # Adding metric values on the bars for clarity.
            for bar in bars:
                height = bar.get_height()
                ax.annotate(
                    f"{height:.2f}",  # Formats the height to two decimal places.
                    xy=(bar.get_x() + bar.get_width() / 2, height),  # Positions the text at the top center of the bar.
                    xytext=(0, 3),  # Offsets the text slightly above the bar.
                    textcoords="offset points",  # Uses offset points for positioning.
                    ha="center",  # Horizontally centers the text.
                    va="bottom",  # Vertically aligns the text to the bottom of the specified position.
                    fontsize=8,  # Sets the font size of the text.
                )

        # Set the labels and title of the chart.
        ax.set_xlabel("Model Name")  # Labels the x-axis.
        ax.set_ylabel("Scores")  # Labels the y-axis.
        ax.set_title(
            f"Comparison Bag of Words Model Performance ({self.total_entries} patents, ratio: {self.total_ratio})"
        )  # Sets the title of the chart, including the number of patents and the ratio.
        ax.set_xticks(ind)  # Sets the positions of the x-axis ticks.
        ax.set_xticklabels(df_plot["Name"], rotation=45,
                           ha="right")  # Labels the x-axis ticks with model names, rotated for better readability.
        ax.legend()  # Adds a legend to differentiate the metrics.

        plt.tight_layout()  # Adjusts the padding between and around subplots to minimize overlap.
        plt.savefig(
            f"Figures/bow_model_performance_grouped_bar_{self.total_entries}_{self.total_ratio}.png",
            dpi=300,  # Sets the resolution of the saved figure.
        )  # Saves the figure as a high-resolution PNG file.
        plt.show()  # Displays the plot.

# -------------------------------------------------------------
# Main Execution
# -------------------------------------------------------------

# The following code runs when the script is executed directly.
# It sets up the environment, initializes the classifier, loads and preprocesses data,
# trains and evaluates the models, and creates plots of the results.
# To run the code, execute the following command in the terminal:
# python class_bow_ensemble_genai.py

if __name__ == "__main__":

    # Ensure that the 'Figures' directory exists; create it if it doesn't.
    if not os.path.exists("Figures"):
        os.makedirs("Figures")  # Creates the 'Figures' directory to store plots.

    # Ensure that the 'Output' directory exists; create it if it doesn't.
    if not os.path.exists("Output"):
        os.makedirs("Output")  # Creates the 'Output' directory to store result CSV files.

    # Dataset parameters
    data_path = os.path.join(
        ".", "Training Data", "20240819_WIPO Patents GenAI US matched_1-1000.csv"
    )  # Defines the path to the main training data CSV file.
    delimiter = ";"  # Specifies that the CSV file uses a semicolon to separate values.

    # Initialize the PatentClassifier with the data path and delimiter
    classifier = PatentClassifier(data_path, delimiter)

    # Load the main training data
    classifier.load_data()  # Reads and processes the main training data.

    # Add non-GenAI patents (anti-seed) to balance the dataset
    n = 3146  # Number of non-GenAI patents to include from the anti-seed data.
    classifier.load_anti_seed(n)  # Loads and integrates the anti-seed data.

    # Preprocess data for classification and vectorization
    classifier.preprocess_data()  # Prepares the data by extracting features and converting text to numerical vectors.

    # Train and evaluate models
    classifier.train_and_evaluate()  # Trains each classifier using cross-validation and evaluates their performance.

    # Create and save plots of the performance metrics
    classifier.plot_all_metrics()  # Generates a bar chart comparing the performance of all classifiers.