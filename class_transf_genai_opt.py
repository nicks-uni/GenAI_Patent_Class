# -------------------------------------------------------------
# Import Required Packages
# -------------------------------------------------------------
# These lines import necessary modules and libraries that the script will use.

from collections import Counter  # Helps count occurrences of elements in a list.
import os  # Provides a way to interact with the operating system, such as creating directories.
import logging  # Facilitates logging messages to track the execution of the script.
import warnings  # Manages warning messages generated by the code.
import gc  # Provides access to garbage collection for memory management.

# Import necessary libraries for data manipulation and modeling
import pandas as pd  # Used for data manipulation and analysis, especially with tabular data.
import numpy as np  # Supports large, multi-dimensional arrays and matrices, along with mathematical functions.
from tqdm import tqdm  # Displays progress bars for loops, making it easier to track long-running processes.
from simpletransformers.classification import ClassificationModel  # Provides an easy interface to train and use transformer-based classification models.

# Import scikit-learn utilities for model evaluation
from sklearn.model_selection import StratifiedKFold  # Splits data into training and testing sets while preserving the percentage of samples for each class.
from sklearn.metrics import (
    accuracy_score,  # Measures the proportion of correctly classified instances.
    roc_auc_score,  # Evaluates the model's ability to distinguish between classes.
    precision_score,  # Measures the accuracy of positive predictions.
    recall_score,  # Measures the ability of the model to find all the relevant cases.
    f1_score,  # Combines precision and recall into a single metric.
    confusion_matrix  # Provides a matrix showing true vs. predicted classifications.
)

# Import plotting libraries
import matplotlib.pyplot as plt  # Used for creating static, animated, and interactive visualizations.
import seaborn as sns  # Builds on matplotlib to provide a higher-level interface for drawing attractive statistical graphics.

# Suppress warnings and set logging level to ERROR to reduce output clutter
warnings.filterwarnings("ignore")  # Ignores warning messages to keep the output clean.
logging.basicConfig(level=logging.ERROR)  # Sets the logging level to only show errors, ignoring warnings and informational messages.
transformers_logger = logging.getLogger("transformers")  # Gets the logger for the transformers library.
transformers_logger.setLevel(logging.ERROR)  # Sets the transformers logger to only show errors.

# Check if CUDA is available (for GPU acceleration)
import torch  # A deep learning library that provides GPU acceleration.
use_cuda = torch.cuda.is_available()  # Checks if a CUDA-compatible GPU is available for faster computations.

# -------------------------------------------------------------
# Define the PatentTransformerClassifier Class
# -------------------------------------------------------------
class PatentTransformerClassifier:
    def __init__(self, data_path, anti_seed_path, delimiter):
        """
        Initialize the classifier with paths to the data and configuration.
        """
        self.data_path = data_path  # Path to the main dataset containing GenAI patents.
        self.anti_seed_path = anti_seed_path  # Path to the anti-seed dataset used to balance the main dataset.
        self.delimiter = delimiter  # Delimiter used in the CSV files to separate values.
        self.data = None  # Placeholder for the main dataset after loading.
        self.IDs = None  # Placeholder for patent IDs extracted from the dataset.
        self.abstract_text = None  # Placeholder for patent abstracts extracted from the dataset.
        self.classes = None  # Placeholder for class labels (GenAI or non-GenAI).
        self.total_entries = None  # Placeholder for the total number of patents in the dataset.
        self.total_ratio = None  # Placeholder for the ratio of GenAI to non-GenAI patents.
        self.results = []  # List to store the results of model evaluations.
        self.classified_values = []  # List to store the classified values for each model.
        self.results_table = None  # Placeholder for the final results table.

    def load_data(self):
        """
        Load and preprocess the training data from a CSV file.
        """
        # Load CSV file with patent data
        self.data = pd.read_csv(self.data_path, delimiter=self.delimiter)  # Reads the main dataset into a pandas DataFrame.

        # Categorize: if 'Finales Resultat' column value >= 1, set 'label_genai' to 1, else 0
        self.data['label_genai'] = (self.data['Finales Resultat'] >= 1).astype(int)  # Creates a new column 'label_genai' based on the condition.

        print('EintrÃ¤ge data:', self.data.count('rows'))  # Prints the number of entries (rows) in the dataset.

    def load_anti_seed(self, n):
        """
        Load and integrate the anti-seed data to balance the dataset.
        """
        # Load AI patents to create an anti-seed set
        anti_seed = pd.read_csv(self.anti_seed_path, delimiter=',')  # Reads the anti-seed dataset into a pandas DataFrame.

        # Filter patents without AI content (where 'actual' column is 0)
        anti_seed = anti_seed[anti_seed['actual'] == 0]  # Keeps only non-AI patents in the anti-seed dataset.

        # Limit the anti-seed dataset to n patents
        anti_seed = anti_seed[:n]  # Selects the first 'n' patents from the anti-seed dataset for optimal performance.

        # Adjust columns in the anti-seed set to combine with training data
        anti_seed['label_genai'] = 0  # Sets the 'label_genai' column to 0 for all anti-seed patents, marking them as non-GenAI.

        # Unify column names (adjust if columns are different)
        anti_seed = anti_seed.rename(columns={
            'app number': 'patent_id',  # Renames 'app number' to 'patent_id' for consistency.
            'abstract': 'patent_abstract'  # Renames 'abstract' to 'patent_abstract' for consistency.
        })

        # Select relevant columns for merging
        anti_seed = anti_seed[['patent_id', 'patent_abstract', 'label_genai']]  # Keeps only the necessary columns.

        # Combine training data with anti-seed data
        self.data = pd.concat(
            [self.data[['patent_id', 'patent_abstract', 'label_genai']], anti_seed],
            ignore_index=True  # Combines the main dataset with the anti-seed dataset into a single DataFrame.
        )

    def prepare_data(self):
        """
        Prepare data for text classification.
        """
        # Extract IDs, abstracts, and class labels
        self.IDs = np.array(self.data['patent_id'].values.tolist())  # Converts patent IDs to a NumPy array.
        self.abstract_text = self.data['patent_abstract'].values.tolist()  # Converts patent abstracts to a list.
        self.classes = self.data['label_genai'].values.tolist()  # Converts class labels to a list.

        # Calculate the distribution of 'label_genai'
        label_counts = self.data['label_genai'].value_counts(normalize=True) * 100  # Calculates the percentage of each class.
        self.total_entries = len(self.data)  # Counts the total number of patents.
        self.total_ratio = f"{label_counts.get(1, 0):.0f}-{label_counts.get(0, 0):.0f}"  # Creates a string representing the ratio of GenAI to non-GenAI patents.

        # Output the distribution and total count
        print(f"Share of entries with '1': {label_counts.get(1, 0):.2f}%")  # Prints the percentage of GenAI patents.
        print(f"Share of entries with '0': {label_counts.get(0, 0):.2f}%")  # Prints the percentage of non-GenAI patents.
        print(f"1/0 Ratio: {self.total_ratio}")  # Prints the ratio of GenAI to non-GenAI patents.
        print(f"Total number of entries: {self.total_entries}")  # Prints the total number of patents in the dataset.

    def train_and_evaluate(self):
        """
        Train and evaluate transformer models.
        """
        # Define a list of classifiers with their corresponding models
        CLASSIFIERS = [
            ["SciBERT", "bert", "allenai/scibert_scivocab_uncased"],  # SciBERT model details
            ["XLNet", "xlnet", "xlnet-base-cased"]  # XLNet model details
        ]

        NUM_OF_SPLITS = 5  # Number of folds for cross-validation

        # Loop through each classifier in the CLASSIFIERS list
        for CL in tqdm(CLASSIFIERS, desc="Evaluating Classifiers", leave=True):
            try:
                name = CL[0]  # Name of the classifier (e.g., SciBERT, XLNet)
                Model1 = CL[1]  # Model type (e.g., 'bert', 'xlnet')
                Model2 = CL[2]  # Pre-trained model identifier from Hugging Face

                y_actual = []  # List to store actual class labels
                y_predicted = []  # List to store predicted class labels
                id_s = []  # List to store patent IDs for reference

                # Initialize Stratified K-Fold cross-validator
                KFoldSplitter = StratifiedKFold(
                    n_splits=NUM_OF_SPLITS,  # Number of folds
                    shuffle=True,  # Shuffles the data before splitting
                    random_state=1  # Sets a seed for reproducibility
                )

                # Perform cross-validation
                for train_i, test_i in tqdm(
                        KFoldSplitter.split(self.abstract_text, self.classes),
                        desc='Cross-Validating',
                        leave=False,
                        total=NUM_OF_SPLITS
                ):
                    Y = np.array(self.classes)  # Converts class labels to a NumPy array
                    Abstract_Text_Array = np.array(self.abstract_text)  # Converts abstracts to a NumPy array
                    train_X, test_X = Abstract_Text_Array[train_i], Abstract_Text_Array[test_i]  # Splits abstracts into training and testing sets
                    train_y, test_y = Y[train_i], Y[test_i]  # Splits class labels into training and testing sets
                    Train_IDs, Test_IDs = self.IDs[train_i], self.IDs[test_i]  # Splits patent IDs into training and testing sets

                    # Check class distribution in the current fold
                    print(f"Class distribution in current fold for model {name}:")
                    print("Train:", Counter(train_y))  # Prints the number of GenAI and non-GenAI patents in the training set
                    print("Test:", Counter(test_y))  # Prints the number of GenAI and non-GenAI patents in the testing set

                    # Prepare training DataFrame
                    TrainingDataframe = list(zip(list(train_X), list(train_y)))  # Zips abstracts and labels together
                    train_df = pd.DataFrame(TrainingDataframe)  # Creates a DataFrame from the zipped data
                    train_df.columns = ["text", "labels"]  # Names the columns as 'text' and 'labels'

                    # Initialize the model with specified parameters
                    model = ClassificationModel(
                        Model1,  # Model type (e.g., 'bert', 'xlnet')
                        Model2,  # Pre-trained model identifier
                        use_cuda=use_cuda,  # Uses GPU if available
                        args={
                            'overwrite_output_dir': True,  # Overwrites the output directory if it exists
                            'use_early_stopping': False,  # Disables early stopping
                            'do_lower_case': True,  # Converts all text to lowercase
                            'silent': True,  # Suppresses detailed logging
                            'no_cache': True,  # Disables caching of the dataset
                            'no_save': True,  # Disables saving of the model after training
                            'optimizer': 'AdamW',  # Specifies the optimizer to use
                            'learning_rate': 5e-5,  # Sets the learning rate for training
                            'adam_epsilon': 1e-8,  # Sets the epsilon value for the Adam optimizer
                            'weight_decay': 0.01,  # Sets the weight decay for regularization
                            'num_train_epochs': 3,  # Number of times the model will see the entire training dataset
                            'train_batch_size': 32,  # Number of samples processed before the model is updated
                            'scheduler_type': 'linear',  # Type of learning rate scheduler
                            'warmup_ratio': 0.1,  # Ratio of the total training steps used for a linear warmup
                        }
                    )

                    # Train the model on the training data
                    model.train_model(train_df)

                    # Make predictions on the test set
                    predictions, raw_outputs = model.predict(list(test_X))

                    id_s.extend(Test_IDs)  # Adds the current fold's test IDs to the list
                    y_actual.extend(test_y)  # Adds the actual labels to the list
                    y_predicted.extend(predictions)  # Adds the predicted labels to the list

                    # Clear memory to free up resources
                    gc.collect()  # Collects garbage to free up memory
                    torch.cuda.empty_cache()  # Clears the GPU cache

                # After all folds are processed for the current model
                Share = np.round(np.mean(y_predicted), 3)  # Calculates the average predicted value

                # Check if both classes are present to calculate ROC-AUC
                if len(set(y_actual)) > 1 and len(set(y_predicted)) > 1:
                    ROC = roc_auc_score(y_actual, y_predicted)  # Calculates the ROC-AUC score
                else:
                    ROC = None  # Sets ROC to None if it's not applicable
                    print(f"ROC-AUC score cannot be calculated for model {name}.")

                # Calculate other evaluation metrics
                Accuracy = accuracy_score(y_actual, y_predicted)  # Calculates accuracy
                Precision = precision_score(y_actual, y_predicted, zero_division=0)  # Calculates precision
                Recall = recall_score(y_actual, y_predicted, zero_division=0)  # Calculates recall
                F1 = f1_score(y_actual, y_predicted, zero_division=0)  # Calculates F1 score
                CM = confusion_matrix(y_actual, y_predicted)  # Generates the confusion matrix

                # Handle confusion matrix to extract true positives, false negatives, etc.
                try:
                    FN = np.round(CM[0][0] / (CM[0][0] + CM[1][0]), 3)  # Calculates False Negatives
                    FP = np.round(CM[0][1] / (CM[0][1] + CM[1][1]), 3)  # Calculates False Positives
                    TN = np.round(CM[1][0] / (CM[0][0] + CM[1][0]), 3)  # Calculates True Negatives
                    TP = np.round(CM[1][1] / (CM[0][1] + CM[1][1]), 3)  # Calculates True Positives
                except ZeroDivisionError:
                    FN = FP = TN = TP = None  # Sets values to None if division by zero occurs

                # Append the results for the current model to the results list
                self.results.append([
                    name,  # Name of the model
                    Share,  # Average predicted value
                    TP, FN, FP, TN,  # Confusion matrix metrics
                    np.round(Accuracy, 3),  # Rounded accuracy score
                    np.round(ROC, 3) if ROC is not None else None,  # Rounded ROC-AUC score
                    np.round(Precision, 3),  # Rounded precision score
                    np.round(Recall, 3),  # Rounded recall score
                    np.round(F1, 3)  # Rounded F1 score
                ])

                # Store the classified values for further analysis
                self.classified_values.append(
                    list(zip(len(id_s) * [name], id_s, y_actual, y_predicted))
                )

            except Exception as e:
                print(f"Error with classifier {CL[0]}: {e}")  # Prints any errors encountered with the current classifier
                continue  # Moves on to the next classifier if an error occurs

    def output_results(self):
        """
        Output the classification results and save them.
        """
        # Convert the results list to a DataFrame
        self.results_table = pd.DataFrame(
            self.results,
            columns=[
                "Name",  # Name of the model
                "Share",  # Average predicted value
                "True-Positives",  # True Positives from confusion matrix
                "False-Negatives",  # False Negatives from confusion matrix
                "False-Positives",  # False Positives from confusion matrix
                "True-Negatives",  # True Negatives from confusion matrix
                "Accuracy",  # Accuracy score
                "AUC",  # ROC-AUC score
                "Precision",  # Precision score
                "Recall",  # Recall score
                "F1"  # F1 score
            ]
        )

        # Add a new column 'Type' with the value 'Transformer' for all entries
        self.results_table["Type"] = "Transformer"  # Labels all entries as 'Transformer' type

        # Rearrange columns for better readability
        self.results_table = self.results_table[
            [
                "Name", "Type", "Share", "True-Positives",
                "False-Negatives", "False-Positives", "True-Negatives",
                "Accuracy", "AUC", "Precision", "Recall", "F1"
            ]
        ]

        # Save the results to a CSV file, sorted by descending accuracy
        output_path = f"Output/Transformer_Classification_Model_GenAI_Performance_{self.total_entries}_{self.total_ratio}.csv"
        self.results_table.sort_values("Accuracy", ascending=False).to_csv(
            output_path,
            index=False  # Excludes row indices from the saved file
        )
        print(f"Results saved to {output_path}")  # Notifies the user that results have been saved

        # Display the results in the console
        print(self.results_table.sort_values("Accuracy", ascending=False))  # Prints the sorted results table

        # Output the classification results for the training dataset
        Final = None  # Placeholder for the final DataFrame

        # Loop through the classification results for each model
        for i in range(len(self.classified_values)):
            Temp = pd.DataFrame(
                self.classified_values[i],
                columns=['Model', 'id', 'Actual', 'Predicted']  # Names the columns accordingly
            )

            name = Temp.iloc[0]['Model']  # Retrieves the model name from the first row

            if i == 0:
                Temp = Temp[['id', 'Actual', 'Predicted']]  # Selects specific columns
                Temp.columns = ['id', 'Actual', name]  # Renames the 'Predicted' column to the model's name
                Final = Temp  # Assigns Temp to Final for the first model
            else:
                Temp = Temp[['id', 'Predicted']]  # Selects 'id' and 'Predicted' columns
                Temp.columns = ['id', name]  # Renames 'Predicted' to the model's name
                Final = Final.merge(Temp, on='id')  # Merges the current model's predictions with the Final DataFrame

        # Save the final DataFrame to a CSV file
        final_output_path = f"Output/Transformer_Classification_GenAI_Results_{self.total_entries}_{self.total_ratio}.csv"
        Final.to_csv(
            final_output_path,
            index=False  # Excludes row indices from the saved file
        )
        print(f"Classification results saved to {final_output_path}")  # Notifies the user that classification results have been saved

    def plot_metrics(self):
        """
        Create and save a grouped bar chart for the evaluation metrics.
        """
        metrics_df = self.results_table  # Retrieves the results table
        metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1']  # Defines the metrics to plot

        # Create the grouped bar chart
        self.plot_all_metrics(metrics_df, metrics)  # Calls the helper function to plot all metrics

    def plot_all_metrics(self, df, metrics):
        """
        Create and save a grouped bar chart for all metrics.
        """
        df_plot = df[['Name'] + metrics]  # Selects the model names and specified metrics for plotting
        num_models = len(df_plot['Name'])  # Counts the number of models
        num_metrics = len(metrics)  # Counts the number of metrics to plot
        ind = np.arange(num_models)  # Generates an array of indices for the x-axis positions
        total_width = 0.8  # Total width allocated for all bars at one position
        width = total_width / num_metrics  # Width of each individual bar

        # Dynamically adjust the size of the plot based on the number of models and metrics
        fig_width = max(8, num_models * num_metrics)  # Sets the width of the figure
        fig_height = 6  # Sets the height of the figure
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))  # Creates a new figure and axes for plotting

        # Loop through each metric and create a bar for it
        for i, metric in enumerate(metrics):
            bars = ax.bar(ind + i * width, df_plot[metric], width, label=metric)  # Plots the bars for the current metric
            # Add labels on top of each bar to show the metric's value
            for bar in bars:
                height = bar.get_height()  # Gets the height of the current bar
                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.3f}',
                        ha='center', va='bottom', fontsize=8)  # Places the text label above the bar

        # Set labels and title for the plot
        ax.set_xlabel('Modellname')  # Sets the x-axis label to 'Model Name'
        ax.set_ylabel('Scores')  # Sets the y-axis label to 'Scores'
        ax.set_title('Vergleich der Transformer-Modellleistung')  # Sets the title of the plot
        ax.set_xticks(ind + total_width / 2 - width / 2)  # Positions the x-ticks in the center of the grouped bars
        ax.set_xticklabels(df_plot['Name'], rotation=45, ha='right')  # Sets the labels for the x-ticks and rotates them for better readability
        ax.legend()  # Adds a legend to the plot to identify each metric

        plt.tight_layout()  # Adjusts the padding of the plot to prevent clipping of labels
        plt.savefig(f"Figures/transformer_model_performance_grouped_bar_{self.total_entries}_{self.total_ratio}.png",
                    dpi=300)  # Saves the plot as a PNG file with high resolution
        plt.show()  # Displays the plot

# -------------------------------------------------------------
# Main Execution
# -------------------------------------------------------------

# The following code runs when the script is executed directly.
# To run the code, execute the following command in the terminal:
# python class_transf_genai_opt.py

if __name__ == '__main__':
    # Ensure necessary directories exist
    if not os.path.exists("Figures"):
        os.makedirs("Figures")  # Creates a directory named 'Figures' if it doesn't exist
    if not os.path.exists("Output"):
        os.makedirs("Output")  # Creates a directory named 'Output' if it doesn't exist

    # Paths to data files
    data_path = os.path.join("Training Data", "20240819_WIPO Patents GenAI US matched_1-1000.csv")  # Path to the main patent dataset
    delimiter = ';'  # Specifies that the CSV file uses a semicolon to separate values

    anti_seed_path = os.path.join("Training Data", "4K Patents - AI 20p.csv")  # Path to the anti-seed dataset

    # Initialize the classifier
    classifier = PatentTransformerClassifier(data_path, anti_seed_path, delimiter)  # Creates an instance of the PatentTransformerClassifier class

    # Load and prepare data
    classifier.load_data()  # Loads the main dataset
    classifier.load_anti_seed(n=3146)  # Loads and integrates the anti-seed dataset with 'n' set to 3146
    classifier.prepare_data()  # Prepares the data for classification by extracting relevant fields and calculating distributions

    # Train and evaluate models
    classifier.train_and_evaluate()  # Trains the models and evaluates their performance using cross-validation

    # Output results and plots
    classifier.output_results()  # Saves and displays the results of the model evaluations
    classifier.plot_metrics()  # Creates and saves visual representations of the evaluation metrics