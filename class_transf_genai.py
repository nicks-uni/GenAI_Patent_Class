# -------------------------------------------------------------
# Import Required Packages
# -------------------------------------------------------------
# These lines import various tools and libraries that the script will use.

from collections import Counter  # Provides a way to count occurrences of elements in a collection.
import os  # Provides functions for interacting with the operating system, such as reading or writing files and creating directories.
import logging  # Enables logging of events that happen when some software runs, useful for debugging and monitoring.
import warnings  # Used to control warning messages generated by the code.
import gc  # Provides functions to interact with the garbage collector, which helps manage memory by freeing up unused resources.

# Import necessary libraries for data manipulation and modeling
import pandas as pd  # A powerful library for data manipulation and analysis, especially for working with tables of data.
import numpy as np  # A library that supports large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.
from tqdm import tqdm  # Provides a progress bar for loops, making it easier to track the progress of long-running operations.
from simpletransformers.classification import ClassificationModel  # Simplifies the use of transformer models for classification tasks.

# Import scikit-learn utilities for model evaluation
from sklearn.model_selection import StratifiedKFold  # A tool for splitting data into training and testing sets while maintaining the percentage of samples for each class.
from sklearn.metrics import (
    accuracy_score,      # Measures the proportion of correct predictions out of all predictions made.
    roc_auc_score,       # Measures the ability of the model to distinguish between classes.
    precision_score,     # Measures the accuracy of positive predictions.
    recall_score,        # Measures the ability of the model to find all positive instances.
    f1_score,            # The harmonic mean of precision and recall, providing a balance between the two.
    confusion_matrix     # Summarizes the performance of a classification algorithm by displaying true vs. predicted labels.
)

# Import plotting libraries
import matplotlib.pyplot as plt  # A plotting library used for creating static, animated, and interactive visualizations.
import seaborn as sns  # A library built on top of matplotlib that provides a higher-level interface for drawing attractive statistical graphics.

# Suppress warnings and set logging level to ERROR to reduce output clutter
warnings.filterwarnings("ignore")  # This line tells Python to ignore all warning messages, keeping the output clean.
logging.basicConfig(level=logging.ERROR)  # Sets the logging level to ERROR, so only error messages are logged.
transformers_logger = logging.getLogger("transformers")  # Gets the logger for the 'transformers' library.
transformers_logger.setLevel(logging.ERROR)  # Sets the 'transformers' logger to only show ERROR level messages.

# Check if CUDA is available (for GPU acceleration)
import torch  # A deep learning library that provides tensor computations and automatic differentiation.
use_cuda = torch.cuda.is_available()  # Checks if a CUDA-compatible GPU is available for faster computations.

# -------------------------------------------------------------
# Define the PatentTransformerClassifier Class
# -------------------------------------------------------------
class PatentTransformerClassifier:
    def __init__(self, data_path, anti_seed_path, delimiter):
        """
        Initialize the classifier with paths to the data and configuration.
        
        Parameters:
        - data_path (str): Path to the main training data CSV file.
        - anti_seed_path (str): Path to the anti-seed data CSV file.
        - delimiter (str): Character used to separate values in the CSV files (e.g., ',', ';').
        """
        self.data_path = data_path  # Stores the path to the main training data.
        self.anti_seed_path = anti_seed_path  # Stores the path to the anti-seed data.
        self.delimiter = delimiter  # Stores the delimiter used in the CSV files.
        self.data = None  # Placeholder for the combined dataset.
        self.IDs = None  # Placeholder for patent IDs.
        self.abstract_text = None  # Placeholder for patent abstracts.
        self.classes = None  # Placeholder for labels indicating GenAI patents (1) or not (0).
        self.total_entries = None  # Placeholder for the total number of patents.
        self.total_ratio = None  # Placeholder for the ratio of GenAI to non-GenAI patents.
        self.results = []  # List to store performance results of each model.
        self.classified_values = []  # List to store individual classification results (actual vs. predicted).
        self.results_table = None  # Placeholder for the table of results.

    def load_data(self):
        """
        Load and preprocess the training data from a CSV file.
        
        This function reads the CSV file containing patent data, processes it by creating a new label column
        indicating whether each patent is related to Generative AI (GenAI), and stores the processed data.
        """
        # Load CSV file with patent data
        self.data = pd.read_csv(self.data_path, delimiter=self.delimiter)  # Reads the CSV file into a pandas DataFrame.

        # Categorize: if 'Finales Resultat' >= 1, set 'label_genai' to 1, else 0
        self.data['label_genai'] = (self.data['Finales Resultat'] >= 1).astype(int)  # Creates a new column 'label_genai' based on the 'Finales Resultat' column.

        print('Einträge data:', self.data.count('rows'))  # Prints the number of rows in the DataFrame.

    def load_anti_seed(self, n):
        """
        Load and integrate the anti-seed data to balance the dataset.
        
        Parameters:
        - n (int): Number of anti-seed entries to include in the dataset.
        
        This function reads additional data containing non-GenAI patents to balance the dataset, ensuring
        that there are enough non-GenAI examples for the model to learn effectively.
        """
        # Load AI patents to create an anti-seed set
        anti_seed = pd.read_csv(self.anti_seed_path, delimiter=',')  # Reads the anti-seed CSV file into a pandas DataFrame.

        # Filter patents without AI content (actual == 0)
        anti_seed = anti_seed[anti_seed['actual'] == 0]  # Filters the DataFrame to include only non-GenAI patents.

        # Limit the anti-seed dataset to n patents
        anti_seed = anti_seed[:n]  # Selects the first 'n' entries from the anti-seed DataFrame.

        # Adjust columns in the anti-seed set to combine with training data
        anti_seed['label_genai'] = 0  # Sets the 'label_genai' column to 0 for all anti-seed patents (non-GenAI).

        # Unify column names (adjust if columns are different)
        anti_seed = anti_seed.rename(columns={
            'app number': 'patent_id',
            'abstract': 'patent_abstract'
        })  # Renames columns to ensure consistency with the main training data.

        # Select relevant columns for merging
        anti_seed = anti_seed[['patent_id', 'patent_abstract', 'label_genai']]  # Keeps only the necessary columns.

        # Combine training data with anti-seed data
        self.data = pd.concat(
            [self.data[['patent_id', 'patent_abstract', 'label_genai']], anti_seed],
            ignore_index=True
        )  # Merges the main data with the anti-seed data to create a balanced dataset.

    def prepare_data(self):
        """
        Prepare data for text classification.
        
        This function extracts the necessary information from the combined dataset and calculates the distribution
        of labels to understand the balance between GenAI and non-GenAI patents.
        """
        # Extract IDs, abstracts, and class labels
        self.IDs = np.array(self.data['patent_id'].values.tolist())  # Extracts patent IDs as a NumPy array.
        self.abstract_text = self.data['patent_abstract'].values.tolist()  # Extracts patent abstracts as a list.
        self.classes = self.data['label_genai'].values.tolist()  # Extracts labels (1 or 0) as a list.

        # Calculate the distribution of 'label_genai'
        label_counts = self.data['label_genai'].value_counts(normalize=True) * 100  # Calculates the percentage of each label.
        self.total_entries = len(self.data)  # Counts the total number of patents.
        self.total_ratio = f"{label_counts.get(1, 0):.0f}-{label_counts.get(0, 0):.0f}"  # Formats the ratio as "GenAI%-Non-GenAI%".

        # Output the distribution and total count
        print(f"Share of entries with '1': {label_counts.get(1, 0):.2f}%")  # Prints the percentage of GenAI patents.
        print(f"Share of entries with '0': {label_counts.get(0, 0):.2f}%")  # Prints the percentage of non-GenAI patents.
        print(f"1/0 Ratio: {self.total_ratio}")  # Prints the ratio of GenAI to non-GenAI patents.
        print(f"Total number of entries: {self.total_entries}")  # Prints the total number of patents.

    def train_and_evaluate(self):
        """
        Train and evaluate transformer models.
        
        This function sets up different transformer-based machine learning models, trains them using cross-validation,
        evaluates their performance using various metrics, and stores the results for comparison.
        """
        # Define a list of classifiers with their corresponding models
        CLASSIFIERS = [
            ["BERT", "bert", "bert-base-uncased"],  # Basic BERT model for classification.
            ["SciBERT", "bert", "allenai/scibert_scivocab_uncased"],  # SciBERT model, specialized for scientific text.
            ["PatentSBERTa", "roberta", "AI-Growth-Lab/PatentSBERTa"],  # A RoBERTa model fine-tuned for patent data.
            ["XLNet", "xlnet", "xlnet-base-cased"]  # XLNet model for classification.
        ]

        NUM_OF_SPLITS = 5  # Number of folds for cross-validation.

        # Iterate through each classifier to train and evaluate.
        for CL in tqdm(CLASSIFIERS, desc="Evaluating Classifiers", leave=True):
            try:
                name = CL[0]  # Name of the model (e.g., BERT, SciBERT).
                Model1 = CL[1]  # Type of the transformer model (e.g., 'bert', 'roberta', 'xlnet').
                Model2 = CL[2]  # Specific pre-trained model name or path.

                y_actual = []  # List to store actual labels from the test sets.
                y_predicted = []  # List to store predicted labels from the models.
                id_s = []  # List to store patent IDs from the test sets.

                # Initialize Stratified K-Fold cross-validator.
                KFoldSplitter = StratifiedKFold(
                    n_splits=NUM_OF_SPLITS,  # Number of folds.
                    shuffle=True,  # Shuffles data before splitting to ensure randomness.
                    random_state=1  # Seed for reproducibility.
                )

                # Iterate through each fold in cross-validation.
                for train_i, test_i in tqdm(
                        KFoldSplitter.split(self.abstract_text, self.classes),
                        desc='Cross-Validating',
                        leave=False,
                        total=NUM_OF_SPLITS
                ):
                    Y = np.array(self.classes)  # Convert class labels to a NumPy array.
                    Abstract_Text_Array = np.array(self.abstract_text)  # Convert abstracts to a NumPy array.
                    train_X, test_X = Abstract_Text_Array[train_i], Abstract_Text_Array[test_i]  # Split abstracts into training and testing sets.
                    train_y, test_y = Y[train_i], Y[test_i]  # Split labels into training and testing sets.
                    Train_IDs, Test_IDs = self.IDs[train_i], self.IDs[test_i]  # Split patent IDs into training and testing sets.

                    # Check class distribution in the current fold.
                    print(f"Class distribution in current fold for model {name}:")
                    print("Train:", Counter(train_y))  # Prints the number of GenAI and non-GenAI patents in the training set.
                    print("Test:", Counter(test_y))  # Prints the number of GenAI and non-GenAI patents in the testing set.

                    # Prepare training DataFrame for the transformer model.
                    TrainingDataframe = list(zip(list(train_X), list(train_y)))  # Combines abstracts and labels into a list of tuples.
                    train_df = pd.DataFrame(TrainingDataframe)  # Creates a pandas DataFrame from the list of tuples.
                    train_df.columns = ["text", "labels"]  # Renames the columns to 'text' and 'labels'.

                    # Initialize the transformer model for classification.
                    model = ClassificationModel(
                        Model1,  # Type of transformer model (e.g., 'bert', 'roberta', 'xlnet').
                        Model2,  # Specific pre-trained model name or path.
                        use_cuda=use_cuda,  # Whether to use GPU acceleration.
                        args={
                            'num_train_epochs': 5,  # Number of times the model will iterate over the entire training dataset.
                            'overwrite_output_dir': True,  # Overwrites the output directory if it exists.
                            'use_early_stopping': False,  # Disables early stopping based on evaluation metrics.
                            'train_batch_size': 50,  # Number of samples processed before the model is updated.
                            'do_lower_case': True,  # Converts all characters to lowercase.
                            'silent': True,  # Disables verbose output during training.
                            'no_cache': True,  # Disables caching of the dataset.
                            'no_save': True  # Disables saving the model after training.
                        }
                    )

                    # Train the model using the training DataFrame.
                    model.train_model(train_df)  # Trains the model on the training data.

                    # Make predictions on the test set.
                    predictions, raw_outputs = model.predict(list(test_X))  # Predicts labels for the test abstracts.

                    # Collect results from the current fold.
                    id_s.extend(Test_IDs)  # Adds patent IDs from the test set to the list.
                    y_actual.extend(test_y)  # Adds actual labels from the test set to the list.
                    y_predicted.extend(predictions)  # Adds predicted labels to the list.

                    # Clear memory to prevent memory leaks.
                    gc.collect()  # Collects garbage to free up memory.
                    torch.cuda.empty_cache()  # Frees up unused memory in the GPU.

                # After all folds are processed, calculate performance metrics.
                Share = np.round(np.mean(y_predicted), 3)  # Calculates the average share of patents classified as GenAI.

                # Check if both classes are present to calculate ROC-AUC.
                if len(set(y_actual)) > 1 and len(set(y_predicted)) > 1:
                    ROC = roc_auc_score(y_actual, y_predicted)  # Calculates the ROC-AUC score.
                else:
                    ROC = None  # Sets ROC to None if it's not applicable.
                    print(f"ROC-AUC score cannot be calculated for model {name}.")

                # Calculate other performance metrics.
                Accuracy = accuracy_score(y_actual, y_predicted)  # Calculates overall accuracy.
                Precision = precision_score(y_actual, y_predicted, zero_division=0)  # Calculates precision.
                Recall = recall_score(y_actual, y_predicted, zero_division=0)  # Calculates recall.
                F1 = f1_score(y_actual, y_predicted, zero_division=0)  # Calculates F1 score.
                CM = confusion_matrix(y_actual, y_predicted)  # Generates a confusion matrix.

                # Handle confusion matrix to extract True Positives, False Negatives, etc.
                try:
                    FN = np.round(CM[0][0] / (CM[0][0] + CM[1][0]), 3)  # Calculates False Negatives rate.
                    FP = np.round(CM[0][1] / (CM[0][1] + CM[1][1]), 3)  # Calculates False Positives rate.
                    TN = np.round(CM[1][0] / (CM[0][0] + CM[1][0]), 3)  # Calculates True Negatives rate.
                    TP = np.round(CM[1][1] / (CM[0][1] + CM[1][1]), 3)  # Calculates True Positives rate.
                except ZeroDivisionError:
                    FN = FP = TN = TP = None  # Sets rates to None if there's a division by zero error.

                # Append the calculated metrics to the results list.
                self.results.append([
                    name,  # Model name.
                    Share,  # Share of GenAI classifications.
                    TP, FN, FP, TN,  # True Positives, False Negatives, False Positives, True Negatives.
                    np.round(Accuracy, 3),  # Rounded Accuracy.
                    np.round(ROC, 3) if ROC is not None else None,  # Rounded ROC-AUC or None.
                    np.round(Precision, 3),  # Rounded Precision.
                    np.round(Recall, 3),  # Rounded Recall.
                    np.round(F1, 3)  # Rounded F1 Score.
                ])

                # Save classification results for each patent.
                self.classified_values.append(
                    list(zip(len(id_s) * [name], id_s, y_actual, y_predicted))
                )  # Combines model name, patent IDs, actual labels, and predicted labels into tuples and adds them to the list.

            except Exception as e:
                print(f"Error with classifier {CL[0]}: {e}")  # Prints an error message if something goes wrong with a classifier.
                continue  # Skips to the next classifier.

    def output_results(self):
        """
        Output the classification results and save them.
        
        This function converts the collected results into a structured format, saves them as CSV files,
        and displays them for review. It also merges individual classification results into a comprehensive table.
        """
        # Convert the results list to a DataFrame
        self.results_table = pd.DataFrame(
            self.results,
            columns=[
                "Name",  # Model name.
                "Share",  # Share of GenAI classifications.
                "True-Positives",  # Number of correct GenAI predictions.
                "False-Negatives",  # Number of GenAI patents incorrectly predicted as non-GenAI.
                "False-Positives",  # Number of non-GenAI patents incorrectly predicted as GenAI.
                "True-Negatives",  # Number of correct non-GenAI predictions.
                "Accuracy",  # Overall accuracy of the model.
                "AUC",  # Area Under the ROC Curve.
                "Precision",  # Accuracy of positive predictions.
                "Recall",  # Ability to find all positive instances.
                "F1"  # Harmonic mean of precision and recall.
            ]
        )

        # Add a new column 'Type' with the value 'Transformer' for all entries
        self.results_table["Type"] = "Transformer"  # Specifies that the metrics are for transformer models.

        # Rearrange columns for better readability
        self.results_table = self.results_table[
            [
                "Name", "Type", "Share", "True-Positives",
                "False-Negatives", "False-Positives", "True-Negatives",
                "Accuracy", "AUC", "Precision", "Recall", "F1"
            ]
        ]  # Orders the columns in a logical sequence.

        # Save the results to a CSV file, sorted by descending accuracy
        output_path = f"Output/Transformer_Classification_Model_GenAI_Performance_{self.total_entries}_{self.total_ratio}.csv"  # Defines the filename for saving the results.
        self.results_table.sort_values("Accuracy", ascending=False).to_csv(
            output_path,
            index=False
        )  # Saves the performance metrics to a CSV file for further analysis.
        print(f"Results saved to {output_path}")  # Confirms that the results have been saved.

        # Display the results in the console
        print(self.results_table.sort_values("Accuracy", ascending=False))  # Prints the sorted results table to the console.

        # Output the classification results for the training dataset
        Final = None  # Initializes an empty variable to store the final classification results.

        # Loop through the classification results for each model
        for i in range(len(self.classified_values)):
            Temp = pd.DataFrame(
                self.classified_values[i],
                columns=['Model', 'id', 'Actual', 'Predicted']
            )  # Creates a temporary DataFrame for the current model's predictions.

            name = Temp.iloc[0]['Model']  # Retrieves the model name from the first row.

            if i == 0:
                Temp = Temp[['id', 'Actual', 'Predicted']]  # Selects relevant columns.
                Temp.columns = ['id', 'Actual', name]  # Renames columns to include the model name.
                Final = Temp  # Initializes the Final DataFrame with the first model's results.
            else:
                Temp = Temp[['id', 'Predicted']]  # Selects relevant columns.
                Temp.columns = ['id', name]  # Renames columns to include the model name.
                Final = Final.merge(Temp, on='id')  # Merges with the Final DataFrame based on 'id'.

        # Save the final DataFrame to a CSV file
        final_output_path = f"Output/Transformer_Classification_GenAI_Results_{self.total_entries}_{self.total_ratio}.csv"  # Defines the filename for saving the detailed classification results.
        Final.to_csv(
            final_output_path,
            index=False
        )  # Saves the detailed classification results.
        print(f"Classification results saved to {final_output_path}")  # Confirms saving.

    def plot_metrics(self):
        """
        Create and save a grouped bar chart for the evaluation metrics.
        
        This visualization helps in comparing the performance metrics of different models side by side.
        """
        metrics_df = self.results_table  # Retrieves the DataFrame containing the results.
        metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1']  # List of metrics to plot.

        # Create the grouped bar chart
        self.plot_all_metrics(metrics_df, metrics)  # Calls the function to plot the metrics.

    def plot_all_metrics(self, df, metrics):
        """
        Create and save a grouped bar chart for all metrics.
        
        Parameters:
        - df (DataFrame): The DataFrame containing model names and their corresponding metrics.
        - metrics (List[str]): List of metric names to plot.
        
        This function generates a bar chart where each model's performance across different metrics is displayed side by side for easy comparison.
        """
        df_plot = df[['Name'] + metrics]  # Selects the 'Name' and metric columns for plotting.
        num_models = len(df_plot['Name'])  # Counts the number of models.
        num_metrics = len(metrics)  # Counts the number of metrics.
        ind = np.arange(num_models)  # The x locations for the groups.
        total_width = 0.8  # Total width for all bars in a group.
        width = total_width / num_metrics  # Width of each individual bar.

        # Dynamische Anpassung der Diagrammgröße
        fig_width = max(8, num_models * num_metrics)  # Sets the figure width based on the number of models and metrics.
        fig_height = 6  # Sets the figure height.
        fig, ax = plt.subplots(figsize=(fig_width, fig_height))  # Creates a figure and a set of subplots with the specified size.

        for i, metric in enumerate(metrics):
            bars = ax.bar(ind + i * width, df_plot[metric], width, label=metric)  # Creates a bar for each metric.
            # Add labels for the height of the bars for clarity.
            for bar in bars:
                height = bar.get_height()  # Gets the height of the bar.
                ax.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.3f}',
                        ha='center', va='bottom', fontsize=8)  # Places the metric value above the bar.

        ax.set_xlabel('Model Name')  # Labels the x-axis.
        ax.set_ylabel('Scores')  # Labels the y-axis.
        ax.set_title('Comparison of Transformer Model Performance')  # Sets a descriptive title.
        ax.set_xticks(ind + total_width / 2 - width / 2)  # Sets the position of the x-axis ticks.
        ax.set_xticklabels(df_plot['Name'], rotation=45, ha='right')  # Labels the x-axis ticks with model names, rotated for better readability.
        ax.legend()  # Adds a legend to differentiate the metrics.

        plt.tight_layout()  # Adjusts the padding between and around subplots to prevent overlapping elements.
        plt.savefig(f"Figures/transformer_model_performance_grouped_bar_{self.total_entries}_{self.total_ratio}.png",
                    dpi=300)  # Saves the figure as a high-resolution PNG file.
        plt.show()  # Displays the plot to the user.

# -------------------------------------------------------------
# Main Execution
# -------------------------------------------------------------
# The following code runs when the script is executed directly.
# To run the code, execute the following command in the terminal:
# python class_transf_genai.py

if __name__ == '__main__':
    # Ensure necessary directories exist.
    if not os.path.exists("Figures"):
        os.makedirs("Figures")  # Creates the 'Figures' directory to store plots.
    if not os.path.exists("Output"):
        os.makedirs("Output")  # Creates the 'Output' directory to store result CSV files.

    # Paths to data files.
    data_path = os.path.join("Training Data", "20240819_WIPO Patents GenAI US matched_1-1000.csv")  # Defines the path to the main training data CSV file.
    delimiter = ';'  # Specifies that the CSV file uses a semicolon to separate values.

    anti_seed_path = os.path.join("Training Data", "4K Patents - AI 20p.csv")  # Defines the path to the anti-seed data CSV file.

    # Initialize the classifier.
    classifier = PatentTransformerClassifier(data_path, anti_seed_path, delimiter)  # Creates an instance of the PatentTransformerClassifier.

    # Load and prepare data.
    classifier.load_data()  # Reads and processes the main training data.
    classifier.load_anti_seed(n=1146)  # Loads and integrates the anti-seed data with a specified number of entries.
    classifier.prepare_data()  # Prepares the data by extracting features and calculating label distributions.

    # Train and evaluate models.
    classifier.train_and_evaluate()  # Trains each classifier using cross-validation and evaluates their performance.

    # Output results and plots.
    classifier.output_results()  # Outputs and saves the classification results.
    classifier.plot_metrics()  # Generates a bar chart comparing the performance of all classifiers.